\documentclass[]{article}
%\usepackage[paperheight=18cm,paperwidth=14cm,textwidth=12cm]{geometry}
%\usepackage[skip=20pt plus1pt, indent=40pt]{parskip}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{sectsty}
\definecolor{bittersweet}{rgb}{1.0, 0.44, 0.37}
\definecolor{grey}{rgb}{0.25, 0.25, 0.28}
\subsectionfont{\color{grey}}
\subsubsectionfont{\color{grey}}
\sectionfont{\color{bittersweet}}

\usepackage[T1]{fontenc}
\renewcommand\familydefault{\sfdefault} 

\usepackage{bm}

\newcommand{\ev}{\mathbb{E}[X]}
\renewcommand{\ev}[1]{\mathbb{E}[#1]}

\newcommand{\definizione}{\paragraph{Definizione:}}
\newcommand{\formula}{\paragraph{Formula generica:}}

\begin{document}
    \tableofcontents
    \newpage
    \section{Introduzione alla probabilità}    
    \subsection{Glossario}
    \begin{itemize}
        \item Sistemi non deterministici $\rightarrow$ \textit{conoscendo i dati iniziali non possiamo determinare i dati finali}
        \item Incertezza degli eventi $\rightarrow$ \textit{la varianza degli eventi che possono succede}
        \item Rumore $\rightarrow$ \textit{possiamo misurare un evento solo approssimatamente}
        \item Probabilità $\rightarrow$ \textit{la materia che studia i sistemi non deterministici}
        \begin{itemize}
            \item Frequestista $\rightarrow $ \textit{probabilità assegnata sulla base di più esperimenti ripetuti nella stessa condizioni}
            \item Soggettivista $\rightarrow $ \textit{non esiste un valore oggettivo ma ci si basa sulla fiducia e sull'incertezza che l'individuo ha riguardo l'occorrenza di un certo evento}
        \end{itemize}
        \item Varianza $\rightarrow$ \textit{dispersione dei dati attorno al valore centrale / media / valore}
        \item Confidenza $\rightarrow$ \textit{intervallo che rappresenta una stima dei valori medi}
        \item Frequenza
        \begin{itemize}
            \item Frequenza assoluta $\rightarrow$ Numero di volte che si verifica un evento
            \item Frequenza relativa $\rightarrow$ Rapporto tra frequenza assoluta e il numero di prove/dati
        \end{itemize}
        \item Dataset $\rightarrow$ numero di dati a disposizione $\boldsymbol{D_n = \{ x_1 \cdots x_n \}} $
        \item Principio di enumerazione $\rightarrow$ Passare solo una volta da ogni elemento della raccolta
        \item Spazio esiti (s o $\Omega$) $\rightarrow$ Tutti i possibili esiti di un evento $\rightarrow$ $\boldsymbol{Dado=\{1 \cdots 6\}}$
        \item Spazio eventi (e) $\rightarrow$ Tutti i possibili risultati di un esperimento $\rightarrow$ $\boldsymbol{Dado=\{1||2\}} \leftarrow$ che esca \textbf{1} oppure \textbf{2}
        \item Assioma $\rightarrow$ Tre assiomi fondamentali su cui si poggia la teoria del calcolo delle probabilità
        \begin{itemize}
            \item 1' Assioma $\rightarrow$ La probabilità di E è un numero reale \textbf{non negativo} $\mathbb{P}(E) \in \mathbb{R}, \mathbb{P}(E) \geq 0$ $ \forall E \subseteq \Omega $ \space \textbf{|} \space $0 \leq P(E) \leq 1$
            \item 2' Assioma $\rightarrow$ Allo spazio degli esiti è sempre associato ad \textbf{1} \\ $\mathbb{P}(s) = 1$
            \item 3' Assioma $\rightarrow$ Per ogni coppia di eventi incompatibili $E_1, E_2 \subseteq \Omega$ \\ la probabilità di $E_1 \cup E_2$ è uguale alla \textbf{somma della loro probabilità} \\ $\mathbb{P}(E_1 \cup E_2) = \mathbb{P}(E_1) + \mathbb{P}(E_2)$
        \end{itemize} 
    \end{itemize}
    \newpage 

    \subsection{Moda e Mediana}
    \subsubsection{Moda}
    \definizione La moda è il valore che presenta la \textbf{massima frequenza} all'interno del dataset
    \formula
    \begin{equation*}
        Moda \rightarrow v_i : f_i = max f_i
        \begin{cases}
            \text{un solo valore} & \textbf{Moda} \\
            \text{più di un valore} & \textbf{Valori modali}
        \end{cases}
    \end{equation*}

    \subsubsection{Mediana}
    \definizione La mediana è il \textbf{valore centrale} all'interno del dataset (dati ordinati in ordine crescente/decresente)
    \formula
    \begin{equation*}
        Mediana =
        \begin{cases}
            \text{n pari} &  \frac{x_\frac{n}{2} + x_{\frac{n}{2}+ 1}}{2} \\
            \text{n dispari} & x_{[\frac{n+1}{2}]} \leftarrow \text{Intero superiore (Ceil)}
        \end{cases}
    \end{equation*}
    \paragraph{Esempio:} \[ D_n = \textbf{\{ 28, 34, 51, 19, 62, 43, 29, 38, 45, 26, 49, 33 \}} \]
    \paragraph{Per la mediana è necessario ordinare i dati in ordine crescente:} \[ D_n = \textbf{\{ 19, 26, 28, 29, 33, 34, 38, 43, 45, 49, 51, 62 \}} \]

    \[ \frac{x_{\frac{12}{2}} + x_{\frac{12}{2} + 1}}{2} = \frac{x_6 + x_7}{2} = \frac{34 + 38}{2} = \frac{72}{2} = \textbf{36} \]

    \paragraph{Nota:} quando si trova ad esempio $x_6$ bisogna andare a sostituire il valore con la posizione di x



    \subsection{Media e Varianza Campionaria}
    \subsubsection{Media Campionaria}
    \definizione La media campionaria è la \textbf{media} degli elementi di un campione.
    \formula  \[ \overline{X} = \frac{1}{n} \sum_{i=1}^{n} X_i \]

    \subsubsection{Varianza Campionaria}
    \definizione La varianza campionaria è la \textbf{dispersione} degli elementi di un campione.
    \formula \[ S^2 = \frac{\sum_{i = 1}^{n} (x_i - \overline{x})^2}{n-1} \]

    \paragraph{Esempio: (Varianza e Media)} $D_n = \textbf{\{ 3, 4, 6, 7, 10 \}}$ \\
    \linebreak[2]
 
    \text{Media del campione:} $ \overline{X} = \boldsymbol{\frac{( 3 + 4 + 6 + 7 + 10)}{5}} = \frac{30}{5} = 6 $ \\

    \text{Varianza campionaria:} $ s^2 = \boldsymbol{\frac{[ (-3)^2 + (-2)^2 + 0^2 + 1^2 + 4^2 ]}{4}} = 7.5 $ 

    \subsection{Disugaglianza di Chebyshev}
    \definizione Dice quanti dati di un campione cadono all'interno di un intervallo con centro la \textbf{media}

    \[ \forall k \geq 1 : k \in \mathbb{R} \]
    \[ (\overline{x} - k_s, \overline{x} + k_s) \longrightarrow \boldsymbol{S_k : [i : 1 \leq i \leq n, |x_i - \overline{x} < k_s|]} \]
    \paragraph{Generalizzando:}
    \[ |x - \overline{x}| < 5 \longrightarrow 68\% \]
    \[ |x - \overline{x}| < 25 \longrightarrow 95\% \]
    \[ |x - \overline{x}| < 35 \longrightarrow 99.7\% \]
    \subsection{Percentile}
    \definizione Il percentile è un indicatore che serve ad \textbf{indicare il valore minimo} sotto al quade ricade una \textbf{determinata percentuale} degli altri elementi sotto osservazione.
    \begin{equation*}
        Valore
        \begin{cases}
            \geq & \text{k \% dati} \\
            \leq & \text{100 - k \% dati}
        \end{cases}
    \end{equation*}
    \text{Prima cosa da fare è ordinare i valori in ordine crescente} \\
    \linebreak[3]
    \text{Dove il secondo quartile è sempre uguale alla \textbf{mediana}}
    \paragraph{Esempio:} $D_n = \textbf{\{ 0, 1, 2, 3, 4, 5, 6, 7, 8 \}}$

    \subsection{Insieme di dati Bivariati}
    \definizione è lo studio della relazione di due variabili.
    \formula \[ D_n : \{(X_1, Y_1) (X_2, Y_2) \cdots (X_n, Y_n)\}\]

    \subsubsection{Coefficiente di correlazione campionario}
    \definizione utilizzato per capire se esiste un legame \textbf{lineare} tra due serie di dati.
    \formula 
    \begin{equation*}
        r = \frac{\sum_{i = 1}^{n}(x_i - \overline{x}) (y_i - \overline{y})}{(n-1) s_x s_y} = \frac{\sum_{i = 1}^{n} (x_i - \overline{x}) (y_i - \overleftarrow{y})}{\sqrt{\sum_{i = 1}^{n} (x_i - \overline{x})^2 \sum_{i = 1}^{n} (y_i - \overline{y})^2}}
    \end{equation*}
    \subsection{Permutazioni, Combinazioni e Disposizioni}
    \subsubsection{Permutazioni}
    \definizione Modi possibili per sistemare \textbf{n} oggetti (\textbf{0! = 1})
    \[ n! = n \cdot (n-1) \cdots (n \cdot (n-1)) \]
    \paragraph{Esempio:} Fattoriale di 6
    \[ 6! = 6 \cdot (6-1) \cdot (6-2) \cdots (6-5) = 720 \]

    \subsubsection{Combinazioni}
    \definizione Modi di disporre \textbf{k} elementi scelti da \textbf{n} elementi (l'ordine \textbf{non conta})
    \[ \frac{n!}{k! \cdot (n-k)!} = \binom{n}{k} \]

    \paragraph{Esempio:} in una classe di \textbf{26} alunni si devono eleggere \textbf{2} rappresentanti
    \[ C_{n,k} = \frac{n!}{k! \cdot (n-k)!} \]
    \text{Sostituiamo \textbf{n} con \textit{26} (numero di alunni) e \textbf{k} con \textit{2} (numeri di rappresentanti)}
    \[ C_{26,2} = \frac{26!}{2! \cdot (26-2)!} = \frac{26!}{2! \cdot 24!} = \frac{25 \cdot 26}{2} = \textbf{325} \]
    \begin{center} 
        \text{è possibile anche semplificare i fattoriali come in questo caso}
    \end{center}

    \subsubsection{Disposizioni}
    \definizione Modi di disporre \textbf{k} elementi scelti da \textbf{n} elementi (l'ordine \textbf{conta})
    \[ \frac{n!}{(n-k)!}\]
    \paragraph{Esempio:} Quante parole is possono ottenere usando 4 \textbf{diverse} lettere da \textit{youmath} 
    \text{In questo caso dobbiamo contare le \textbf{disposizioni} senza ripetizione di \textbf{classe 4 di 7}}
    \[ D_{7,4} = \frac{7!}{(7-4)!} = \frac{7!}{3!} = \frac{5040}{6} = \textbf{840} \]



    \subsection{Probabilità condizionata}
    \definizione è la probabilità che succeda un evento \textbf{E} dato un evento \textbf{F}
    \[ P(E | F) = \frac{P(E | F)}{P(F)}\]
    \paragraph{Esempio:} \textbf{3} \text{scatole con contenuto nascosto dove in una è presente il premio} \\
    \[ P(Vincita) = \frac{1}{3}\]
    \[ P(Vincita | \text{1' pacco contiene un gatto}) = \frac{1}{2}\]
    \[ P(Vincita | \text{1' pacco \textbf{NON} contiene un gatto}) = 0\]



    \subsubsection{Teorema di Bayes}
    \formula \[ P(F_j | E) = \frac{P(F_j \cap E)}{P(E)} = \frac{P(E | F_j) \cdot P(F_j)}{\sum_{i = 1}^{p} P(E | F_i) \cdot P(F_i)} \]
    \begin{center} 
        \text{Probablità di \textbf{$F_j$} sapendo che si sia verificato l'evento \textbf{E}}
    \end{center}

    \subsection{Operazioni e proprietà tra eventi}
    \definizione Prendiamo come esempio \textbf{E} ed \textbf{F} come eventi
    \begin{itemize}
        \item E $\cup$ F $\longleftarrow$ Unione
        \item E $\cap$ F $\longleftarrow$ Intersezione
        \item E $\subset$ F | E $ \subseteq $ F $\longleftarrow$ Contenuto 
        \item E $\supset $ F | E $ \supseteq $ F $\longleftarrow$ Contiene
        \item $E^c$ $\longleftarrow$ Complemento
    \end{itemize}
    \paragraph{Le seguenti operazioni possono essere combinate tra di loro:} formando cosi le proprietà che seguono:
    \begin{itemize}
        \item $ E \cup (F \cup G) = (E \cup F) \cup G \longrightarrow$ Associativa unione
        \item $ (E \cup F) \cap G = (E \cap G) \cup (F \cup G) \longrightarrow$ Distributiva intersezione
        \item $ E \cap (F \cap G) = (E \cap F) \cap G \longrightarrow$ Associativa intersezione
        \item $ (E \cap F) \cup G = (E \cup G) \cap (F \cup G) \longrightarrow$ Distributiva unione
        \item $ (E \cup F)^c = \frac{E^c \cap F^c}{(E \cap F)^c} = E^c \cup F^c $
    \end{itemize}

    \section{Variabile aleatorie}
    \definizione La variabile aleatoria è una variabile che può assumere \textbf{valori diversi} \\ 
    in dipendenza da \textit{qualche esperimento casuale}.
    \[
        X
        \begin{cases}
            Discrete & \text{Solo \textbf{valori finiti}} \\
            Continue & \text{Possono assumere \textbf{range illimitati}}
        \end{cases}
    \]
    \subsection{Funzione di ripartizione (Tutte le variabili)}
    \definizione
    \text{La Probabilità che la variabile aleatoria \textbf{X} assuma un valore minore o uguale ad x}
    \formula $ F(x) = P(X \leq x) $
    \begin{itemize}
        \item F = funzione di ripartizione
        \item X = variabile aleatoria
        \item x = variabile normale
    \end{itemize}
    
    \paragraph{Esempio}:  \\
    \linebreak[4]
    $ P(a < X \leq b) $ \\
    \linebreak[4]
    $ P(X \leq b) = P(X \leq a) + P(a < X \leq b) $ \\
    \linebreak[4]
    $ P(a < X \leq b) = P(X \leq b) - P(X \leq a) = F(b) - F(a) $
    
  
    \subsection{Funzione di massa (Variabili discrete)}
    \formula $p(a) = P(X = a) $ \\
    \linebreak[4]
    \text{se si ha la funzione di ripartizione è possibile ottenere la funzione di massa perche:}
    \[ X \leq a = \cup X_i \]
    \formula \[ F(x) = P(X \leq a) = \sum_{x \leq a}^{} p(x_i) \]
    TODO- GRAFICO
    \paragraph{Esempio:} \text{variabile aleatoria X che può assumere valori \textbf{1}, \textbf{2} o \textbf{3}} \\
    \text{Dato che p(1) + p(2) + p(3) = \textbf{1}}
    \paragraph{Se:}
    \[ p(1) = \frac{1}{2} \]
    \[ p(2) = \frac{1}{3} \]
    \paragraph{Allora:}
    \[ p(3) = \frac{1}{6}\]

    \text{La funzione di ripartizione F di X è data da:}
    \begin{equation*}
        F(a) = \\
        \begin{cases}
            0 & a < 1 \\
            \frac{1}{2} & 1 \leq a < 2 \\
            \frac{5}{6} & 2 \leq a < 3 \\
            1 & 3 \leq a
        \end{cases}
    \end{equation*}

    \subsection{Funzione della densità di probabilità (Variabili continue)}
    \formula
    \[ P(X \in B) = \int_{B}^{} f(x) \,dx \]
    \[ P(X \in (-\infty, +\infty)) = \int_{-\infty}^{+\infty} f(x) \,dx=1 \]
    integrando $-\infty \text{ a} +\infty$ la probabilità che avvenga x è per forza 1 perche andiamo ad includere tutti i valori di $\mathbb{R}$

    \begin{center}
        \text{Se abbiamo che \textbf{B = [a, b]}} $\longrightarrow P(a \leq X \leq b) = \int_{a}^{b} f(x) \, dx $
        \\
        \text{Se abbiamo che \textbf{B = [a]}} $\longrightarrow P(X = a) = \int_{a}^{a} f(x) \, dx = 0 $
    \end{center}
   

    \text{Relazione che lega la funzione di ripartizione \textbf{F} alla densità \textbf{f}:}
    \[ F(x) = P(X \leq x) = \int_{-\infty}^{x} f(x )\, dx \]

    \text{Derivando entrambi i membri otteniamo che:}
    \[ \frac{d}{da} F(a) = f(a)  \]
    \paragraph{Esempio:} Sia assegnata una variabile aleatoria X con densità data da:
    \begin{equation*}
        f(x) = \\
        \begin{cases}
            C(4x - 2x^2) & 0 < x < 2 \\
            0 & \text{altrimenti}
        \end{cases}
    \end{equation*}
    \text{\textbf{(a)} quanto vale C? \textbf{(b)} quanto vale P(X > 1)? } \\
    \linebreak[4]
    \text{\textbf{(a)} siccome f è una densita allora:}
    \begin{equation*}
        \begin{split}
            1 & = C \int_{0}^{2} (4x - 2x^2) \, dx\\
            & = C[2x^2 - \frac{2x^3}{3}] \bigg\rvert_{x=0}^{x=2} = C \cdot \frac{8}{3} \\
            & = C = \frac{3}{8}
        \end{split}
    \end{equation*} \\
    \linebreak[4]
    \text{\textbf{(b)} conoscendo ora la densità f possiamo trovare la P(X > 1):}
    \[ P(X > 1) = \int_{1}^{\infty} f(x) \, dx = \frac{3}{8} \int_{1}^{2}(4x - 2x^2) \, dx = \frac{1}{2} \]
    \newpage

    \section{Funzioni a due variabili}
    Questo tipo di funzioni ci sono utili quando l'utilizzo di una sola variabile è impossibile poichè 
    \textit{l'oggetto in questione è basato sulla relazione di due variabili aleatorie}
    \subsection{Funzione di ripartizione congiunta}
    \definizione Funzione di ripartizione a due variabili aleatorie X e Y
    \formula \[ F(x, y) = P(X \leq x, Y \leq y) \]
    \text{Se vogliamo trovare solamente la funzione di ripartizione di una singola variabile aleatoria:}
    \begin{equation*}
    \begin{split}
        F_X(x) & = P(X \leq x) \\
        & = P(X \leq x, Y \leq \infty) \\
        & = F(x, \infty) \\
    \end{split}
    \end{equation*}
    Applicabile anche alla $F_y(y)$
    \[ F_Y(y) = F(\infty, y) \]

    \subsection{Funzione di massa congiunta}
    \definizione Probabilita che accadano due eventi (\textbf{X} e \textbf{Y}) nello stesso istante.
    \formula $ p(x_i, y_j) = P(X=x_i, Y=y_j) $

    \text{Se vogliamo trovare solamente la funzione di massa di una singola variabile aleatoria:}
    \begin{equation*}
        \begin{split}
            p_X(x_i) & := P(X = x_i) \\ 
            & = P(\underset{j}{\cup} \{ X=x_i, Y=y_j \}) \\
            & = \sum_{j}^{} P(X=x_i, Y=y_j) \\ 
            & = \sum_{j}^{} p(x_i, y_j) \\
        \end{split}
    \end{equation*}
    Applicabile anche alla $p_Y$
    \[ p_Y(y_j) = \sum_{i}^{} p(x_i, y_j) \]
    \linebreak[4]
    \[ \sum_{x}^{} \sum_{y}^{} p(x, y) = 1 \]

    \newpage
    \subsection{Funzione densità congiunta}
    \definizione Due variabili aleatorie X e Y sono \textit{congiuntamente continue} se esiste un funzione non negativa f(x,y)
    definita per tutti gli x e gli y
    \formula
    \[ P((X,Y) \in C) = \int_{}^{} \int_{(x,y) \in C}^{} f(x,y) \, dx \, dy \] \\

    \text{se A e B sono sottoinsiemi qualsiasi di $\mathbb{R}$ e C:= A x B}
    \[ C:= {(x,y) \in \mathbb{R}^2 : x \in A, y \in B}\]
    \text{Possiamo riscrivere la funzione di ripartizione congiunta di X e Y come segue:}
    \begin{equation*}
        \begin{split}
            F(a,b) & = P(X \leq a, Y \leq b) \\
            & = P(X \in a, Y \in b) \\
            & = \int_{B}^{} \int_{A}^{} f(x,y) \, dx \, dy \\
            & = \int_{-\infty}^{a} \int_{-\infty}^{b} f(x,y) \, dx \, dy \\
        \end{split}
    \end{equation*}
    \paragraph{Esempio:} Siano X e Y due variabili aleatorie congiuntamente continue con densità
    di probabilità data da:
    \begin{equation*}
        f(x,y) =
        \begin{cases}
            2e^{-x} e^{-2y} & x > 0, y > 0 \\
            0 & altrimenti
        \end{cases}
    \end{equation*}
    Si calcolino \textbf{(a)} P(X > 1, Y < 1)
    \begin{equation*}
        \begin{split}
            P(X > 1, Y < 1) & = \int_{0}^{1} \int_{1}^{\infty} 2e^{-x} e^{-2y} \, dx \, dy \\
            & = \int_{0}^{1} 2e^{-2y} (\int_{1}^{\infty} e^{-x} \, dx) \, dy \\
            & = \int_{0}^{1} 2e^{-2y}\{ -e^{-x}\} \rvert_{x=1}^{\infty} \, dy \\
            & = e^{-1} \int_{0}^{1} 2e^{-2y} \, dy \\
            & = e^{-1}(1 - e^{-2})
        \end{split}
    \end{equation*}
    \centerline{In questo caso si è integrato prima in una variabile e poi nell'altra}

    \subsection{Variabili aleatorie indipendenti}
    \subsubsection{X,Y indipendenti}
    \definizione
    \text{Un evento su una variabile non influenza l'altra.}
    \formula Se soddisfano questa richiesta le variabili si dicono \textit{indipendenti} 
    \[ P(X \in A, Y \in B) = P(X \in A) P(Y \in B)\]

    Usando gli assiomi della probabilità è possibile dimostrare che la definizione di sopra è equivalente a:
    \[ P(X \leq a, Y \leq b) = P(X \leq a) P(Y \leq b)\]
    \[ \forall a,b \in \mathbb{R} \]
    \linebreak[3]

    Ovvero che la funzione di ripartizione congiunta sia il prodotto delle marginali:
    \[ F(a,b) = F_X(a) F_Y(b)\]

    \textbf{Funzione di massa:}
    \[ p(a,b) = p_X(a) p_Y(b) \]
    \[ P(X=a, Y=b) = P(X=a) P(Y=b)\]
    \paragraph{Dimostrazione:}
    \begin{equation*}
        \begin{split}
            P(X \in A, Y \in B) & = \sum_{x \in A}^{} \sum_{y \in B}^{} p(x, y) \\
            & = \sum_{x \in A}^{} \sum_{y \in B}^{} p_X(x) p_Y(y) \\
            & = \sum_{x \in A}^{} p_X(x) \sum_{y \in B}^{} p_Y(y) \\
            & = P(X \in A) P(Y \in B)
        \end{split}
    \end{equation*}

    \paragraph{Funzione di densità:}
    \[ f(x, y) = f_X(x) f_Y(y)\]
    \[ \forall x,y \in \mathbb{R} \]
    
    \paragraph{Esempio con variabili indipendenti continue e con stessa funzione di densità:}
    \begin{equation*}
        f_X(t) = f_Y(t) =
        \begin{cases}
            e^{-t} & t > 0 \\
            0 & altrimenti
        \end{cases}
    \end{equation*}
    Quale è la densità di probabilità della variabile aleatoria data dal rapporto X/Y
    \begin{equation*}
    \begin{split}
        F_{X|Y}(a) & = P(X | Y \leq a) \\
        & = \int_{(x,y)}^{}\int_{x \leq ay}^{} f(x, y) \, dx \, dy \\ 
        & = \int_{(x,y)}^{}\int_{x \leq ay}^{} f(x) f(y) \, dx \, dy \\ 
        & = \int_{0}^{\infty} \int_{0}^{ay} e^{-x} f(x) f(y)\, dx \, dy \\ 
        & = \int_{0}^{\infty} e^{-y}(\int_{0}^{ay} e^{-x}) \, dy \\
        & = \int_{0}^{\infty} e^{-y} (1-e^{-ay}) \, dy \\ 
        & = [-e^{-y} + \frac{e^{-(a+1)y}}{(a+1)}] \bigg\rvert_{0}^{\infty} \\ 
        & =  1 - \frac{1}{a+1} 
    \end{split}
    \end{equation*}
    La funzione di densità si ricava infine \textbf{derivando} la funzione di ripartizione
    \[f_{X|Y}(a) = \frac{d}{da}(1- \frac{1}{a + 1}) = \frac{1}{(a + 1)^2} a > 0 \]

    \newpage
    \subsection{Distribuzioni condizionate}
    \definizione La distribuzione condizionata di Y dato X è la probabilità di X quando è conosciuto il valore assunto da X. \\
    A ogni distribuzione condizionata è associato un valore atteso condizionato e una varianza condizionata
    \formula $P(E|F) = \frac{P(E \cap F)}{P(F)}$

    \subsection{funzione di massa condizionata (Discrete)}
    \formula
    \begin{equation*}
        \begin{split}
            p_{X|Y}(X | Y) & = P (X=x, Y=y) \\
            & = \frac{P(X=x, Y=y)}{P(Y=y)} \\
            & = \frac{p(X,Y)}{p_Y(x, y) > 0} 
        \end{split}
    \end{equation*}
    \[ \forall x, \forall y \text{ con } p_Y(y) > 0\]
    \linebreak[4]
    \text{Se y non è un valore possibile di Y, ovvero se P(Y = y) = 0, la quantità $p_{X|Y}(x|y)$ non è definita }
    
    \paragraph{Esempio:} Siano X e Y due variabili aleatorie discrete con funzione di massa congiunta p dato che: 
    \begin{center}
        \begin{minipage}{0.2\textwidth}
            $p(0,0) = 0.4$
        \end{minipage}
        \begin{minipage}{0.2\textwidth}
            $p(0,1) = 0.2$
        \end{minipage}
        \begin{minipage}{0.2\textwidth}
            $p(1,0) = 0.1$
        \end{minipage}
        \begin{minipage}{0.2\textwidth}
            $p(1,1) = 0.3$
        \end{minipage}
    \end{center}
    Calcolare la massa di X condizionata da Y = 1
    \[ P(Y = 1) = \sum_{x}^{} p(x, 1) = p(0,1) + p(1,1) = 0.5 \]
    \paragraph{Quindi:}
    \[ P(X = 0 | Y = 1) = \frac{p(0,1)}{P(Y = 1)} = \frac{2}{5} \]
    \[ P(X = 1 | Y = 1) = \frac{p(1,1)}{P(Y = 1)} = \frac{3}{5} \]
    Se X e Y sono variabili congiuntamente continue, non è possibile utilizzare la
    definizione di distribuzione condizionata valida per quelle discrete, infatti sappiamo
    che P(Y = y) = 0 per tutti i valori di y


    \subsection{funzione di densità condizionata (Continue)}
    \formula
    \[ f_{X|Y}(x | y) = \frac{f(x, y)}{f_Y(y)}\]
    Se X e Y sono congiuntamente continue e A è un sottoinsieme di numeri reali per ogni y si può definire:
    \[ P(X \in A | Y = y) := \int_{A}^{} f_{X|Y}(x | y) \, dx \]

    Notiamo che X e Y sono indipendenti allora:
    \begin{center}
        \begin{minipage}{0.45 \textwidth}
            $ f_{X|Y}(x,y) = f_X(x) $
        \end{minipage}
        \begin{minipage}{0.45 \textwidth}
            $ P(X \in A | Y = y) = P(X \in A) $
        \end{minipage}
    \end{center}


    \paragraph{Esempio:} è data la seguente densità congiunta di X e Y



    \begin{equation*}
        f(x,y) =
        \begin{cases}
            \frac{12}{5} x(2-x-y) & 0 < x < 1, 0 < y < 1 \\
            0 & altrimenti
        \end{cases}
    \end{equation*} \\
    \linebreak[2]
    Si calcoli la densità condizionata di X rispetto a Y = y per 0 < y < 1. \\
    Se questi due numeri sono compresi tra 0 e 1 abbiamo che:

    \begin{equation*}
        \begin{split}
            f_{X|Y}(x,y):&= \frac{f(x,y)}{f_Y(y)} \\
            & = \frac{f(x, y)}{\int_{-\infty}^{\infty} f(x', y) \, dx'} \\
            & = \frac{x(2-x-y)}{\int_{0}^{1} x'(2-x'-y) \, dx'} \\
            & = \frac{x(2-x-y)}{\frac{2}{3} - \frac{y}{2}} \\
            & = \frac{6x(2-x-y)}{4-3y}
        \end{split}
    \end{equation*}


    \newpage
    \section{Valore atteso}
    \definizione Rappresenta la media pesata dei valori di una variabile aleatoria

    \subsection{Funzione di massa (Discrete)}
    \[ \ev{X} := \sum_{i}^{} x_i P(X = x_i) \]
    Si può dire quindi che il valore atteso è anche detto \textit{media} di X oppure \textit{aspettazione}

    \paragraph{Esempio semplice:} Se X è una variabile aleatoria con funzione di massa
    \[ p(0) = \frac{1}{2} = p(1) \]
    Allora:
    \[ \ev{X} = 0 \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} = \frac{0+1}{2} = \frac{1}{2}\]

    \paragraph{Esempio dado fair 6 facce $P(x_i = i) = \frac{1}{6}$}
    \[ \ev{X} = \sum_{i=1}^{6} i \frac{1}{6} = \frac{1}{6} \sum_{i=1}^{6} i = \frac{21}{6} = \frac{7}{2} = 3.5 \]
    Oppure:
    \[ \ev{X} := 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = \frac{7}{2} = 3.5 \]
    \text{dove il risultato è la media dei valori che X può assumere}

    \paragraph{Se N è molto grande} allora \textbf{$N_i \approx N_p(x_i)$}
    \[ \sum_{i}^{n} x_i p(x_i) \approx \sum_{i}^{n} x_i \frac{n_i}{n}\]
    
    
    \subsection{Funzione di densità (Continue)}
    \formula \[ \ev{X} := \int_{-\infty}^{+\infty} x f(x) \, dx \]
    \paragraph{Esempio:} Siamo in attesa di una comunicazione che deve arrivare dopo le ore 17. \\
    a partire dalle 17 è una variabile aleatoria con funzione di densità data da:
    \begin{equation*}
        f(x) = \\
        \begin{cases}
            \frac{1}{1.5} & \text{se 0 < x < 1.5} \\
            0 & \text{altrimenti}
        \end{cases}
    \end{equation*}
    Il valore atteso del tempo che trascorre tra le 17 e il momento di arrivo della comunicazione è quindi:
    \[ \ev{X} = \int_{0}^{1.5} \frac{x}{1.5} \, dx = 0.75\]

    \subsection{Valore atteso di una funzione}
    \definizione è possibile calcolare il valore atteso di una funzione g(X) notando che essa stessa è una variabile aleatoria \\
    quindi si applicano le stesse proprietà, come segue: \\
    \linebreak[4]

    \begin{minipage}{0.45\textwidth}
        \text{Variabile discreta:}
        \begin{equation*}
            \ev{g(X)} = \sum_{i}^{} g(x_i) p(x_i)
        \end{equation*}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \text{Variabile Continua:}
        \begin{equation*}
            \ev{g(x)} = \int_{-\infty}^{+\infty} g(x) f(x) \, dx
        \end{equation*}
    \end{minipage}

    \paragraph{Esempio (discrete):} quanto vale il valore atteso del quadrato di una variabile X con le seguenti funzioni di massa?
    \begin{equation*}
        \begin{minipage}{0.3\textwidth}
            p(0) = 0.2
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
            p(1) = 0.5
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
            p(2) = 0.3
        \end{minipage}
    \end{equation*} \\
    Se poniamo $Y := X^2$ questa diventa una variabile che può assumere i valori $0^2$, $1^2$, $2^2$
    \[ p_Y(0) := P(Y = 0^2) = 0.2 \]
    \[ p_Y(1) := P(Y = 1^2) = 0.5 \]
    \[ p_Y(4) := P(Y = 2^2) = 0.3 \]
    Quindi:
    \[ \ev{X^2} = \ev{Y} = 0 \cdot 0.2 + 1 \cdot 0.5 + 4 \cdot 0.3 = 1.7 \]
    Oppure (utilizzando la proposizione delle variabili discrete)
    \[ \ev{X^2} = 0^2 \cdot 0.2 + 1^2 \cdot 0.5 + 2^2 \cdot 0.3 = 1.7\]

    \paragraph{Esempio (continue):} Il tempo – in ore – necessario per localizzare un guasto nell’impianto
    elettrico di una fabbrica è una variabile aleatoria X con funzione di densità
    \begin{equation*}
        f(x) =
        \begin{cases}
            1 & 0 < x < 1 \\
            0 & \text{altrimenti}
        \end{cases}
    \end{equation*}
    Se il danno economico provocato da una interruzione di x ore è $x^3$, qual è il valore
    atteso di questo costo? \\
    \paragraph{Applicando la proposizione della variabile continua} possiamo ottenere quanto segue:
    \[ \ev{X^3} = \int_{0}^{1} x^3 \, dx = \frac{1}{4} \]

    \subsection{Dimostrazioni}
    \text{Sia per discreto che per continuo si applicano le seguenti proprietà:}
    \[ \ev{aX+b} = a \ev{X} + b \]
    Se proviamo a ponere a = 0 scopriamo che:
    \[ \ev{b} = b \]
    Se proviamo a ponere b = 0 scopriamo che:
    \[ \ev{aX} = a \ev{X} \]
    Ovvero, il valore atteso di un fattore costante moltiplicato per una variabile aleatoria,
    è pari alla costante per il valore atteso della variabile aleatoria.

    \paragraph{Per caso discreto:}
    \begin{equation*}
        \begin{split}
            \ev{aX + b} & = \sum_{x}^{} (ax + b) p(x) \\
            & = a \sum_{x}^{} xp(x) + b \sum_{x}^{} p(x) \\
            & = a\ev{X} + b
        \end{split}
    \end{equation*}

    \paragraph{Per caso continuo:}
    \begin{equation*}
        \begin{split}
            \ev{aX + b} & = \int_{-\infty}^{+\infty} (ax + b) f(x) \, dx \\
            & = a \int_{-\infty}^{+\infty} xf(x) \,dx + b \int_{-\infty}^{+\infty} f(x) \, dx \\
            & = a\ev{X} + b
        \end{split}
    \end{equation*}

    \subsection{Momenti N-esimi nel valore atteso}
    \definizione se n = 1,2 ... n, la quantità $\ev{X^n}$ se esiste viene detta \textit{momento n-esimo} della variabile aleatoria X. \\
    \linebreak[4]
    \centerline{è possibile applicare le formule di prima, come segue:}
    \begin{equation*}
        \ev{X^n} =
        \begin{cases}
            \sum_{x}^{} x^n p(x) & \text{se X è discreta} \\
            \int_{-\infty}^{+\infty} x^n f(x) \, dx & \text{se X è continua}
        \end{cases}
    \end{equation*}

    \subsection{Valore atteso della somma di due variabili}
    \definizione è possibile applicare le formule viste sopra anche quando abbiamo due variabili aleatorie \\
    \linebreak[4]
    se in questo caso $\boldsymbol{\ev{g(X,Y)}}$ esiste allora: \\
    \begin{equation*}
        \ev{g(X,Y)} =
        \begin{cases}
            \sum_{x}^{} \sum_{y}^{} g(x,y) p(x,y) & \text{Se discreto} \\
            \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x,y) f(x,y) \, dx \, dy & \text{Se continuo}
        \end{cases}
    \end{equation*}
    \linebreak[4]
    % \[ X, Y -> g(X,Y) - \boldsymbol{g: \mathbb{R}^2 \rightarrow \mathbb{R}} \]
    \text{se $g(X,Y)$ come $\boldsymbol{g = X + Y}$ allora}
    \[ \ev{X + Y} = \ev{X} + \ev{Y} \]

    \paragraph{Dimostrazione caso discreto:}
    \begin{equation*}
        \begin{split}
            \ev{X + Y} &= \sum_{x}^{} \sum_{y}^{} (x + y) p(x, y) \\
            & = \sum_{x}^{} x \cdot [\sum_{j}^{} p(x_i, y_j)] + \sum_{x}^{} y \cdot [\sum_{i}^{} p(x_i, y_j)] \\
            & = \sum_{x}^{} xp_X(x) + \sum_{y}^{} yp_Y(y) \\
            & = \ev{X} + \ev{Y}
        \end{split}
    \end{equation*}

    \paragraph{Dimostrazione caso continuo:}
    \begin{equation*}
        \begin{split}
            \ev{X + Y} &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x + y) f(x, y) \, dx \, dy \\
            & = \int_{-\infty}^{\infty} x [\int_{-\infty}^{\infty} f(x,y) \, dy] \, dx + \int_{-\infty}^{+\infty} y [ \int_{-\infty}^{+\infty} f(x,y) \, dx ] \, dy \\
            & = \int_{-\infty}^{\infty} x f_X(x) \, dx + \int_{-\infty}^{\infty} yf_Y(y) \, dy \\
            & = \ev{X} + \ev{Y}
        \end{split}
    \end{equation*}

    \paragraph{è possibile applicare la ricorsione per il numero di variabili aleatori}
    \begin{equation*}
        \begin{split}
            \ev{X + Y + Z} &= \ev{(X + Y) + Z} \\
            & = \ev{X + Y} + \ev{Z} \\
            & = \ev{X} + \ev{Y} + \ev{Z}
        \end{split}
    \end{equation*}

    \paragraph{In generale per ogni n}
    \[ \ev{X_1 + X_2 + \cdots + X_n} = \ev{X_1} + \ev{X_2} + \cdots \ev{X_n} \]


    \paragraph{Esempio: 2 dadi a 6 facce}
    \begin{equation*}
      \begin{split}
        \ev{X + Y} & = \ev{X} + \ev{Y} \\
        & = \sum_{i = 1}^{6} x_i p(x_i) + \sum_{i = 1}^{6} y_i p(y_i) \\
        & = \sum_{i = 1}^{6} x_i \frac{1}{6} + \sum_{i = 1}^{6} y_i \frac{1}{6} \\
        & = \frac{7}{2} + \frac{7}{2} = 7 
      \end{split}  
    \end{equation*}
    
    \centerline{Dove \textbf{7} è il valore atteso della somma dei due dadi.}
    Se vogliamo predire il valore di X possiamo scegliere un numero che sarà ugual ad X.
    L'errore che commeteremo sarà di $\boldsymbol{(X - c)^2}$ \\

    Se \boldsymbol{$c = \ev{X}$} l'errore sarà minimizzato $\boldsymbol{\mu := \ev{X}}$
    \[ \ev{(X - c)^2} \geq \ev{(X - \mu)^2}\]

    \section{Varianza}
    \definizione Indica di quanto i dati si discostano dalla media al quadrato \\
    \begin{minipage}{0.45\textwidth}
        \[ \mu = \ev{X} \leftarrow \text{Primo momento}\]
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \[ \ev{X^2} \leftarrow \text{Momento secondo}\]
    \end{minipage}

    \formula \[ Var(X) := \ev{(X - \mu)^2} \]

    \paragraph{Generalizzazione:}
    \begin{equation*}
        \begin{split}
            Var(X) & = \ev{(X - \mu)^2} \\
            & = \ev{X^2 - 2 \mu \cdot X + \mu^2} \\
            & = \ev{X^2} - 2 \mu \cdot \ev{X} + \mu^2 \\
            & = \ev{X^2} - \mu^2 \longrightarrow \ev{X^2} - \ev{X}^2
        \end{split}
    \end{equation*}
    
    \paragraph{Esempio: Varianza di un dado}
    \begin{equation*}
        \begin{split}
            \ev{X^2} & = \sum_{1}^{6} i^2 P(X = i) \\
            & = 1^2 \cdot \frac{1}{6} + 2^2 \cdot \frac{1}{6} + 3^2 \cdot \frac{1}{6} + 4^2 \cdot \frac{1}{6} + 5^2 \cdot \frac{1}{6} + 6^2 \cdot \frac{1}{6} \\
            & = \frac{91}{6}
        \end{split}
    \end{equation*} \\

    Sapendo che $\ev{X} = \frac{7}{2}$
    \[ Var(X) = \ev{X^2} - \ev{X}^2 = \frac{91}{6} - (\frac{7}{2})^2 = \frac{35}{12} \]

    \subsection{Costanti reali nella varianza}
    Una utile identità che riguarda la varianza è la seguente (per ogni coppia di costanti reali a e b)
    \[ Var(aX + b) = a^2 \cdot Var(X) \] \\

    Per dimostrare ciò ricordiamoci sempre di $\boldsymbol{\mu := \ev{X}}$
    \paragraph{Dimostrazione:}
    \begin{equation*}
        \begin{split}
            Var(aX + b) :&= \ev{(aX + b - \ev{aX + b})^2} \\
            & = \ev{(aX + b -a\mu - b)^2} \\
            & = \ev{a^2(X-\mu)^2} \\
            & = a^2 \ev{(X - \mu)^2} \\
            & = a^2 Var(X)
        \end{split}
    \end{equation*}
    Se sostituiamo i valori di \textbf{a} e \textbf{b} troviamo che: \\
    SE $a = 0 \longrightarrow Var(b) = 0 \longrightarrow \text{le costanti hanno varianza \textbf{nulla}}$ \\
    SE $a = 1 \longrightarrow Var(X + b) = Var(X) \longrightarrow \text{sommando una const. non cambia la varianza}$ \\
    SE $b = 0 \longrightarrow Var(aX) = a^2 \cdot Var(X)$ \\
    

    \newpage
    \section{Deviazione Standard}
    \definizione Indica di quanto dei dati si \textbf{discotastano dalla media} (non al quadrato)

    \formula \[ S = \sqrt{Var(X)} \]

    \[ Var(X + X) = Var(2 \cdot X) = 4 \cdot Var(X) \]
    \linebreak[1]
    \textbf{Se X è indipendente allora:}
    \[ Var(X) + Var(X) = Var(X + X) \]

    \section{Covarianza}
    \definizione Misura la \textbf{variazione} tra due variabili aleatorie associate tra di loro
    \formula \[ \boldsymbol{Cov(X,Y) := \ev{(X - \mu_x) (Y - \mu_y)}} = \ev{XY} - \ev{X} \ev{Y} \]
    \textbf{Dove:} \\
    $ \mu_x = \ev{X} $ \\
    $ \mu_y = \ev{Y} $

    \begin{center} 
        \textit{La covarianza può essere negativa, positiva o nulla } 
    \end{center}
    \text{\textbf{Positivo} $\longrightarrow$ Le due variabili crescono o decrescono insieme} \\
    \text{\textbf{Negativo} $\longrightarrow$ Quando una variabile cresce l'altra decresce} \\
    \text{\textbf{Nullo} $\longrightarrow$ Le due variabili sono indipendenti} \\

    \subsection{Proprietà della covarianza}
    
    $ Cov(X, Y) = Cov(Y, X) \longleftarrow \text{Commutativo} $ \\
    $ Cov(X, Y) = \ev{XY} - \ev{X} \ev{Y} $ \\ 
    $ Cov(X, X) = Var(X) $ \\
    $ Cov(X+Y, Z) = Cov(X, Z) + Cov(Y, Z) $ \\
    $ Cov(X+Y, Z+W) = Cov(X, Z) + Cov(X, W) + Cov(Y, Z) + Cov(Y, W) $ \\
    $ Var(X + Y) = Var(X) + Var(Y) + 2 Cov(X, Y) $ \\
    
    \centerline{- Se $X_1 \cdots X_n$ e $Y_1 \cdots Y_n$ sono variabili aleatorie qualsiasi allora:}
    \[ Cov(\sum_{i = 1}^{n}x_i, \sum_{j = 1}^{m} y_j) = \sum_{i = 1}^{n} \sum_{j = 1}^{m} Cov(X_i, Y_j) \]
    \linebreak[10]
    \centerline{Se X e Y sono \textbf{indipendenti:}}
    \[ \ev{XY} = \ev{X} \ev{Y} \]
    \[ Cov(X, Y) = 0 \longrightarrow \text{se sono indipenti}\]

    
    \subsection{Coefficiente di correlazione lineare}
    \definizione numero puro che tiene conto della deviazione standard di X e Y
    \formula
    \begin{equation*}
        Corr(X,Y) := \frac{Cov(X,Y)}{\sqrt{Var(X) \cdot Var(Y)}}
    \end{equation*}

    \begin{center}
        La correlazione può assumere valori compresi tra \textbf{-1} e \textbf{1}
    \end{center}
    \text{\textbf{-1} $\longrightarrow$ Le due variabili sono inversamente proporizionali} \\
    \text{\textbf{0} $\longrightarrow$ Le due variabili sono indipendenti} \\
    \text{\textbf{1} $\longrightarrow$ Le due variabili sono crescono o decrescono con lo stesso rapporto} \\

    \newpage
    \section{Funzione generatrice dei momenti}
    \definizione Funzione che ci permette di calcolare i momenti della distribuzione.
    \formula \[ \phi(t) = \ev{e^{tX}}\]
    \begin{center}
        Dove X è una variabile aleatoria e t è un parametro reale
    \end{center}

    \begin{equation*}
        \phi(t) = \ev{e^{tX}} 
        \begin{cases}
            \sum_{x}^{} e^{tX}p(x) & \text{se X discreta} \\
            \int_{-\infty}^{+\infty} e^{tX} f(x) \, dx & \text{se X continua}
        \end{cases}
    \end{equation*}
    \linebreak[10]
    \centerline{Derivando la funzione si ottengono i momenti:}
    \[ \phi'(t) = \frac{d}{dt} \ev{e^{tX}} = \ev{\frac{d}{dt} e^{tX}} = \ev{Xe^{tX} \longrightarrow \phi'(0) = \ev{X}}\]

    \paragraph{Generalizzando:}
    \[ \phi^{n}(0) = \ev{X^n} \]
    \linebreak[2]

    \begin{minipage}{0.45\linewidth}
        Media:
        \[\mu_x = \phi'(0)\]
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\linewidth}
        Varianza:
        \[ \sigma^{2}_x = \phi''(0) = \{\phi(0)'\}' \]
    \end{minipage}
    \subsection{Disugaglianza di Markov}
    \definizione Ci permette di sapere la probabilità che una variabile assuma valori molto grandi
    \definizione Serve per calcolare che una variabile aleatoria assuma un minimo di "a" $\longrightarrow a \in \mathbb{R}$
    \paragraph{Solo per variabili positive:} $X \in (0, +\infty) $
    \formula \[ P(X \geq a) \leq \frac{\ev{X}}{a}\]

    \paragraph{Generalizzazione:}
    \begin{equation*}
        \begin{split}
            \ev{X} & = \int_{0}^{+\infty} xf(x) \, dx \\
            & = \int_{0}^{a} x f(x) \, dx + \int_{a}^{+\infty} x f(x) \, dx \geq \int_{a}^{+\infty} x f(x) \, dx \geq \int_{a}^{+\infty} a f(x) \, dx \\
            & = a \int_{a}^{+\infty} f(x) \, dx \\ 
            & = \boldsymbol{aP(X \geq a)}
        \end{split}
    \end{equation*}
    
    \subsection{Disugaglianza di Chebyshev}
    \definizione Ci permette di sapere la probabilità che una variabile si discosti dalla media per più di un certo numero di deviazioni standard.
    \begin{equation*}
        X \text{var aleatoria}
        \begin{cases}
            \mu & \text{Media} \\
            \sigma^2 & \text{Varianza}
        \end{cases}
    \end{equation*}

    \centerline{Per ogni \textbf{r > 0} $\longrightarrow$ valore che indica il discostamento dalla media}
    \formula 
    \[ P(|X - \mu| \geq r) \leq \frac{\sigma^2}{r^2} \]
    \centerline{Serve per ottenere le stime di probabilità di eventi rari di variabili} \\
    \centerline{cui conosciamo solo la \textbf{media} e la \textbf{varianza}}
    \section{Legge debole dei grandi numeri}
    \definizione Dice che la probabilità che la differenza tra la media campionaria e il valore atteso superi una determinata soglia diventa sempre più piccola all'aumentare del numero di osservazioni

    \newpage
    \section{Modelli di variabili aleatorie}

    \subsection{Bernoulli}

    \subsection{Binomiali}
    \subsubsection{Valore atteso e varianza di Binomiali}
    \subsubsection{Funzione di massa e di ripartizione di Binomiali}

    \subsection{Poisson}

    \subsection{Ipergeometriche}

    \subsection{Uniformi}

    \subsection{Normali o Gaussiane}

    \subsection{Esponenziali}

    \subsection{Processi stocastici (Poisson)}

    \subsection{Gamma}

    \subsection{Chi-quadro}

    \subsection{Distribuzione T}

    \subsection{Distribuzione F}

    \subsection{Distribuzione logistica}

\end{document}