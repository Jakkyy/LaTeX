\documentclass[]{article}

\usepackage[paperheight=18cm,paperwidth=14cm,textwidth=12cm]{geometry}
\usepackage[skip=20pt plus1pt, indent=40pt]{parskip}

\usepackage{hyperref}

\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{float}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{sectsty}
\definecolor{bittersweet}{rgb}{1.0, 0.44, 0.37}
\definecolor{grey}{rgb}{0.25, 0.25, 0.28}
\definecolor{black}{rgb}{0, 0, 0}
\subsubsectionfont{\color{black}}
\subsubsectionfont{\color{grey}}
\sectionfont{\color{bittersweet}}

\usepackage[T1]{fontenc}
\renewcommand\familydefault{\sfdefault} 

\usepackage{bm}

\newcommand{\ev}{\mathbb{E}[X]}
\renewcommand{\ev}[1]{\mathbb{E}[#1]}

\newcommand{\definizione}{\paragraph{Definizione:}}
\newcommand{\formula}{\paragraph{Formula generica:}}

\newcommand{\highlight}[1]{\colorbox{yellow}{$\displaystyle #1$}}

\begin{document}
    \tableofcontents
    \newpage
    
    \section{Introduzione}
    $X_1 = 1.7$ \\
    $X_2 = 1.82$ \\
    $X_3 = 1.73$\\
    $X_4 = 1.7$\\
    $X_5 = 1.8$ \\
    $\hat{\theta}? \quad \text{Altezza della popolazione}$
    \paragraph{Possibile soluzione}: 
    \[ \hat{\theta_a} = \frac{1}{n} \sum_{4}^{5} x_i = \frac{1.7 + 1.82 + 1.73 + 1.7 + 1.8}{5} = \frac{8.75}{5} = 1.75 \]
    \[ \hat{\theta_b} = \frac{min(x_i) + \max(x_i)}{2} = \frac{3.52}{2} = 1.76 \]
    \[ \hat{\theta_c} = \frac{1}{3} \sum_{2}^{4} x_i = \frac{1}{3} (1.8 + 1.73 + 1.7) = \frac{5.23}{3} = 1.743 \]
    \centerline{Scartiamo il più \textit{piccolo} e il \textit{massimo}, calcolando poi la \textbf{media} dei rimanenti}
    \paragraph{Stima parametrica}(Point) Parametric Estimation \\ \\
    \underline{Ipotesi}:
    - Esiste un parametro $\theta$ incognito $n$ dati a disposizione $\{X_1, X_2, X_n\}$ \\
    \textbf{Legge di probabilità} che descrive il fenomeno che ha generato i dati
    \formula Bayes
    \[ P(\theta / X_1 \ldots X_n) = \frac{P(X_1 \ldots X_n / \theta) P(\theta)}{P(X_1 \ldots X_n)} \]
    \centerline{Verosomiglianza (likelihood)}
    \paragraph{MLE} Maximum Likelihood Estimation (Stima a Massima Verosomiglianza)
    \[ \hat{\theta} = argmax L(\theta) = argmax[f(X_1 \ldots X_n / \theta )] \]
    \paragraph{Esempio} (Legge -> Distribuzione di Poisson)
    \begin{equation*}
        \begin{split}
            f(X_1, X_2 \ldots X_n / \theta) &= f(X_1 / \theta) \cdot f(X_2 / \theta) \ldots f(X_n / \theta) \\
            &= \frac{1}{\theta} e^{-\frac{X_1}{\theta}} \cdot \frac{1}{\theta} e^{-\frac{X_2}{\theta}} \cdot \ldots \frac{1}{\theta} e^{-\frac{X_n}{\theta}} \\
            &= \frac{1}{\theta n} e^{-\frac{1}{\theta} \sum_{i}^{} X_i }
        \end{split}
    \end{equation*}
    \paragraph{Esempio} (MLE Ipotesi di Bernoulli)
    \begin{equation*}
        X_i =
        \begin{cases}
            0 \\
            1
        \end{cases}
    \end{equation*}
    \[ P\{X_i = 1\} = 1 - P\{X_i = 0\} \]
    \[ P\{X_i = x\} = P^x (1-P)^x \quad x \in \{0, 1\} \]
    \centerline{Dove \textbf{X} è una \textit{variabile aleatoria} e \textbf{x} una \textit{variabile sperimentale} }
    \[ f(x_1 \ldots x_n / P) = P^{x_1} (1-P)^{1-x_1} \cdot P^{x_2} (1-P)^{1-x_2} \ldots P^{x_n} (1-P)^{1-x_n} = \]
    \[ P^{\sum_{i}^{n}x_i} (1-P)^{n - \sum_{1}^{n} x_i} \longrightarrow \text{Bisogna trovare il \textbf{massimo} della funzione} \]
    \begin{equation*}
        \begin{split}
            log(f(x_1 \ldots x_n / P)) &= \sum_{1}^{n} x_i log P - (n - \sum_{i}^{n} x_i) log(1-P) \\
            &= \frac{d}{dP}[log(f)] = 0 = \frac{1}{\hat{P}} \sum_{i}^{n} x_i - \frac{n - \sum_{i}^{} x_i }{(1-\hat{P})} \\
            &= (1- \hat{P}) \sum_{i}^{} x_i = \hat{P} (n - \sum_{i}^{} x_i) \\
            &= \hat{P} = \frac{\sum_{i}^{} x_i }{n} \quad \text{MLE}
        \end{split}
    \end{equation*}
    \paragraph{Esercizio 1} Probabilità che Oneto dia 30L (Lode) \\
    $n = 120$ \\
    $\sum_{i}^{120} x_i = 18$ \\
    $\hat{P} = \frac{18}{120} = 0.15 \rightarrow 15\%$ \\
    \paragraph{Esercizio 2} N studenti da 30 e lode \\
    $n_1 = 18 \leftarrow \text{Oneto}$ \\ 
    $n_2 = 20 \leftarrow \text{Anguita}$ \\
    $n_{1,2} = 10 \leftarrow \text{30L sia con Oneto che con Anguita}$ \\
    $N= ? \quad \text{Studenti da \textbf{30 e Lode}}$ \\
    \begin{minipage}{0.30\textwidth}
        \[ \hat{P_1} \approx \frac{n_1 2}{n_2} \]
    \end{minipage}
    \begin{minipage}{0.30\textwidth}
        \[ \hat{P_1} \approx \frac{n_1}{N} \]
    \end{minipage}
    \begin{minipage}{0.30\textwidth}
        \[ \frac{n_{1,2}}{n_2} = \frac{n_1}{N} \]
    \end{minipage} \\
    $ \Longrightarrow N= \frac{n_1 \cdot n_2}{n_{1,2}} \rightarrow \frac{18 \cdot 20}{10} = 36 $
    \paragraph{MLE POISSON}
    \begin{equation*}
        \begin{split}
            f(x_1, x_2 \ldots x_n / \lambda) &= \frac{e^{-\lambda} \lambda^{x_1}}{x_1 !} \cdot \frac{e^{-\lambda} \lambda^{x_2}}{x_2 !} \cdots \frac{e^{-\lambda} \lambda^{x_n}}{x_n !} \\
            &= \frac{e^{-n\lambda} \lambda^{\sum_{i}^{} x_i}}{x_1! x_2! \ldots x_n!}
        \end{split}
    \end{equation*}
    \formula
    $\lambda = \frac{\sum_{i}^{} x_i}{\lambda} \quad \text{MLE} $
    \paragraph{Esercizio 3} Stima del numero di incidenti medio in auto n = 10 \\
    $x_1 = \{ 4,0,6,5,2,1,2,0,4,3 \}$ \\
    $\hat{\lambda} = \frac{\sum_{i}^{} x_i}{n} = \frac{27}{10} = 2.7$
    \[ P\{x \leq 2 \} = e^{-2.7} (\frac{2.7^0}{0!} + \frac{2.7^1}{1!} + \frac{2.7^2}{2!}) \approx .4936 \rightarrow 49.36\% \]
    \centerline{Probabilità che non ci siano più di \textbf{2 incidenti} }
    \paragraph{MLE UNIFORME}
    \begin{equation*}
        f(x_1, x_2 \ldots x_n / \theta) =
        \begin{cases}
                \frac{1}{\theta^n} & \quad 0 < x_i < \theta \\
                0 & \quad \text{altrimenti}
        \end{cases}
    \end{equation*}
    $\hat{\theta} = \max\{x_i\}$ \\
    $\frac{\hat{\theta}}{2} = \frac{\max\{ x_i\}}{2}$
    \paragraph{MLE GAUSSIANA}
    \[ f(x_1,x_2 \ldots x_n / \mu, \sigma) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi} \sigma} e^{\frac{-(x_1 - \mu)^2}{2 \sigma^2}} \]
    \[ (\frac{1}{2 \pi})^{\frac{n}{2}} \frac{1}{\sigma^n} e^{\frac{-\sum_{i}^{}(x_i - \mu)^2}{2 \sigma}} \]
    $log[f] = - \frac{n}{2} log 2\pi - n log \sigma - \frac{\sum_{i}^{}(x_i - \mu)^2}{2\sigma^2}$ \\ \\
    $\frac{d log f}{d \mu} = 0 = \frac{\sum_{i}^{}(x_i - \mu)^2}{\sigma^2} \longrightarrow \hat{\mu} = \frac{\sum_{i}^{}x_i}{n}$ \\
    $\frac{d log f}{d \sigma} = 0 = - \frac{n}{\sigma} + \frac{\sum_{i}^{} (x_i - \mu)^2}{4 \sigma^4} \rightarrow \sigma = \sqrt{\frac{\sum_{i}^{}(x_i - \mu)^2}{n}}$
    \paragraph{Esercizio} primo \\
    $x_1 = 1.7$ \\
    $x_2 = 1.82$ \\
    $x_3 = 1.73$\\
    $x_4 = 1.7$\\
    $x_5 = 1.8$ \\
    \[ \hat{\mu} = \frac{\sum_{i}^{} x_i}{n} = \frac{1.7 + 1.82 + 1.73 + 1.7 + 1.8}{5} = 1.75 \]
    \[ \hat{\sigma} = \sqrt{\frac{0.05^2 + 0.07^2 + 0.02^2 + 0.05^2 + 0.05^2}{5}} \approx 0.051 \]
    \paragraph{Intervalli di confidenza} normali
    TODO
    \paragraph{Intervalli di confidenza} gaussiani
    $\sigma^2$ Nota \\
    $x_1m x_2 \ldots x_n$ \\
    $\hat{\mu} \longleftarrow \mu$ \\
    $\frac{\overline{x} - \mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0,1)$ \\
    \[ P(-1.96 < \frac{\overline{x} - \mu}{\frac{\sigma}{\sqrt{n}}} < +1.96) = 0.95 \]
    \[ \longrightarrow P(-1.96 \frac{\sigma}{\sqrt{n}} < \overline{x} - \mu < 1.96 \frac{\sigma}{\sqrt{n}}) \]
    \[ P(\overline{x} - 1.96 \frac{\sigma}{\sqrt{n}} < \mu < \overline{x} + 1.96 \frac{\sigma}{\sqrt{n}}) \]
    \paragraph{Esempio:} Sistema di comunicazione
    $\sigma^2 = 4 \quad n = 9$
    \[ x_1 = \{ 5.85, 12, 15, 7, 9, 7.5, 6, 5, 10.5 \} \]
    \[ \hat{\mu} = \frac{1}{n} \sum_{i}^{n} x_i = \frac{1}{9} \sum_{i}^{n} x_i = \frac{81}{9} = 9 \] \\
    \begin{equation*}
        \begin{aligned}
            & P\left(9-1.96 \frac{\sigma}{\sqrt{m}}<\mu<9+1.96 \frac{\sigma}{\sqrt{m}}\right)=0.95 \\
            & p\left(9-1.96 \frac{2}{3}<\mu<9+1.96 \frac{2}{3}\right)=0.95 \\
            & \longrightarrow[7.693,10.31] \rightarrow \mu \text { si trova tra 7.693 e 10.31} \\
        \end{aligned}
    \end{equation*}
    \paragraph{In generale} Prob = $1-\alpha$
    \[ (\overline{x} - z_a \frac{\sigma}{\sqrt{n}}, \overline{x} + z_a \frac{\sigma}{\sqrt{n}} ) \rightarrow \text{Si rileva dalle tavole} \]
    \subsection{Intervalli di confidenza (Bilaterali)}
    \[ \overline{X} = \frac{1}{n} \sum_{i}^{n} x_i \]
    \[ x_i \sim \mathcal{N}(\mu, \sigma^2) \]
    \[ \overline{X} \sim(\mu, \frac{\sigma^2}{n}) \]
    \[ \mathcal{Z} = \frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0, 1) \quad Var(\frac{x}{2}) = \frac{1}{\sigma^2} Var(x) \]
    \text{Supponiamo che $\sigma$ sia nota:}
    \begin{equation*}
        \begin{aligned}
            &\begin{aligned}
            & \operatorname{Pr}\left\{-z_{\frac{\alpha}{2}}<Z<+z_{\frac{\alpha}{2}}\right\}=1-\alpha \\
            & \operatorname{Pr}\left\{-z_{\frac{\alpha}{2}}<\frac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{m}}}<+z_{\frac{\alpha}{2}}\right\}=1-\alpha \\
                & \operatorname{Pr}\left\{-z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{m}}<\bar{x}-\mu<+z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{m}}\right\}
        \end{aligned}\\
        &\begin{aligned}
        & \operatorname{Pr}\left\{-\bar{x}-z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{m}}<-\mu<-\bar{x}+z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{m}}\right\}= \\
        & \operatorname{Pr}\left\{\bar{x}-z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{m}}<\mu<\bar{x}+z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{m}}\right\}=1-\alpha
        \end{aligned}
        \end{aligned}
    \end{equation*}
    \subsection{Intervalli di confidenza (Unilaterali)}
    \begin{equation*}
        \begin{aligned}
        & \operatorname{Pr}\left\{z<z_\alpha\right\}=1-\alpha \\
        & \operatorname{Pr}\left\{\frac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{m}}}<z_\alpha\right\}=1-\alpha \\
        & \operatorname{Pr}_r\left\{\bar{x}-\mu<z_\alpha \frac{\sigma}{\sqrt{m}}\right\}=1-\alpha \\
        & \operatorname{Pr}\left\{-\mu<-\bar{x}+z_\alpha \frac{\sigma}{\sqrt{m}}\right\}=1-\alpha \\
        & \operatorname{Pr}\left\{\bar{x}-z_\alpha \frac{\sigma}{\sqrt{m}}<\mu\right\}=1-\alpha \\
        & \mu \in\left(\bar{x}-z_\alpha \frac{\sigma}{\sqrt{m}},+\infty\right)
        \end{aligned}
    \end{equation*}
    \subsection{Esempio:} Pesca stagionale dei salmoni (\textit{Fisso intervallo -> trovo $n$}) \\
    \text{Ad ogni stagione il peso medio dei salmoni è diverso ma $\sigma = 0.3$ Kg} \\
    \text{Intervallo di confidenza al 95\%, quindi $\alpha = 0.05$}
    \[ (\overline{X} - 1.96 \frac{\sigma}{\sqrt{n}}, \overline{X} + 1.96 \frac{\sigma}{\sqrt{n}}) \]
    \[ 1.96 \frac{\sigma}{\sqrt{n}} \geq 0.1 \quad \sqrt{n} \geq \frac{1.96}{0.1} \sigma \]
    \[ n \geq (\frac{1.96}{0.1} 0.3)^2 = 5.88^2 \approx 34.6 \leftarrow \text{salmoni} \]
    \subsection{Intervallo di confidenza} con \textit{media} e \textit{varianza} \textbf{incognite}
    \begin{equation*}
        \begin{aligned}
            & Z=\frac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{n}}} \sigma \quad \text{ Non nota} \\
            & s^2=\frac{1}{n-1} \sum_i\left(x_i-\bar{x}\right)^2=\frac{1}{n-1} \sum_i^n\left(x_i^2-n \bar{x}^2\right) \\
            & =\frac{1}{n-1} \sum_i\left(x_i^2+\bar{x}^2-2 x_i \bar{x}\right) \\
            & =\frac{1}{n-1} \sum_i x_i^2+\frac{n \bar{x}^2}{n-1}-2 \bar{x} \frac{\bar{x} n}{n-1} \\
        \end{aligned}
    \end{equation*}
    \[ T = \frac{\overline{X} - \mu}{\frac{s}{\sqrt{n}}} \sim T_n - 1 \quad \text{(T studenti con n gradi di libertà)} \]
    \paragraph{Esempio:} Trasimttente ($\mu$) e ricevitore ($\mu$ + rumore)
    \[ 95\% (7.69, 10.31) \quad \hat{\mu} = 9, \sigma^2 = 4 \]
    $X_i \{ 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5 \}$ \\
    $\hat{\mu} = \overline{X} = \frac{1}{9} \sum_{i}^{n} X_i = \frac{81}{9} = 9$ \\
    $s^2 = \frac{1}{8}\sum_{i}^{}(X_i^2-9.81) \approx 9.5 \quad s = 3.082$
    \[ \mu \in (9-2.306 \frac{3.082}{3}, 9 + 2.306 \frac{3.082}{3}) = (6.63, 11.37) \]
    \centerline{Si può dimostrare che $T_{\frac{\alpha}{2} \cdot n - 1} \ev{S} \geq z_\alpha\sigma$}
    \subsection{Integrali Monte Carlo}
    \[ \theta = \ev{f(u)} = \int_{-\infty}^{+\infty} f(u) p(u) \, du = \int_{-\infty}^{+\infty} f(u) \, du \]
    \paragraph{Esempio}: \\
    $\int_{0}^{1} \sqrt{1-x^2} \,dx = ?$
    $\ev{\sqrt{1-x^2}} \quad n = 100$ \\
    $X_i = \sqrt{1 - U_i^2} \quad X = \{ X_1, X_2 \ldots X_100 \}$ \\
    $\hat{\theta} = \overline{X} \pm t_{\frac{\alpha}{2}}, 99 \frac{s}{\sqrt{100}} \rightarrow \text{Per vedere se il risultato è corretto \textit{(confidenza)}}$ 
    \subsection{Intervallo di confidenza di Bernoulli}
    n esperimenti \\
    Binomiale \\
    media np \\
    varianza np(1-p) \\
    \[ \hat{P} = \frac{1}{n} \sum_{i}^{n}X_i \quad X_i \in \{ 0, 1 \} \] \\
    \[ X = n\hat{P} \quad P_r \{ -z_{\frac{\alpha}{2}} < z < z_{\frac{\alpha}{2}} \approx 1-\alpha \}\] \\
    \centerline{Dove z = $\frac{X-np}{\sqrt{np(1-p)}}$}
    \[ \frac{x-nP}{\sqrt{nP(1-P)}} \sim \mathcal{N}(0,1)\]
    \begin{equation}
        \begin{aligned}
        & \rho_r\left\{-z_{\frac{a}{2}}<\frac{x-m p}{\sqrt{m p(1-\hat{p})}}<z_{\frac{a}{2}}\right\} \cong 1-\alpha \\
        & \rho_r\left\{\hat{p}-z_{\frac{a}{2}} \sqrt{\frac{p(1-p)}{m}}<\mu<\hat{p}+z_{\frac{\alpha}{2}} \sqrt{\frac{\hat{p}(1-p)}{m}}\right\} \simeq 1-\alpha
        \end{aligned}
    \end{equation}

    \section{Intervalli di confidenza}
    \paragraph{Se $\sigma^2$ è nota} allora:
    \[ X_i \sim \mathcal{N}(\mu, \sigma^2) \quad \overline{X} = \frac{1}{n} \sum_{i}^{n} X_i \]
    \begin{equation*}
        \begin{aligned}
            \mu \in (-\infty, \overline{X} + z_\alpha \frac{\sigma}{\sqrt{n}}) \\
            \mu \in (\overline{X} - z_{\frac{\sigma}{\sqrt{n}}}, \overline{X} + z_{\frac{\sigma}{\sqrt{n}}}) \quad p_r (1-\alpha) \\
            \mu \in (-\infty, \overline{X} + z_\alpha \frac{\sigma}{\sqrt{n}}) \\
            \mu \in (\overline{X} - z_\alpha \frac{\sigma}{\sqrt{n}}, \infty)
        \end{aligned}
    \end{equation*}
    $s^2 = \frac{1}{n - 1} \sum_{i}{} (X_i - \overline{X})$ \\
    Se $\sigma^2$ è ignota allora: \\
    \[ \mu \in (\overline{X} - z_{\frac{\alpha}{2}}, n - 1 \frac{s}{\sqrt{n}}) \quad \sigma^2 \rightarrow s^2 = z \rightarrow t \]
    \subsection{Intervallo di confidenza nella varianza}
    \[ (n-1) \frac{S^2}{\sigma^2} \sim \mathcal{X}^2 \quad X_i \sim \mathcal{N}(\mu, \sigma^2) \]
    \begin{equation*}
        \begin{aligned}
            p_r \left \{ \mathcal{X}_{1- \frac{\alpha}{2}, n - 1}^2 \leq (n-1)\frac{s^2}{\sigma^2} \leq \mathcal{X}_{\frac{\alpha}{2}, n-1} \right \} \\
            p_r \left \{ \frac{s^2(n-1)}{\mathcal{X}_{\frac{\alpha}{2}, n-1}} \leq \sigma^2 \leq \frac{s^2(n-1)}{\mathcal{X}_{1- \frac{\alpha}{2}, n - 1}^2} \right \} = 1-\alpha \\
            \sigma^2 \in \left (\frac{s^2(n-1)}{\mathcal{X}_{\frac{\alpha}{2}, n-1}^2}, \frac{s^2(n-1)}{\mathcal{X}_{1- \frac{\alpha}{2}, n-1}}\right )  \quad p_r = 1- \alpha
        \end{aligned}
    \end{equation*}
    \paragraph{Esempio:} Laminatoio n = 4
    $X_i = \{ 0.123, 0.124, 0.126, 0.12 \} \text{spessore in \textbf{mm}}$ \\
    \paragraph{Svolgimento}
    \begin{equation*}
        \begin{aligned}
            \frac{1}{4} \sum_{i}^{4} X_i = \frac{0.493}{4} = 0.12325 \\
            \frac{1}{4-1} \sum_{i}^{4}\left ( X_i - 0.12325 \right )^2 = 1.875 \cdot 10^{-5} \\
            \sigma^2 \in \left ( \frac{s^2(n-1)}{9.348}, \frac{s^2(n-1)}{0.216} \right )
        \end{aligned} 
    \end{equation*}
    \centerline{Dove \textbf{9.348} e \textbf{0.216} sono ricavati dalle tabelle}
    Facciamo la radice: \\
    $\sigma \in (0.0014, 0.0093) \rightarrow 95\% $
    \subsection{Intervallo di confidenza}della differenza di due medie: \\ \\
    \begin{minipage}{0.5\textwidth}
        \centerline{\textbf{N} campioni}
        \[ X_i \sim \mathcal{N}(\mu_1, \sigma_1^2) \]
        \[ \overline{X} = \frac{1}{n} \sum_{i}^{n} X_i \]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centerline{\textbf{M} campioni}
        \[ Y_i \sim \mathcal{N}(\mu_2, \sigma_2^2) \]
        \[ \overline{Y} = \frac{1}{m} \sum_{i}^{m} Y_i \]
    \end{minipage}
    \[ \overline{X} - \overline{Y} \sim \mathcal{N}\left ( \mu_1 - \mu_2, \frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m} \right ) \]
    \[ \mathcal{N}(0,1) \sim \frac{\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}} \]
    $\mu_1 - \mu_2 \in \left ( \overline{X} - \overline{Y} - z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}, \overline{X} - \overline{Y} + z_{\frac{\alpha}{2}}, \overline{X} - \overline{Y} + z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}} \right )$ \\ \\
    Se $\sigma_1^2, \sigma_2^2$ non sono note: \\ \\
    \begin{minipage}{0.5\textwidth}
        $S_1^2 = \frac{1}{n-1} \sum_{i}^{n}(X_i - \overline{X})$ \\ \\
        $(n-1) \frac{S_1^2}{\sigma_1^2} \sim \mathcal{X}_{n-1}^2$
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        $s_2^2 = \frac{1}{m - 1} \sum_{i}^{m} (X_i - \overline{Y})$ \\ \\
        $(n-1) \frac{s_2^2}{\sigma_2^2} \sim \mathcal{X}_{n-1}^2$
    \end{minipage} \\ \\
    \textbf{Possiamo andare avanti solo se \boldsymbol{$\sigma_1^2 = \sigma_2^2 = \sigma^2$}}
    \[ (n-1) \frac{s_1^2}{\sigma^2} + (n-1) \frac{s_2^2}{\sigma^2} \sim \mathcal{X}_{n+m-2}^2 \]
    \[ \frac{\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}{\sqrt{\sigma^2 (\frac{1}{n} + \frac{1}{m})}} \longrightarrow \frac{\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}{\sqrt{S_p(\frac{1}{n} + \frac{1}{m})}} \]
    \[ \sim \mathcal{N}(0,1) \quad \quad \quad \quad \quad \sim T_{n+m-2}\]
    $S_p = \frac{(n-1) S_1^2 + (m-1) S_2^2}{n+m-2}$ \\ \\
    \centerline{Se $\sigma$ sono ignote ma uguali}
    $\mu_1 - \mu_2 \in (\overline{X} - \overline{Y} -T_{\frac{\alpha}{2},n+m-2} \sqrt{s^2(\frac{1}{n} + \frac{1}{m})})$ \\
    $\overline{X} - \overline{Y} + T_{\frac{\alpha}{2}, n+m-2} \sqrt{s^2 (\frac{1}{n} + \frac{1}{m})}$
    \subsection{Intervallo di previsione}
    $X_1, \ldots X_n, X_{n+1} \sim \mathcal{N}(\mu, \sigma^2)$ \\
    $\overline{X}_n = \frac{1}{n} \sum_{i}^{n} X_i \quad \overline{X}_n \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})$ \\
    $\overline{X}_n - X_{n+1} \sim \mathcal{N}(0, \sigma^2 + \frac{\sigma^2}{n}) \rightarrow (\mu - \mu, \sigma^2 + \frac{\sigma^2}{n})$ \\
    $\sigma^2(1 + \frac{1}{n})$
    \begin{minipage}{0.4\textwidth}
        $\frac{X_n - X_n + 1}{\sigma \sqrt{1 + \frac{1}{n}}} \sim \mathcal{N}(0,1)$
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        $s_n^2 = \frac{1}{n-1} \sum_{i}^{} (X_i - \overline{X}_n)^2$
    \end{minipage}
    \[ X_{n+1} \in(\overline{X}_n - T_{\frac{\alpha}{2}, n-1} s_n \sqrt{1 + \frac{1}{n}}, \overline{X}_n + T_{\frac{\alpha}{2}, n-1} s_n \sqrt{1 + \frac{1}{n}}) \longrightarrow P_r(1-\alpha) \]
    \paragraph{Esempio} smartwatch contapassi $\boldsymbol{n = 7}$ \\ \\
    \centerline{\begin{minipage}{0.3\textwidth}
        $LUN \quad 6922 \quad X_1$ \\
        $MAR \quad 5333 \quad X_2$ \\
        $MER \quad 7420 \quad X_3$ \\ 
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        $GIO \quad 7432 \quad X_4$ \\
        $VEN \quad 6252 \quad X_5$ \\
        $SAB \quad 7005 \quad X_6$ \\
    \end{minipage}} \\
    \[ DOM \quad 6752 \quad X_7 \]
    \[ \overline{X}_n = \frac{1}{n} \sum_{i}^{m} X_i = \frac{47016}{7} \approx 6717 \]
    \[ 1-\alpha = 95\% \quad \alpha = 5\% \]
    \[ t_{0.0025,6} = 2.997 \]
    \[ S_n = \sqrt{S_n^2} = 7.333.8 \]
    \[ x_{n+1} \in (6717 -2.447 \cdot 733397 \sqrt{1 + \frac{1}{7}}, 6717 + 2.447 \cdot 73397 \sqrt{1 + \frac{1}{7}}) \]
    $X_{n+1} \in (9796, 8637) \mu \in (6037, 7396)$
    \subsection{Qualità di uno stimatore}
    $X = X_1 \ldots X_n \quad \theta \leftarrow \text{parametro} \quad \quad d(x) \leftarrow \text{stimatore di $\theta$}$ \\
    \begin{minipage}{0.5\textwidth}
        \[ \boldsymbol{(d(x) - \theta)^2}\] Errore Quadratico (\textit{misura della qualità})
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \[ \boldsymbol{\ev{(d(x)-\theta)^2}} \] Errore Quadratico Medio (\textit{M.S.E})
    \end{minipage} \\
    Rischio $r(d, \theta) = \ev{(d-\theta)^2}$ 
    Lo stimatore "ottimo" sarà quello con il rischio minimo -> d con r minimo $\theta$
    \paragraph{Esempio}
    $d^* (x) = 4 \quad \text{se} \theta = 4 \Rightarrow d^* = \text{stimatore ottimo(per tutti gli altri valori non va bene)}$
    \subsection{Proprietà di uno stimatore}
    Def: $b_\theta(d) = \ev{d} - \theta \rightarrow \text{bias o polarizzazione}$
    Uno stimatore non è \textbf{polarizzato} se $b_\theta(d) = 0$
    \paragraph{Esempio}:
    $X_1 \ldots X_n \quad \theta \text{media}$ \\
    $d_1(X_1 \ldots X_n) = X_1$ \\
    $d_2(X_1 \ldots X_n) = \frac{X_1 + X_2}{2}$ \\
    $d_3(X_1 \ldots X_n) = \frac{X_1 + X_2 + \ldots X_n}{n}$
    \centerline{Tutti questi sono \textbf{unbiased}}
    \subsection{Stimatore unbaieseo}
    $r(d, \theta) = \ev{(d(x) - \theta)^2} = \ev{(d(x) - \ev{d(x)})^2} = Var(d)$ \\
    tra gli stimatori non polarizzati di ottimo è quello con la varianza minima
    \subsection{Valutazione di uno stimatore}
    \[ X = X_1 \ldots X_n \quad \theta = ? \]
    \centerline{Dove $\theta$ è un \textit{parametro} e $d(x)$ è uno \textit{stimatore} di $\theta$} \\ \\
    $r(d, \theta) \text{(mse) rischio} \quad \quad b_\theta(d) = \ev{d} - \theta$ \\
    se $b_\theta(d) = 0 \Rightarrow r(d, \theta) = Var(d)$ \\
    \centerline{se $b_\theta(d) \not = 0$ ? $r(d, \theta)$ = ?}
    \begin{equation*}
        \begin{split}
            r(d, \theta) = \ev{(d(x) - \theta)^2} &= \ev{(d(x) - \ev{d} + \ev{d} - \theta)^2} \\
            &= \ev{(d - \ev{d})^2 + (\ev{d} - \theta)^2 - 2(d - \ev{d}) (\ev{d} - \theta)} \\
            &= \ev{(d - \ev{d})^2} + \ev{(\ev{d} - \theta)^2} - 2(\ev{d} - \theta) \cdot \ev{(d- \ev{d})}
        \end{split}
    \end{equation*}
    \begin{equation*}
        \begin{split}
           r(d, \theta) &= \ev{(d - \ev{d})^2} + \ev{(\ev{d} - \theta)^2} \\
           &= Var(d) + b_\theta(d)^2 \leftarrow \text{bias$^2$}
        \end{split}
    \end{equation*}
    \subsection{Esempio:} Stimatore della media di una \textit{distribuzione uniforme} \\\\
    \begin{minipage}{0.4\textwidth}
        $\ev{X_i} = \theta / 2$ \\
        $X_1, X_2 \ldots X_n$
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        $d_1 = 2 \frac{1}{n} \sum_{i}^{n} X_i$ \\
        $d_2 = \max X_i$
    \end{minipage} \\[4ex]
    $d_1 : \ev{d_1} = \frac{2}{n} \sum_{i}^{} \ev{X_i} = \frac{2}{n} n \frac{\theta}{2} = \theta$ \\
    $r(d_1, \theta) = Var(d_1) = \frac{4}{n^2} n Var(X_i) = \frac{4}{n} \frac{\theta^2}{12} = \boldsymbol{\frac{\theta^2}{3n}} \Leftarrow$ \textit{Unbiased} \\[4ex]
    \begin{equation*}
        \begin{split}
            F_2(x) = P_r \{ d_2(x) \leq x \} &= P_r \{ \max X_1 \leq x \} \\
            &= P_r \{ X_1 \leq, \forall i \in 1 \} = \prod_{i = 1}^{n} P_r \{ X_i \leq x \} = (\frac{x}{\theta})^n \\
            f_2(x) &= \frac{d}{dx} F_2(x) = n \frac{x^{n-1}}{\theta^n} \quad x \leq \theta
        \end{split}
    \end{equation*} \\[6ex]
    $\displaystyle \ev{d_2} = \int_{0}^{\theta} x f_x(x) \, dx = \int_{0}^{\theta} \frac{n}{\theta^n} x^n \, dx = \frac{n}{\theta^n} \left[ \frac{x^{n+1}}{n+1} \bigg\rvert_{0}^{\theta}\right] = \frac{n}{n+1} \theta$ \\[2ex]
    $\displaystyle \ev{d_2^2} = \frac{n}{\theta^n} \int_{0}^{\theta} x^2 f(x) \, dx = \frac{n}{\theta^n} \int_{0}^{\theta} x^{n+1} \, dx = \frac{n}{\theta^n} \left[ \frac{x^{n+2}}{n+2} \bigg\rvert_{0}^{\theta}\right] = \frac{n}{n+2} \theta^2$ \\[2ex]
    $Var(d_2) = \ev{d^2} - \ev{d_2}^2 = \frac{n}{n+2}\theta^2 - \frac{n^2}{(n+1)^2} \theta^2 = \boldsymbol{\frac{n}{(n+2)(n+1)^2} \theta^2}$ \\[4ex]
    $r(d_2, \theta) = Var(d_2) + (\ev{d_2} - \theta)^2 = \boldsymbol{\frac{2 \cdot \theta^2}{(n+1)(n+2)}}$ \\[4ex]
    $n \geq 4 \quad r(d_2, \theta) < r(d_1, \theta) \quad \quad d_3 = \frac{n+1}{n} d_2$

    \paragraph{In sintesi}
    \begin{equation*}
        \begin{aligned}
            r(d_1, \theta) = \frac{\theta^2}{3n} \Leftarrow \text{Unbiased} \\
            r(d_2, \theta) = \frac{2\theta^2}{(n+1)(n+2)} \Leftarrow \text{Biased} \\
            r(d_3, \theta) = \frac{\theta^2}{n^2 + 2n} \Leftarrow \text{Unbiased} \\
            r(d_4, \theta) = \frac{\theta^2}{(n+1)^2} \Leftarrow \text{Biased}
        \end{aligned}
    \end{equation*}
    \section{Test di ipotesi}
    \paragraph{Ipotesi:} Affermazione rispetto a uno o più parametri di una distribuzione
    Ipotesi da confutare: $H_0$ (ipotesi nulla) \\
    \paragraph{Esempio}:
    \begin{equation}
        \begin{aligned}
            X_1 \ldots X_n \sim \mathcal{N}(\mu, \sigma^2) \\
            H_0 : \mu = 0 \\
            H_a : \mu \not = 0
        \end{aligned}
    \end{equation}
    \centerline{Diamo per scontato che l'ipotesi sia \textbf{vera}}
    \centerline{Dobbiamo cercare di \textit{confutarla}}
    \paragraph{Definizione} Regione critica tale che: \\\\
    $(X_1 \ldots X_n) \in C \rightarrow H_0 \text{è rifiutata}$ \\
    $(X_1 \ldots X_n) \not\in C \rightarrow H_0 \text{è accettata}$ \\
    $\alpha = $ Livello di \textbf{significatività} del test ($\alpha = 10\%, 5\% \ldots$)
    \paragraph{Procedimento}
    \begin{itemize}
        \item Fisso alpha
        \item Suppongo che $\alpha$ sia vera
        \item calcolo stima di $\mu$
        \item verifico che non sia "\textit{troppo distante}"
    \end{itemize}
    $X_1 \ldots X_n \sim \mathcal{N}(\mu, \sigma^2)$ \\
    $H_0 : \mu = \mu_0 \quad H_a : \mu \not = \mu_0$ \\
    $\overline{X} = \frac{1}{n} \sum_{i}^{} X_i $
    \paragraph{Regione critica}
    $\{ (X_1 \ldots X_n): \rvert \overline{X} - \mu_0 \rvert > c \}$ \\
    $P_{r_{\mu_0}} \left\{ | \overline{X} - \mu_0 | > c \right\} = \alpha$ \\
    $P_{r_{\mu_0}} \left\{ \frac{|\overline{X} - \mu_0|}{\frac{\sigma}{\sqrt{n}}} > \frac{c}{\frac{\sigma}{\sqrt{n}}} \right\} = \alpha$ \\
    $P_{r_{\mu_0}} \left\{ |z| > z_\alpha \right\} = \alpha$
    \paragraph{Esempio} (5 transimissioni) \\
    \begin{minipage}{0.45\textwidth}
        $\boldsymbol{n=5}$ \\
        $\overline{X} = 9.5$
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        $H_0 : \mu = 8$ \\
        $\alpha = 5\%$
    \end{minipage} \\[4ex]
    Ipotizzando che $H_0$ sia vera:
    \[ \frac{|\overline{X} - \mu|}{\frac{\sigma}{\sqrt{n}}} = \frac{|9.5 - 8|}{\frac{2}{\sqrt{5}}} \approx 1.68 \]
    Se: \\
    $\alpha = P_r$(rifiuto $H_0$ / $H_0$ vera) \\
    $\alpha \uparrow$ più \textit{"facile"} rifiutare l'ipotesi \\
    $\alpha \downarrow$ più \textit{"difficile"} rifiutare l'ipotesi
    \subsection{Metolodogia alternativa}
    \[ Ts = \frac{\overline{X} - \mu_0}{\frac{\sigma}{\sqrt{n}}} \rightarrow \text{Statistica di test} \]
    $P-$value = \textbf{probabilità} di ottenere un valore più "anomalo" di quello osservato
    \paragraph{Esempio:} $X_i \sim \mathcal{N}(\mu, 4)$ \\
    \begin{minipage}{0.5\textwidth}
        \[ n = 5 \]
        \[ \overline{X} = 8.5\]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \[ H_0 : \mu = 8 \]
        \[ H_a : \mu \not = 8\]
    \end{minipage}
    \[ \frac{|\overline{X} - \mu_0|}{\frac{\sigma}{\sqrt{n}}} = \frac{|8.5 - 8|}{\frac{2}{\sqrt{5}}} = \frac{\sqrt{5}}{2} 0.5 \approx 0.559 \]
    \[ P \{ |z| > 0.559 \} = 2P \{ z > 0.559 \} \approx 2 \cdot 0.288 = 0.579 \rightarrow \text{P-value}\] \\[2ex]
    Se $\boldsymbol{\overline{X} = 11.5}$:
    \[ \frac{|\overline{X} - \mu_0|}{\frac{\sigma}{\sqrt{n}}} = \frac{|11.5 - 8|}{\frac{2}{\sqrt{5}}} \approx 3.913 \]
    \[ P \{ |z| > 3.913 \} = 2P \{ z > 3.913 \} \leq 0.00005 \rightarrow \text{\underline{Rifiuto ipotesi $H_0$}}\]
    \subsection{Test di Hp unilaterale}
    \begin{minipage}{0.5\textwidth}
        \[ H_0 : \mu = \mu_0 (\mu \leq \mu_0) \]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \[ H_a : \mu > \mu_0 \]
    \end{minipage}
    \[ C = \{ (X_1 \ldots n) \cdot \overline{X} - \mu_0 > c \} \]
    \[ P_{r_{\mu_0}} \{ \overline{X} - \mu_0 > c \} = P_r \{ \frac{\overline{X} - \mu_0}{\frac{\sigma}{\sqrt{n}}} > \frac{c}{\frac{\sigma}{\sqrt{n}}}\} = P_{r_{\mu_0}} \{ z > z_a \} = \alpha \] \\[2ex]
    \centerline{Statistica test $\boldsymbol{\frac{\overline{X} - \mu_0}{\frac{\sigma}{\sqrt{n}}} \leq z_\alpha}$ accetto}
\end{document} 
