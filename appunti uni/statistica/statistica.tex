\documentclass[]{article}

\usepackage[paperheight=18cm,paperwidth=14cm,textwidth=12cm]{geometry}
\usepackage[skip=20pt plus1pt, indent=40pt]{parskip}

\usepackage{hyperref}

\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{float}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\def\mathcolor#1#{\@mathcolor{#1}}
\def\@mathcolor#1#2#3{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}
\usepackage[dvipsnames]{xcolor}
\usepackage{sectsty}
\definecolor{bittersweet}{rgb}{1.0, 0.44, 0.37}
\definecolor{grey}{rgb}{0.25, 0.25, 0.28}
\definecolor{black}{rgb}{0, 0, 0}
\subsubsectionfont{\color{black}}
\subsubsectionfont{\color{grey}}
\sectionfont{\color{bittersweet}}

\usepackage[T1]{fontenc}
\renewcommand\familydefault{\sfdefault} 

\usepackage{bm}

\newcommand{\ev}{\mathbb{E}[X]}
\renewcommand{\ev}[1]{\mathbb{E}\left[#1\right]}
\newcommand*{\diff}{\mathop{}\!\mathrm{d}}

\newcommand{\definizione}{\paragraph{Definizione:}}
\newcommand{\formula}{\paragraph{Formula generica:}}

\newcommand{\highlight}[1]{\colorbox{yellow}{$\displaystyle #1$}}

\begin{document}
    \tableofcontents
    \newpage 
    \section{Introduzione}
    In probabilità quello che facciamo noi è quello di supporre che le nostre distribuzioni siano \textbf{note}. \\
    in statistica facciamo il contrario, ossia dire qualcosa (anche detto \textit{fare dell'inferenza}) su \textbf{parametri sconosciuti}. \\
    Dato che i parametri sono scimage.pngonosciuti il massimo che possiamo fare è quello di ottenere \textit{una stima} dei parametri \textit{incogniti}. \\[2ex]
    Codesti signorini sono chiamati \textbf{stimatori puntuali} e sono indicati con il simbolo $\boldsymbol{\hat{\theta}}$ (in questo caso stiamo parlando di uno stimatore del parametro incognito $\boldsymbol{\theta}$) \\[2ex]
    Esisono anche gli \textit{stimatori non puntuali}, noti come \textbf{intervalli di confidenza}, ossia un intervallo di valori in cui può essere contenuto il \textit{dato incognito}.
    \paragraph{Esempio} $\hat{\theta}? \quad \text{Altezza della popolazione}$ \\[2ex]
    \begin{minipage}{0.49\textwidth}
        $X_1 = 1.7$ \\
        $X_2 = 1.82$ \\
        $X_3 = 1.73$
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        $X_4 = 1.7$ \\
        $X_5 = 1.8$ \\
    \end{minipage}
    \paragraph{Possibile soluzione}: 
    \[ \hat{\theta_a} = \frac{1}{n} \sum_{4}^{5} x_i = \frac{1.7 + 1.82 + 1.73 + 1.7 + 1.8}{5} = \frac{8.75}{5} = 1.75 \]
    \[ \hat{\theta_b} = \frac{min(x_i) + \max(x_i)}{2} = \frac{3.52}{2} = 1.76 \]
    \[ \hat{\theta_c} = \frac{1}{3} \sum_{2}^{4} x_i = \frac{1}{3} (1.8 + 1.73 + 1.7) = \frac{5.23}{3} = 1.743 \]
    \centerline{Scartiamo il più \textit{piccolo} e il \textit{massimo}, calcolando poi la \textbf{media} dei rimanenti}
    \section{MLE}
    \definizione Stima a Massima Verosomiglianza (Maximum Likelihood Estimation) \\
    Questa classe di stimatori sono molto usati in statistica, servono per determinare i migliori parametri del modello che si adattano ai dati e comparare molteplici modelli per \textit{determinare} quello che si adatta di più ai dati. \\
    Ad esempio la stima di massima verosomiglianza $\hat{\theta}$ è definita come il valore di $\theta$ che rende massima $\boldsymbol{f(x_1, x_2, \ldots, x_n \rvert \theta)} \rightarrow$ anche detta \textit{funziona di likelihood} \\[2ex]
    \textbf{Likelihood}: avendo dei dati quale è la probabilità che un certo modello descriva al meglio la natura dei nostri dati
    \[ \hat{\theta} = argmax L(\theta) = argmax[f(X_1 \ldots X_n / \theta )] \]
    \paragraph{Stima parametrica}(Point) Parametric Estimation \\ \\
    \underline{Ipotesi}:
    - Esiste un parametro $\theta$ incognito $n$ dati a disposizione $\{X_1, X_2, X_n\}$ \\
    \textbf{Legge di probabilità} che descrive il fenomeno che ha generato i dati
    \formula Bayes
    \[ P(\theta / X_1 \ldots X_n) = \frac{P(X_1 \ldots X_n / \theta) P(\theta)}{P(X_1 \ldots X_n)} \]
    \centerline{Verosomiglianza (likelihood)}
    \subsection{MLE di una Bernoulliana}
    Vengono realizzate \textit{n} prove indipendenti con probabilità $p$ di successo
    \begin{equation*}
        X_i =
        \begin{cases}
            1 & \text{se la prova i-esima ha successo} \\
            0 & \text{altrimenti}
        \end{cases}
    \end{equation*}
    La distribuzione dell $X_i$ è la seguente:
    \[ P(X_i = k) = p^k (1-p)^{1-k}, \qquad k \in \{0,1\} \]
    La likelihood (ossia la \textit{funzione di massa congiunta}) è:
    \begin{equation*}
        \begin{split}
            f(x_1, x_2, \ldots, x_n \rvert p) &:= P(X_1 = x_1, X_2 = x_2, \ldots X_n = x_n \rvert p) \\
            &= p^{x1}(1-p)^{1-x1} \ldots p^{x_n}(1-p)^{1-x_n} \\
            &= p^{\sum_{i}^{} x_1}(1-p)^{n- \sum_{i}^{} x_1} \qquad x_1 = 0,1 \qquad i = 1, \ldots, n
        \end{split}
    \end{equation*}
    Possiamo derivare rispetto a $p$:
    \[ \frac{d}{dp} \log f(x_1, x_2, \ldots, x_n \rvert p) = \frac{1}{p} \sum_{i = 1}^{n} x_i - \frac{1}{1- p} \left(n-\sum_{i = 1}^{n} x_1 \right) \]
    Da questo bro possiamo ottenere un'espressione per la stima $\boldsymbol{\hat{p}}$:
    \[ \hat{p} = \frac{1}{n} \sum_{i = 1}^{n} x_i \]
    \subsection{MLE di una Poisson}
    La funzione di \textit{likelihood} è data da:
    \begin{equation*}
        \begin{split}
            f(x_1, x_2 \ldots x_n / \lambda) &= \frac{\lambda^{x_1} e^{-y}}{x_1!} \ldots \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \\
            &= \frac{\lambda^{\sum_{i}^{} x_i} e^{-\lambda}}{x_1 ! \ldots x_n !} \\
        \end{split}
    \end{equation*}
    Come sempre deriviamo e otteniamo:
    \[ \frac{d}{d\lambda} \log f(x_1, x_2, \ldots, x_n \rvert \lambda) = \frac{1}{\lambda} \sum_{i = 1}^{n} x_i - n \]
    Da questo bro possiamo ottenere un'espressione per la stima $\hat{\lambda}$:
    \[ \hat{\lambda} = \frac{1}{n} \sum_{i = 1}^{n} x_i \]
    La stessa formula può essere applicata al campione $X_1, X_2, \ldots, X_n$:
    \[ P\{X_i = 1\} = 1 - P\{X_i = 0\} \]
    \paragraph{Esempio} Numero di incidenti stradali in 10 giornate senza pioggia \\
    Dataset: \{ 4 0 6 5 2 1 2 0 4 3 \} \\
    Si vuole stimare per quell'anno la frazione di giornate senza pioggia con \textit{2 incidenti o meno}
    \[ \overline{X} = \frac{1}{10} \sum_{i = 1}^{10} X_i = \boldsymbol{2.7} \]
    Cosi otteniamo che la media della poissoniana è 2.7, la stima desiderata è data da:
    \[ (1+2.7+ (2.7)^2 /2 ) e^{-2.7} \approx 0.4936 \]
    \subsection{MLE distribuzione Uniforme}
    \begin{equation*}
        f(X_1, \ldots X_n \rvert \theta) =
        \begin{cases}
            \frac{1}{\theta} & 0 < x_1 < \theta \\
            0 & \text{altrimenti}
        \end{cases}
    \end{equation*}
    La formula per la stima di $\theta$
    \[ \hat{\theta} = \max \{ X_1, \ldots, X_n \} \]
    \subsection{MLE distribuzione Normale} 
    TODO AGGIUNGERE THETA MAX THETA / 2
    \definizione La distribuzione normale ha media $\mu$ e dev. st. $\sigma$ \textbf{incognite} \\
    La densità congiunta (la likelihood) è data da:
    \[ f(x_1, x_2, \ldots, x_n \rvert \mu, \sigma) = \prod_{i= 1}^{n} \frac{1}{\sqrt{2 \mu \sigma}} \exp \left \{ - \frac{(x_1-\mu)^2}{2 \sigma^2} \right \} \]
    La log-likelihood (metodo semplificato per migliorarci la vita che è già una merda) è data da:
    \[ \log f(x_1, x_2, \ldots, x_n \rvert \mu, \sigma) = - \frac{n}{2} \log(2\pi) - n \log \sigma - \frac{1}{2\sigma^2} \sum_{i = 1}^{n} (x_i - \mu)^2 \]
    La risoluzione (che lasciamo al libro) ci porta alle formule per le stime:
    \[ \hat{\mu} = \frac{1}{n} \sum_{i = 1}^{n} x_1 \]
    \[ \hat{\sigma} = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(x_i - \hat{\mu})^2 } \]
    TODO TEORIA DEL LIMITE CENTRALE
    \section{Intervalli di confidenza}
    \subsection{$\mu$ incognita e varianza  $\sigma^2$ nota}
    Sia $\boldsymbol{X_1, X_2, \ldots, X_n}$ un campione di una popolazione normale con $\mu$ \textit{incognita} e varianza $\sigma^2$ \textit{nota}
    \[ \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1) \]
    Chiedo aiuto alla regia, non so cosa stia sta roba ma comunque:
    \[ P \left( \overline{X} - 1.96 \frac{\sigma}{\sqrt{n}} < \mu < \overline{X} + 1.96 \frac{\sigma}{\sqrt{n}} \approx 0.95 \right) \]
    Il 95\% circa delle volte $\mu$ starà a una distanza non superiore a 1.96 $\sigma / \sqrt{n}$ dalla
    media aritmetica dei dati. Se osserviamo il campione, e registriamo che $\overline{X} = \overline{x}$,
    allora possiamo dire che "con il 95\% di confidenza"
    \[ \left( \overline{x} - 1.96 \frac{\sigma}{\sqrt{n}}, \overline{x} + 1.96 \frac{\sigma}{\sqrt{n}} \right) \]
    \centerline{Questo intervallo è detto \textit{intervallo di confidenza} ad un livello del 95\%}
    \paragraph{Esempio} segnale elettrico di valore $\mu$ \\
    i valori registrati sono i seguenti: 5 8.5 12 15 7 9 7.5 6.5 10.5 \\
    Otteniamo $\boldsymbol{\overline{x}}$:
    \[ \overline{x} = \frac{81}{9} = 9 \]
    Un intervallo di confidenza al 95\% per $\mu$ è
    \[ \left( 9 - 1.96 \frac{2}{3}, \quad 9 + 1.96 \frac{2}{3}\right) = (7.69, 10.31) \]
    \centerline{Otteniamo quindi il 95\% di fiducia che il messaggio fosse \textbf{compreso} tra 7.69 e 10.31}
    \begin{figure}[H]
        \caption{TODO CAPIRE CHE SFACCIMM è STA ROBA}
        \includegraphics[width=\textwidth]{images/boh.png}
    \end{figure}
    \subsection{$\mu$ incognita e varianza  $\sigma^2$ incognita}
    Dato che tutti i nostri parametri sono ignoti, non possiamo basarci sul fatto che $\sqrt{n}(\overline{X} - \mu) / \sigma$ è una \textit{normale standard}, dobbiamo quindi ricorrere a una varianza campionaria come segue:
    \[ S^2 := \frac{1}{n-1} \sum_{i}^{} (X_i - \overline{X})^2 \longrightarrow \frac{\overline{X} - \mu}{\frac{S}{\sqrt{n}}} \sim t_{n-1}\]
    \centerline{Alla fine otteniamo una variabile aleatoria di tipo $t$ con n-1 gradi di libertà}
    \paragraph{Per Bilaterale}
    \[ P \left\{ \overline{X} - t_{\frac{\alpha}{2}, n-1} \frac{S}{\sqrt{n}} < \mu < \overline{X} + t_{\frac{\alpha}{2}, n_1} \frac{S}{\sqrt{n}} \right\} = 1 - \alpha \]
    \paragraph{Per Unilaterale}
    \[ P \left(  \overline{X} - t_{\frac{\alpha}{2}, n-1} \frac{\sigma}{\sqrt{n}} < \mu \right) / P \left( \mu < \overline{X} + t_{\frac{\alpha}{2}, n_1} \frac{\sigma}{\sqrt{n}}\right) = 1 - \alpha  \]
    \subsection{Metodo Montecarlo}
    supponendo di avere una funzione $f$ da $\mathbb{R}^r$ in $\mathbb{R}$ e vogliamo stimare la quantità $\theta$:
    \[ \theta := \int_{0}^{1} \int_{0}^{1} \cdots \int_{0}^{1} f(y_1, y_2, \ldots, y_n) \, dy_1 \, dy_2 \ldots \, dy_n \]
    Possiamo notare che $U_1, U_2, \ldots, U_r$ sono var. al. \textit{uniformi} su 0,1 quindi:
    \[ \ev{f(U_1, U_2, \ldots, U_r)} = \theta \]
    Se produciamo un numero casuale distribuito come la funzione e lo ripetiamo $n$ volte, possiamo stimare $\boldsymbol{\theta}$
    \[ \hat{\theta} = \frac{1}{n} \sum_{i = 1}^{n} X_i \]
    \paragraph{Esempio} pensiamo alla stima di questo integrale:
    \[ \theta := \int_{0}^{1} \sqrt{1 - y^2} \, dy = \ev{\sqrt{1-U^2}} \]
    Se $\boldsymbol{U_1, U_2, \ldots, U_{100}}$ sono variabili aleatorie con tale distribuzione e \textit{indipendenti} ponendo 
    \[ X_i := \sqrt{1-U^2_i} \qquad i = 1,2, \ldots, 100 \]
    Otteniamo un campione di \textbf{100} variabili aleatorie di media $\theta$. Calcoliamo ora la \textit{media campionaria}:
    \[ \hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} X_1 = 0.786 \]
    e successivamente la \textit{deviazione standard campionaria}:
    \[ S = 0.23 \]
    dato che $t_{0.025, 99} \approx 1.985$ otteniamo che un intervallo di confidenza al 95\% per $\theta$ è il seguente:
    \[ 0.786 \pm 1.985 \cdot 0.023 \]
    \centerline{Quindi il valore è compreso tra 0.740 e 0.832}
    \section{Intervalli di predizione}
    \subsection{$\mu$ incognita e varianza $\sigma^2$ incognita}
    Supponiamo che $X_1, X_2, \ldots, X_n, X_{n+1}$ sia un campione normale di media $\mu$ e varianza $\sigma^2$ entrambe \textit{incognite}
    \[ \mu = \overline{X}_n = \frac{1}{n} \sum_{i = 1}^{n} X_i \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})  \]
    \paragraph{Per la riproducibilità}
    \[ X_{n+1} - \overline{X}_n \sim \mathcal{N}(0, \sigma^2 + \frac{\sigma^2}{n}) \longrightarrow \frac{X_{n+1} - \overline{X}_n}{\sigma \sqrt{1+1 /n}} \]
    Dato che $\sigma$ è incognita dobbiamo sostituirla col suo stimatore (scegliendo la \textit{deviazione standard campionaria} quindi poniamo:
    \[ S^2_n := \frac{1}{n -1} \sum_{i= 1}^{n} (X_i - \overline{X}_n)^2 \]
    Questa grandezza è \textit{indipendente} da $\overline{X}_n$ quindi otteniamo
    \[ \frac{X_{n+1} - \overline{X}_n}{S_n \sqrt{1+1/n}} \sim t_n - 1 \]
    \paragraph{Esempio} prendiamo in campione i valori rilevati da un contapassi negli ultimi 7 giorni \\
    Dataset: 6822 5333 7420 6252 7005 6752 \\
    Si trovi l'intervallo di predizione al 95\% di confidenza \\
    \textbf{Risoluzione:} le statistiche del campione sono:
    \[ \overline{X}_7 \approx 6716.57 \qquad \qquad S_7 \approx 733.97 \]
    Dalle tabelle ricaviamo che $t_{0.025,6} \approx 2.447$ (+ altri passaggi) concludiamo col dire che il 95\% di confidenza che $X_8$ cadrà nell'intervallo [4796, 8637]
    \subsection{Intervalli di confidenza per la varianza} 
    Se $X_1, X_2, \ldots, X_n$ è un campione di una distribuzione \textit{normale} con parametri $\mu$ $\sigma^2$ \textbf{incogniti} ci possiamo basare sul fatto che
    \formula \[ (n-1) \frac{S^2}{\sigma^2} \sim \mathcal{X}^2_{n-1}  \]
    \paragraph{Per caso Bilaterale}:
    \begin{equation}
        \left(\frac{(n-1) s^2}{\chi_{\frac{\alpha}{2}, n-1}^2}, \quad \frac{(n-1) s^2}{\chi_{1-\frac{\alpha}{2}, n-1}^2}\right)
    \end{equation}
    \paragraph{Per caso Unilaterale}:
    \begin{equation}
        P \left( 0 < \sigma^2  < \frac{(n-1)S^2}{\mathcal{X}^2_{1-\alpha, n-1}}\right) / P \left( \frac{(n-1)S^2}{\mathcal{X}^2_{\alpha, n-1}} < \sigma^2 \right)
    \end{equation}
    \subsection{Stime per la differenza tra le medie di due popolazioni normali}
    Siano $X_1, X_2, \ldots, X_n$ e $Y_1, Y_2, \ldots, Y_m$ due campioni normali e differenti e denotiamo con $\mu_1$ e $\sigma^2_1$ e con $\mu_2$ e $\sigma^2_2$ \\
    $\boldsymbol{\overline{X} - \overline{Y}}$ è lo stimatore di massima verosomiglianza $\mu_1 - \mu_2$
    \begin{figure}[H]
        \includegraphics[width=\textwidth]{images/boh_2.png}
    \end{figure}
    Per ottenere uno \textit{stimatore non puntuale}, dobbiamo conoscere la distribuzione di $\overline{X} - \overline{Y}$ poiche:
    \[ \overline{X} \sim \mathcal{N} \left( \mu_1, \frac{\sigma^2_1}{n} \right)  \qquad e \qquad \overline{Y} \sim \mathcal{N}\left( \mu_2, \frac{\sigma^2_2}{m} \right) \]
    Possiamo dedurre che:
    \[ \overline{X} - \overline{Y} \sim \mathcal{N}\left( \mu_1 - \mu_2, \frac{\sigma^2_1}{n} + \frac{\sigma^2_2}{m} \right) \]
    Ipotizzando di conoscere $\sigma^2_1$ e $\sigma^2_2$ abbiamo che:
    \[ \frac{\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}{\sqrt{\sigma^2_1 / n + \sigma^2_2 / m}} \sim \mathcal{N}(0,1) \]
    e possiamo dedurre, con i passaggi che ci sono ormai familiari, che
    \paragraph{Per caso Bilaterale}
    $$
    \begin{aligned}
    1-\alpha & =P\left(-z_{\frac{\alpha}{2}}<\frac{\overline{X}-\overline{Y}-\left(\mu_1-\mu_2\right)}{\sqrt{\sigma_1^2 / n+\sigma_2^2 / m}}<z_{\frac{\alpha}{2}}\right) \\
    & =P\left(\overline{X}-\overline{Y}-z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m}}<\mu_1-\mu_2<\overline{X}-\overline{Y}+z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m}}\right)
    \end{aligned}
    $$
    \paragraph{Per caso Unilaterale}
    $$
    \begin{aligned}
    1 - \alpha &= P\left( \overline{X} - \overline{Y} - z_{\alpha} \sqrt{\frac{\sigma^2_1}{n} + \frac{\sigma^2_2}{m} < \mu_1 - \mu_2} \right) / P \\
    &= \left( \mu_1 - \mu_2 < \overline{X} - \overline{Y} - z_{\alpha} \sqrt{\frac{\sigma^2_2}{n} + \frac{\sigma^2_2}{m}} \right)\\
    \end{aligned}
    $$
    \section{Intervalli di confidenza}
    \subsection{Intervalli approssimati per Bernoulli}
    Nel caso avessimo $n$ oggetti con una quantita $X$ di oggetti che soddisfano i requisiti, possiamo dire che $X$ ha distribuzione \textit{binomiale} di parametri $n$ e $p$
    \[ \frac{X - np}{\sqrt{np(1-p)}} \sim \mathcal{N}(0,1) \]
    Per ottenere un intervallo per p denotiamo con $\hat{p} := X / n$ la frazione degli oggetti del campione che soddisfano i requisiti, quindi:
    \[ \frac{X - np}{\sqrt{n \hat{p}(1-\hat{p})}} \sim \mathcal{N}(0,1) \]
    Da questa formula possiamo ottenere cosi un intervallo di confidenza
    \paragraph{Per caso Bilaterale}
    \[ 1 - \alpha = P \left(\hat{p} - z_{\frac{\alpha}{2}} \sqrt{\hat{p} (1- \hat{p}) / n} < p < \hat{p} + z_{\frac{\alpha}{2}} \sqrt{\hat{p} (1- \hat{p}) / n} \right) \]
    \paragraph{Per caso Unilaterale}
    \[ P \left(\hat{p} - z_{\frac{\alpha}{2}} \sqrt{\hat{p} (1- \hat{p}) /n} < p \right) / P \left(p < \hat{p} + z_{\frac{\alpha}{2}} \sqrt{\hat{p} (1-\hat{p}) / n} \right)\]
    \paragraph{Esempio} Un campione di \textit{100} transitor viene testato. \textit{80} pezzi sono \textit{adeguati} \\
    Volendo trovare un intervallo del 95\% per la percentuale $p$ scriviamo:
    \[ \left(0.8 - 1.96 \sqrt{0.8 \cdot 0.2 / 100}, \quad 0.8 + 1.96 \sqrt{0.8 \cdot 0.2 / 100} \right) = (0.7216, \quad 0.8784) \]
    Possiamo dire quindi con il 95\% di confidenza che sarà \textit{accettabile} una percentuale compresa tra il \textbf{72.16\%} e il \textbf{87.84\%}
    \begin{figure}[H]
        \includegraphics[width=\textwidth]{images/boh_3.png}
    \end{figure}
    \subsection{Qualità ed efficienza degli stimatori}
    Sia $X := (X_1, X_2, \ldots, X_n)$ un campionare di una distribuzione \textit{nota} tranne per il parametro $\theta$ che è incognito e $d(X)$ uno stimatore di $\theta$ \\
    Come possiamo valutare la sua efficacia? un criterio può essere quello dell'\textit{errore quadratico medio} ossia:
    \[ r(d, \theta) := \ev{(d(X) - \theta)^2} \]
    \centerline{e sarà questo il nostro indicatore del valore di $d$ come stimatore di $\theta$}
    \subsubsection{Bias e Polarizzazione}
    \definizione Sia $d= d(X)$ uno stimatore del parametro $\theta$ allora:
    \[ b_\theta (d) := \ev{d(X)} - \theta \]
    \centerline{Questo viene detto \textit{bias} di d come stimatore di $\theta$}
    \centerline{Se il bias è nullo, si dice che è uno stimatore \textit{corretto} o \textit{non distorto} }
    \paragraph{Esempio} Sia $X_1, \ldots, X_n$ un campione con media \textit{incognita} $\theta$ quindi:
    \[ d_1 (X_1, X_2, \ldots, X_n) = X_1 \]
    \[ d_2 (X_1, X_2, \ldotp, X_n) = \frac{X_1 + X_2 + \cdots + X_n}{n} \]
    \centerline{sono entrambi \textit{stimatori non distorti di $\theta$}}
    verifichiamo:
    \[ \ev{X_1} = \ev{\frac{X_1 + X_2 + \cdots + X_n}{n}} = \theta \]
    Se $d = d(X)$ è uno \textit{stimatore corretto}, il suo errore quadratico medio è
    \begin{equation*}
        \begin{split}
            r(d, \theta) &= \ev{(d- \theta)^2} \\
            &= \ev{(d-\ev{d})^2} \\
            &= Var(d)
        \end{split}
    \end{equation*}
    \centerline{Questo è lo stimatore migliore con Varianza minima}
    \paragraph{Regola generale}
    \[ d_3(X_1, X_2, \ldots, X_n) := \sum_{i=1}^{n} \lambda_i X_i \text{  è corretto se } \sum_{i=1}^{n} \lambda_i = 1 \] 
    \subsubsection{Combinazioni di stimatori corretti}
    Consideriamo due stimatori \textit{corretti} e \textit{indipendenti} di parametro $\theta$ (denotati con $d_1$ e $d_2$) con varianze rispettivamente $\sigma^2_1$ $\sigma^2_2$
    \[ \ev{d_i} = \theta \quad Var(d_i) = \sigma^2_i \qquad i=1,2  \]
    uno stimatore corretto di $\theta$ è il seguente
    \[ d := \lambda d_1 + (1- \lambda) d_2 \]
    Successivamente vogliamo trovare anche il valore di $\lambda$ che produce lo stimatore $d$ con il \textit{minore errore quadratico medio}
    \begin{equation*}
        \begin{split}
            r(d, \theta) &= Var(d) \\
            &= \lambda^2 Var(d_1) + (1- \lambda)^2 Var(d_2) \qquad \text{per l'indipendenza di $d_1$ e $d_2$} \\
            &= \lambda^2 \sigma^2_1 + (1-\lambda)^2 \sigma^2_2
        \end{split}
    \end{equation*}
    ayo bro what's this shit, le me calculate the derivata with latti:
    \[ \frac{d}{d \lambda} r(d, \theta) = 2 \lambda \sigma^2_1 - 2(1- \lambda) \sigma^2_2 \]
    e belin lo studiamo sto segno o no? denotiamo con $\hat{\lambda}$ il valore di $\theta$ che produce il minimo
    \[ 2 \hat{\lambda} \sigma^2_1 - 2(1- \hat{\lambda}) \sigma^2_2 = 0 \]
    da cui otteniamo:
    \[ \hat{\lambda} = \frac{\sigma^2_2}{\sigma^2_1 + \sigma^2_2} = \frac{1 / \sigma^2_1}{1 / \sigma^2_1 + 1 / \sigma^2_2} \]
    il peso ottimale da dare a uno stimatore deve essere \textbf{inversamente} proporzionale alla sua varianza \\
    La migliore combinazione lineare delle $d_i$ per l'errore quadratico medio è:
    \begin{equation*}
        \begin{split}
            r(d, \theta) &= Var(d) \\
            &= \left( \frac{1}{\sum_{i=1}^{n}} 1 / \sigma^2_i \right)^2 \sum_{i=1}^{n} \left( \frac{1}{\sigma^2_i} \sigma^2_i \right) \\
            &= \frac{1}{\sum_{i=1}^{n} 1 / \sigma^2_i}
        \end{split}
    \end{equation*}
    \[ r(d, \theta) = Var(d) \]
    \paragraph{Bias/Polarizzazione}
    Se $d(X)$ è \textbf{distorto}:
    \begin{equation*}
        \begin{split}
            r(d, \theta) &= \ev{(d - \theta)^2} \\
            &= Var(d) + 0 + \ev{b_\theta (d)^2} \\
            &= Var(d) + b_\theta(d)^2
        \end{split}
    \end{equation*}
    \begin{figure}[h]
        \includegraphics[width=\textwidth]{images/boh_4.png}    
    \end{figure}
    \subsection{Stimatore della media di una distribuzione uniforme}
    Siano $X_1, X_2, \ldots, X_n$ un campione estratto da una popolazione con distribuzione \textit{uniforme} su $(0, \theta)$ dove $\theta$ è un parametro incognito. \\
    Dato che (non si sa come) $\boldsymbol{\ev{X_i} = \theta / 2}$ è uno stimatore naturale per $\theta$ è dato da
    \[ d_1 = d_1 (X) := 2 \overline{X} := \frac{2}{n} \displaystyle\sum_{i=1}^{n} X_i \]
    Siccome $\boldsymbol{\ev{d_1} = \theta}$, otteniamo che:
    \begin{equation*}
        \begin{split}
            \boldsymbol{r(d_1, \theta)} &= Var(d_1) \\
            &= \frac{4}{n} Var(X_i) \\
            &= \frac{4}{n} \cdot \frac{\theta^2}{12} \\
            &= \frac{\theta^2}{3n}
        \end{split}
    \end{equation*}
    Un secondo stimatore che abbiamo è quello di \textbf{massima verosomiglianza} ($d_2$):
    \[ d_2 = d_2(X) = MLE := \max(X_i) \]
    Per trovare l'errore quadratico medio di $d_2$ dobbiamo prima conoscere la sua \textit{media} e la sua \textit{varianza}
    \[ \ev{d_2} = \int_{0}^{\theta} x \frac{nx^{n-1}}{\theta^n} \, dx = \frac{n}{n + 1} \theta \]
    \[ \ev{d^2_2} = \int_{0}^{\theta} x^2 \frac{nx^{n-1}}{\theta^n} \, dx = \frac{n}{n+2} \theta^2 \]
    \[ Var(d_2) = \ev{d^2_2} - \ev{d_2}^2 = \frac{n \theta^2}{(n+2) (n+1)^2} \]
    Quindi ora calcoliamo la $\boldsymbol{r(d_2, \theta)}$:
    \begin{equation}
        \begin{aligned}
        \boldsymbol{r\left(d_2, \theta\right)} & =\operatorname{Var}\left(d_2\right)+\left(E\left[d_2\right]-\theta\right)^2 \\
        & =\frac{n \theta^2}{(n+2)(n+1)^2}+\frac{\theta^2}{(n+1)^2} \\
        & =\frac{\theta^2}{(n+1)^2}\left[\frac{n}{n+2}+1\right] \\
        & =\frac{2 \theta^2}{(n+1)(n+2)}
        \end{aligned}
    \end{equation}
    Confrontando gli errori quadratici medi notiamo che $d_2$ è \textbf{migliore} di $d_1$ per $\theta$
    \[ \frac{2 \theta^2}{(n+1)(n+2)} \leq \frac{\theta^2}{3n} \qquad \text{$d_2$ migliore} \]
    \begin{figure}[H]
        \includegraphics[width=\textwidth]{images/boh_5.png}
    \end{figure}
    \section{Stimatori Bayesiani}
    \definizione Quando il parametro incognito $\boldsymbol{\theta}$ possiamo considerarlo come una variabile aleatoria, questo approccio viene detto \textit{bayesiano}. \\
    Se abbiamo delle informazioni su quelli che possono essere assunti i valori da $\theta$ ed esse assumono la forma di distribuzione di probabilità si dice che abbiamo \textbf{una distribuzione a priori per $\boldsymbol{\theta}$} \\[2ex]
    Se i valori che osserviamo sono $X_i = x_i \qquad i=1,2, \ldots, n$ la \textit{densità di probabilità condizionale di $\theta$} è data da:
    \begin{equation*}
        \begin{split}
            \mathcolor{ForestGreen}{\boldsymbol{f(\theta \rvert x_1, x_2, \ldots, x_n)}} &= \frac{f(x_1, x_2, \ldots, x_n, \theta)}{\mathcolor{Turquoise}{\boldsymbol{f(x_1, x_2, \ldots, x_n)}}} \\
            &= \frac{\mathcolor{red}{\boldsymbol{f(x_1, x_2, \ldots, x_n \rvert \theta)}} \mathcolor{blue}{\boldsymbol{p(\theta)}}}{\int_{}^{} f(x_1, x_2, \ldots, x_n \rvert \theta') p(\theta') d\theta'}
        \end{split}
    \end{equation*}
    Dove:
    \begin{itemize}
        {\color{ForestGreen} \item $\boldsymbol{f(\theta \rvert x_1, x_2, \ldots, x_n)}$ Viene detta \textit{probabilità a posteriori}}
        {\color{Turquoise} \item $\boldsymbol{f(x_1, x_2, \ldots, x_n)}$ è la \textit{MLE Marginale}}
        {\color{red} \item $\boldsymbol{f(x_1, x_2, \ldots, x_n \rvert \theta)}$ è la \textit{MLE}}
        {\color{blue} \item $\boldsymbol{p(\theta)}$ è la \textit{distribuzione a priori}}
    \end{itemize}
    Una buona stima per $\theta$ può essere la \textbf{media} perciò:
    \[ \ev{\theta \rvert X_1 = x_1, \ldots, X_n = x_n} = \int_{-\infty}^{\infty} \theta f(\theta \rvert x_1, x_2, \ldots, x_n) \, d\theta \quad \text{\textit{nel caso continuo}} \]
    \subsection{Stimatore di $\theta$ per Bernoulli}
    Se abbiamo $X_1, X_2, \ldots, X_n$ Bernoulliane, con massa di probabilità:
    \[ f(x \rvert \theta) = \theta^x (1-\theta)^{1-x} \qquad x = 0,1 \]
    \centerline{Dove $\theta$ è un parametro sconosciuto}
    Supponiamo quindi che la \textit{distribuzioni a priori} di $\theta$ sia uniforme su (0,1), denotiamo con $p$ la densità a propri di $\theta$
    \[ p(\theta)=1 \qquad 0 < \theta < 1 \]
    La densità condizionale di $\theta$ date $x_1, x_2, \ldots, x_n$ è 
    \begin{equation}
        \begin{aligned}
        f\left(\theta \mid x_1, x_2, \ldots, x_n\right) & =\frac{f\left(x_1, x_2, \ldots, x_n, \theta\right)}{f\left(x_1, x_2, \ldots, x_n\right)} \\
        & =\frac{f\left(x_1, x_2, \ldots, x_n \mid \theta\right) p(\theta)}{\int_0^1 f\left(x_1, x_2, \ldots, x_n \mid \vartheta\right) p(\vartheta) d \vartheta} \\
        & =\frac{\theta^{\sum_i x_i}(1-\theta)^{n-\sum_i x_i}}{\int_0^1 \vartheta^{\sum_i x_i}(1-\vartheta)^{n-\sum_i x_i} d \vartheta}
        \end{aligned}
    \end{equation}
    Non è difficile provare (e invece lo è) integrando per parti un certo numero di volte che per ogni valore di $m$ e $r$:
    \[ \int_{0}^{1} \theta^m (1-\theta)^r \, d\theta = \frac{m! r!}{(m+r+1)!} \]
    poniamo ora $\boldsymbol{x := \sum_{i=1}^{n} x_i}$
    \[ f(\theta \rvert x_1, x_2, \ldots, x_n) = \frac{(n+1)!}{x! (n-x)!} \theta^x (1-\theta)^{n-x} \qquad 0 < \theta < 1 \]
    Ora siamo in grado di calcolare \textit{la stima bayesiana}
    \begin{equation}
        \begin{aligned}
        E\left[\theta \mid x_1, x_2, \ldots, x_n\right] & =\frac{(n+1) !}{x !(n-x) !} \int_0^1 \theta^{1+x}(1-\theta)^{n-x} d \theta \\
        &=\frac{(n+1) !}{x !(n-x) !} \frac{(1+x) !(n-x) !}{(n+2) !} \\
        &= \frac{x+1}{n+2} \\
        &= \boldsymbol{\frac{1 + \sum_{i=1}^{n} X_i}{n + 2}}
        \end{aligned}
    \end{equation}
    \paragraph{Esempio} Se raccogliamo un campione di \textit{10 bernoulliane} e trovassimo \textbf{6 successi}, lo stimatore bayesiano di $\theta$ fornirebbe un valore di \textbf{7 / 12} \\
    Lo stimatore di massima verosomiglianza vale invece \textbf{6 / 10}
    \subsection{Stimatore di $\theta$ per una Normale}
    Supponiamo che $X_1, X_2, \ldots, X_n$ sia una distribuzione normale con media $\theta$ \textit{incognita} e varianza $\sigma^2_0$ \textbf{nota} \\[2ex]
    Calcoliamo la \textit{densità condizionale} di $\theta$:
    \[ f(\theta \rvert x_1, x_2, \ldots, x_n) = \frac{f(x_1, x_2, \ldots, x_n \rvert \theta) p(\theta)}{f(x_1, x_2, \ldots, x_n)} \]
    Ora calcoliamo la \textit{media}:
    \[ \ev{\theta \rvert X_1, X_2, \ldots, X_n } = \boldsymbol{\frac{n / \sigma^2_0}{n / \sigma^2_0 + 1 / \sigma^2} \overline{X} + \frac{1 / \sigma^2}{n / \sigma^2_0 + 1 / \sigma^2} \mu} \]
    e successivamente la \textit{varianza}:
    \[ Var(\theta \rvert X_1, X_2, \ldots, X_n) = \boldsymbol{\frac{1}{n / \sigma^2_0 + 1 / \sigma^2}} \]
    \subsection{Stimatore di $\theta$ per Uniformi}
    Avendo una funzione di likelihood $f(x_1, x_2, \ldots, x_n \rvert \theta)$ e sapendo che la distribuzione è \textit{uniforme} \\
    $\theta \in [a,b]$ \\
    $d_b = d_{MLE}$
    \section{Verifica delle ipotesi}
    Se prima noi cercavamo di stimare determinati parametri ora cerchiamo invece (avendo un campione raccolto) di trovare \textit{le ipotesi che li coinvolga}
    \subsection{Livelli di significatività}
    Consideriamo una popolazione con distribuzione $F_\theta$ che dipende da $\theta$ incognito e vogliamo verificare una qualche ipotesi su $\theta$, una distribuzione normale con media $\theta$ e varianza 1 abbiamo due ipotesi
    \begin{enumerate}
        \item $H_0 : \theta = 1 \rightarrow$ \textit{Ipotesi nulla \textbf{semplice}}
        \item $H_0 : \theta \leq n \rightarrow$ \textit{Ipotesi nulla \textbf{composta}}
    \end{enumerate}
    La prima ipotesi afferma che la popolazione ha come distribuzione $\boldsymbol{\mathcal{N}(1,1)}$ mentre la seconda sostiene che è normale con \textit{varianza} 1 e \textit{media} \textbf{non} superiore a 1. 
    Queste due ipotesi si possono verificare su un campiona aleatorio $X_1, X_2, \ldots X_n$ \\[2ex]
    Esiste una regione critica \textbf{C} per cui se il campione aleatorio vi appartiene l'ipotesi \textit{non viene accettata}. Esiste un livello di tolleranza specificato all'interno della regione critica per cui un'ipotesi può essere \textit{ancora accettata}.
    Questa tolleranza è definita dal \textbf{livello di significatività}, ovvero viene definito $\alpha$ tale che se l'ipotesi è vera la probabilità di rifiutarla non superi $\alpha$
    \[ \text{accetta } H_0 \text{ se} (X_1, X_2, \ldots, X_n) \not \in C \]
    \centerline{e}
    \[ \text{rifiuta } H_0 \text{ se} (X_1, X_2, \ldots, X_n) \in C \]
    Esistono \textbf{due tipi di errori}:
    \begin{enumerate}
        \item \textbf{Prima Specie}: Si rifiuta $H_0$ anche se è vera
        \item \textbf{Seconda Specie}: si accetta $H_0$ anche se è falsa
    \end{enumerate}
    \subsection{Verifica di ipotesi sulla media di una popolazione normale}
    Supponiamo di avere $X_1, X_2, \ldots, X_n$ sia un campione aleatorio di una popolazione normale di parametri $\mu$ $\sigma^2$ con \textit{varianza} nota e \textit{media} incognita, vogliamo verificare le seguenti ipotesi:
    \[ H_0 : \mu = \mu_0 \]
    e l'ipotesi \textit{alternativa}
    \[ H_1 : \mu \not = \mu_0 \]
    Lo \textbf{stimatore puntuale} per $\mu$ è:
    \[ \overline{X} := \frac{1}{n} \sum_{i=1}^{n} X_i \]
    La \textbf{regione critica del test} invece è:
    \[ C := \{ (X_1, X_2, \ldots, X_n) : \rvert \overline{X} - \mu_0 \rvert > c \} \]
    \centerline{Dove c rappresenta la \textit{tolleranza}}
    Quando $\mu = \mu_0$ sappiamo che $\overline{X}$ ha distribuzione \textbf{normale} con media $\mu_0$ e varianza $\sigma^2 / n$ allora:
    \[ \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \sim Z \]
    \centerline{Dove la relazione $\sim$ è \textbf{condizionata} all'ipotesi $H_0 : \mu = \mu_0$} \\[2ex]
    \textbf{c} deve soddisfare la seguente relazione:
    \[ \alpha = P(\text{errore di I specie}) = \boldsymbol{P_{\mu_0}(\rvert \overline{X} - \mu_0) > c} \]
    Possiamo scrivere l'equazione di sopra in questo modo:
    \[ \alpha = 2P \left( Z > \frac{c \sqrt{n}}{\sigma} \right) \]
    Per $P(Z > c \sqrt{n} / \sigma)$ per la definzione $z_{\frac{\alpha}{2}}$ vale:
    \[ P\left( Z > z_{\frac{\alpha}{2}} \right)= \frac{\alpha}{2} \longrightarrow c = z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\]
    Il test con livello di significatività ha \textbf{due esiti}:
    \[ \text{si rifiuta } H_0 \text{ se} \bigg\rvert \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \bigg\rvert > z_{\frac{\alpha}{2}} \]
    \[ \text{si accetta } H_0 \text{ se} \bigg\rvert \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \bigg\rvert \leq z_{\frac{\alpha}{2}} \]
    p dei dati $= \sqrt{n} \frac{\overline{X} - \mu_0}{\sigma}$ \\[2ex]
    $H_0$ si \textbf{accetta} se $2P(Z > z_{\frac{\alpha}{2}}) \text{ è elevata}$ \\
    $H_0$ si \textbf{rifiuta} se $2P(Z > z_{\frac{\alpha}{2}}) \text{ è bassa}$ \\
    Perche se la probabilità che $Z$ sia $> z_{\frac{\alpha}{2}}$ è \textit{alta} allora il mio valore sarà vicino al mezzo e va bene. Se è basso allora è \textit{lontano} dal mezzo e non va bene.
    \paragraph{Esempio} supponiamo una media campionaria dei \textit{5} segnali ricevuti fosse \textit{8.5}:
    \[ \frac{\sqrt{n}}{\sigma} \rvert \overline{X} - \mu_0 \rvert = \frac{\sqrt{5}}{2} \cdot 0.5 \approx 0.559 \]
    Dato che:
    \[ P(|Z| > 0.559) = 2P(Z > 0.559) \approx 2 X 0.288 = 0.576 \]
    Otteniamo che il \textit{p-dei-dati} è 0.576 e quindi l'ipotesi nulla che il segnale inviato fosse \textbf{8}, che viene accettata per ogni $\alpha < 0.576$ \\
    Se avessimo ottenuto che $\overline{X} = 11.5$ il valore del \textit{p-dei-dati} sarebbe:
    \[ P \left( |Z| > \frac{\sqrt{5}}{2} \cdot 3.5 \right) \approx 2P(Z > 3.913) \approx 0.00005 \]
    Con un valore così piccolo, l'ipotesi che il messaggio fosse stato 8, \textbf{va rifiutata}. \\[3ex]
    Riprendendo il discorso degli errori di specie andiamo a vedere ora \textit{gli errori di \textbf{seconda specie}}. \\
    Rinfreschiamo la memoria, l'errore di seconda specie è quando \textit{si accetta $H_0$ anche se è falsa}, quindi:
    \[ \boldsymbol{\beta(\mu)} := P_\mu (\text{accettare } H_0) = \boldsymbol{P_\mu \left( -z_{\frac{\alpha}{2}} \leq \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \leq z_{\frac{\alpha}{2}}\right)}\]
    La funzione $\beta(\mu)$ è detta \textbf{curva OC} (\textit{curva operativa caratteristica}) e rappresenta la probabilità di accettare $H_0$ quando la media reale è $\mu$.
    \begin{figure}[H]
        \includegraphics[width=\textwidth]{images/boh_6.png}
        \caption{Curva OC di un test \textit{bilaterale} per la media di una popolazione normale, con $\alpha = 0.05$}
    \end{figure}
    Per calcolare la probabilità ricordiamoci il fatto che $\overline{X} \sim \mathcal{N}(\mu, \sigma^2 / n)$:
    \[ Z := \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1) \]
    Quindi:
    \begin{equation*}
        \begin{split}
            \beta(\mu) &= P_\mu \left( -z_{\frac{\alpha}{2}} \leq \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \leq z_{\frac{\alpha}{2}} \right) \\
            &= \boldsymbol{\Phi \left( \frac{\mu_0 - \mu}{\sigma / \sqrt{n}} + z_{\frac{\alpha}{2}}\right) - \Phi \left( \frac{\mu_0 - \mu}{\sigma / \sqrt{n}} - z_{\frac{\alpha}{2}} \right) = 1 - \alpha}
        \end{split}
    \end{equation*}
    \centerline{Dove $\Phi$ indica la \textit{funzione di ripartizione} della distribuzione normale standard}
    \paragraph{Esempio} quanto vale la probabilità di accettare $\mu = 8$ quando in realtà $\mu = 10$:
    \[ \frac{\sqrt{n}}{\sigma} (\mu_0 - \mu) = \frac{\sqrt{5}}{2} (-2) = - \sqrt{5} \]
    Dato che $\boldsymbol{z_{0.025} \approx 1.96}$ ricaviamo la probabilità cercata:
    \begin{equation*}
        \begin{split}
            \beta(10) &\approx \Phi(- \sqrt{5} + 1.96) - \Phi(- \sqrt{5} - 1.96) \\
            &= 1 - \Phi(0.276) - 1 + \Phi(4.196) \\
            &\approx -0.609 + 1 = \boldsymbol{0.391}
        \end{split}
    \end{equation*}
    Riprendendo il discorso della curva OC, ci permette di dimensionare il campione in modo che \textit{l'errore di seconda specie} soddisfi le condizioni specifiche. \\[2ex]
    Come facciamo a trovare $n$ tale che la probabilità di \textit{accettare} $H_0 : \mu = \mu_0$ quando il vero valore è $\mu_1$ sia un valore fissato $\beta$ per $n$ tale che $\boldsymbol{\beta(\mu_1) \approx \beta}$
    \[ n \approx \left[ \frac{(z_{\frac{\alpha}{2}} + z_\beta) \sigma}{\mu_1 - \mu_0} \right]^2 \]
    \centerline{Notiamo che anche nel caso in cui $\boldsymbol{\mu_1 < \mu_0}$ troviamo sempre la stessa formula}
    \paragraph{Esempio} Quante volte è necessario inviare il segnale con verifica dell'ipotesi $H_0 : \mu = 8$ con livello di significatività 0.05 con almeno il 75\% di probabilità di rifiutare l'ipotesi nulla quando $\boldsymbol{\mu = 9.2}$ \\
    Dato che $z_{0.025 \approx 1.96}$ e $z_{0.25} \approx 0.67$ 
    \[ n \approx \left( \frac{1.96 + 0.67}{1.2} \right)^2 \cdot 4 \approx 19.21 \]
    Come vediamo per il risultato è necessario un \textit{campione di 20 segnali}, quindi con $n=20$
    \begin{equation*}
        \begin{split}
            \beta(9.2) &\approx \Phi \left( - \frac{1.2 \sqrt{20}}{2} + 1.96 \right) - \Phi \left( - \frac{1.2 \sqrt{20}}{2} - 1.96 \right) \\
            &\approx \Phi(-0.723) - \Phi(-4.643) \\
            &\approx 1 - \Phi(0.723) \approx \boldsymbol{0.235}
        \end{split}
    \end{equation*}
    Quindi ricapitolando, se il segnale viene trasmesso 20 volte c'è il 76.5\% di probabilità che l'ipotesi nulla $\mu = 8$ sia \textbf{rifiutata} se la media reale è \textbf{9.2}

    \subsection{Test unilaterali}
    Introduzione bla bla bla \\
    Verifichiamo due ipotesi:
    \[ H_0 : \mu = \mu_0 \quad \text{ contro } \quad H_1 : \mu > \mu_0 \]
    Dovremmo rifiutare l'ipotesi nulla quando lo stimatore di $\mu$ è molto più grande di $\mu_0$, la regione critica è quindi:
    \[ C := \{ (X_1, X_2, \ldots, X_n) : \overline{X} - \mu_0 > c \} \]
    la probabilità di rifiuto dovrebbe essere $\alpha$ quando $H_0$ è vera, occorre però che \textit{c} soddisfi la relazione:
    \[ P_{\mu_0} (\overline{X} - \mu_0 > c) = \alpha \]
    Il test \textit{con livello di significatività} $\alpha$ dovrà rifiutare $\boldsymbol{H_0 \text{ se } \overline{X} - \mu_0 > z_\alpha \cdot \sigma / \sqrt{n}}$
    \[ \text{si rifiuta } H_0 \text{ se} \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} > z_\alpha \]
    \[ \text{si accetta } H_0 \text{ se} \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \leq z_\alpha \]
    Quella trovata è detta \textit{regione criticia \textbf{unilaterale}} o a una coda, quindi il problema di verificare le ipotesi alternative
    \[ H_0 : \mu = \mu_0 \]
    \[ H_1 : \mu > \mu_0 \]
    \centerline{Si dice problema di \textbf{test unilaterale}} \\[2ex]
    poniamo $Z := \sqrt{n}(\overline{X} - \mu) / \sigma$ questa statistica è una \textit{normale standard} quindi:
    \begin{equation*}
        \begin{split}
            \beta(\mu) &:= P_\mu(\text{accettare } H_0) \\
            &= P_\mu \left( Z \leq \frac{\mu_0 - \mu}{\sigma / \sqrt{n}} + z_\alpha \right) \\
            &= \boldsymbol{\Phi \left( \frac{\mu_0 - \mu}{\sigma / \sqrt{n}} + z_\alpha \right) = 1 - \alpha}
        \end{split}
    \end{equation*}
    Dato che $\Phi$ in quanto \textit{funzione di ripartizione è \textbf{crescente}} però $\beta(\mu)$ è una funzione \textbf{decrescente} \\
    L'ipotesi \textit{unilaterale}
    \[ H_0 : \mu \leq \mu_0 \]
    contro l'alternativa
    \[ H_1 : \mu > \mu_0 \]
    Per accertarci che il \textit{livello di significatività} sia rimasto $\alpha$ \\
    Al variare di $\mu$ la probabilità di rifiuto è data da $\boldsymbol{1 - \beta(\mu)}$ \\
    Dobbiamo verificare che per ogni $\mu$ compatibile con $H_0$ per ogni $\boldsymbol{\mu \leq \mu_0}$ 
    \[ 1 - \beta(\mu) \leq \alpha, \qquad \text{per ogni } \mu \leq \mu_0 \]
    Quindi:
    \[ \beta(\mu) \geq 1 - \alpha, \qquad \text{per ogni } \mu \leq \mu_0 \]
    \paragraph{Osservazione} è possibile verificare l'ipotesi
    \[ H_0 : \mu = \mu_0 \]
    contro l'ipotesi \textit{alternativa}
    \[ H_1 : \mu < \mu_0 \]
    ad un livello di significatività $\boldsymbol{\alpha}$, decidendo che:
    \[ \text{si rifiuta } H_0 \text{ se } \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \leq -z_\alpha \]
    \[ \text{si accetta } H_0 \text{ se } \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \geq -z_\alpha \]
    \subsection{Il test t}
    Fino ad ora abbiamo supposto che l'unico parametro incognito fosse la \textit{media}, in questo caso la nostra varianza $\boldsymbol{\sigma^2}$ \textbf{non è nota} \\
    In questa situazione consideriamo che si possa verificare l'ipotesi nulla che $\mu$ sia uguale ad un valore assegnato $\mu_0$ contro l'ipotesi alternativa $\mu \not= \mu_0$
    \[ H_0 : \mu = \mu_0 \]
    \[ H_1 : \mu \not= \mu_0 \]
    (Cambiare poi con desc più corta)
    Come in precedenza, sembra ragionevole rifiutare l’ipotesi nulla quando $\overline{X}$ cade
    lontano da $\mu_0$ tuttavia la distanza a cui deve essere da $\mu_0$ per giustificare questo
    rifiuto, dipende dalla deviazione standard $\sigma$ che in quella sede era nota; in particolare
    $|\overline{X} - \mu0|$ doveva essere maggiore di $z_{\frac{\alpha}{2}} \cdot \sigma / \sqrt{n}$ o equivalentamente
    \[ \left[ \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \right] > z_{\frac{\alpha}{2}} \]
    Qui $\boldsymbol{\sigma}$ non è più conosciuta, sostituiamola quindi con la \textit{deviazione standard campionaria} $S$
    \[ S := \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X})^2} \]
    rifiutando l'ipotesi nulla quando 
    \[ \big\rvert \frac{\overline{X} - \mu_0}{S / \sqrt{n}} \big\rvert \] 
    Quindi alla fine noi dobbiamo ottenere una distribuzione t
    \[  t_{n-1} \sim \frac{\overline{X} - \mu}{S/ \sqrt{n}} \]
    Se si denota con T la statistica di questo test, ovvero 
    \[ T := \frac{\overline{X} - \mu_0}{S / \sqrt{n}} \]
    allora quando $H_0$ è vera (visto che $\mu = \mu_0$) ha distribuzione \textbf{$t$ con $n-1$ gradi di libertà.}
    \[ P_{\mu_0} \left( - c \leq \frac{\overline{X} - \mu_0}{S / \sqrt{n}} \leq c \right) = \boldsymbol{1- \alpha} \]
    Se vogliamo ricavare c:
    \begin{equation*}
        \begin{split}
            \alpha &= 1 - P(-c \leq T < c) \\
            &= P(T \leq -c) + P(T \geq c) \\
            &= 2P(T \geq c)
        \end{split}
    \end{equation*}
    Per cui $P(T > c) = \frac{\alpha}{2}$, e quindi deve valere $c = t_{\frac{\alpha}{2}, n-1}$, quindi in fin dei conti:
    \[ \text{si rifiuta } H_0 \text{ se } \big| \frac{\overline{X} - \mu_0}{S / \sqrt{n}} \big| > t_{\frac{\alpha}{2}, n-1} \]
    \[ \text{si accetta } H_0 \text{ se } \big| \frac{\overline{X} - \mu_0}{S / \sqrt{n}} \big| \leq t_{\frac{\alpha}{2}, n-1} \]
    \centerline{Vedere tabella sotto per tutt'e cose}
    \begin{figure}[H]
        \caption{$X_1, X_2, \ldots, X_n$ è un campionare estratto da una popolazione $\mathcal{N}(\mu, \sigma^2)$}
        \includegraphics[width=\textwidth]{images/boh_7.png}
    \end{figure}
    \paragraph{Esempio} Vogliamo verificare l'ipotesi che il consumo \textit{medio} di acqua sia \textit{350} galloni al giorno. \\
    Si misurano i consumi medi di un campione di \textit{20 campioni} che seguono:
    \begin{figure}[H]
        \includegraphics[width=\textwidth]{images/boh_8.png}
    \end{figure}
    Dobbiamo verificare le due ipotesi seguenti:
    \[ H_0 : \mu = 350 \quad \text{contro} \quad H_1 : \mu \not = 350 \]
    Calcoliamo ora la \textbf{media} e la \textbf{deviazione standard campionaria}
    \[ \overline{X} = 353.8 \qquad S \approx 21.85 \]
    troviamo ora il valore della statistica del test:
    \[ T \approx \frac{\sqrt{20} \cdot 3.8}{21.85} \approx 0.778 \]
    il valore che abbiamo trovato è minore di $t_{0.05, 19} \approx 1.729$ l'ipotesi nulla è \textit{accettata} ad un livello del \textit{5\%}
    TODO FINIRE PAGINA PDF 324 / 342 TOTALE
    \begin{figure}[H]
        \caption{$X_1, X_2, \ldots, X_n$ è un campionare estratto da una popolazione $\mathcal{N}(\mu, \sigma^2)$ e $\sigma^2$ non è nota}
        \includegraphics[width=\textwidth]{images/boh_9.png}
    \end{figure}
    \subsection{Verifica se due popolazioni hanno la stessa media}
    Una situazione che accade spesso è decidere se \textit{vari approcci} portano allo stesso risultato, oppure no. \\
    Questa problematica si ricordune spesso alla verifica dell'ipotesi che due popolazioni normali \textit{abbiano la stessa media.}
    \subsubsection{Il caso in cui le varianze sono note}
    Supponiamo di avere $X_1, X_2, \ldots, X_n$ e $Y_1, Y_2, \ldots, Y_n$ sono due campioni di due popolazioni \textit{normali} di medie $\mu_x$ $\mu_y$ e varianze \textit{note} $\sigma^2_x$ e $\sigma^2_y$ \\
    Come sempre verifichiamo le due ipotesi
    \[ H_0 : \mu_x = \mu_y \]
    \[ H_1 : \mu_x \not = \mu_y \]
    Dato che $\overline{X}$ e $\overline{Y}$ sono rispettivamente \textit{stimatori} di $\mu_x$ e $\mu_y$ \\
    Possiamo dire che $\overline{X} - \overline{Y}$ può essere \textbf{usato come stimatore} di $\mu_x - \mu_y$
    \[ \text{si rifiuta } H_0 \text{ se } \big|\overline{X} - \overline{Y} \big| > c \]
    \[ \text{si accetta } H_0 \text{ se } \big|\overline{X} - \overline{Y} \big| \leq c \]
    Come facciamo sempre noi possiamo trovare il valore di $c$ che rende questo test di livello di significatività $\alpha$ in questo modo:
    \[ \frac{\overline{X} - \overline{Y} - (\mu_x - \mu_y)}{\sqrt{\sigma^2_x / n + \sigma^2_y / m}} \sim \mathcal{N}(0,1) \]
    Per verificare l'ipotesi nulla $H_0 : \mu_x = \mu_y$ contro $H_1 : \mu_x \not = \mu_y$ facciamo cosi:
    \[ \text{si rifiuta } H_0 \text{ se } \frac{|\overline{X} - \overline{Y}|}{\sqrt{\sigma^2_x / n + \sigma^2_y / m}} > z_{\frac{\alpha}{2}} \]
    \[ \text{si accetta } H_0 \text{ se } \frac{|\overline{X} - \overline{Y}|}{\sqrt{\sigma^2_x / n + \sigma^2_y / m}} \leq z_{\frac{\alpha}{2}} \]
    \subsubsection{Il caso in cui le varianze non sono note ma supponiamo siano uguali}
    Prendiamo in considerazione i campioni di prima, tutti i nostri parametri sono \textit{incogniti} e studiamo le due ipotesi
    \[ H_0 : \mu_x = \mu_y \quad \text{ contro } \quad H_1 : \mu_x \not = \mu_y \]
    Prima di far tutto possiamo supporre che le due varianze \textit{incognite} siano uguali tra di loro quindi:
    \[ \sigma^2 := \sigma^2_x = \sigma^2_y \]
    Calcoliamo le due \textit{varianze campionarie}
    \[ S^2_x = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X})^2 \]
    \[ S^2_y = \frac{1}{m-1} \sum_{j=1}^{m} (X_j - \overline{Y})^2 \]
    Equazione idk:
    \[ \frac{\overline{X} - \overline{Y} - (\mu_x - \mu_y)}{S_p \sqrt{1 / n + 1 / m}} \sim t_{n+m-2}\]
    Dove $\boldsymbol{S^2_p}$ è lo \textit{stimatore pooled} di $\sigma^2$ e viene definito in questo modo:
    \[ S^2_p := \frac{(n-1) S^2_x + (m-1) S^2_y}{n + m -2} \]
    Quando $H_0$ è vera ($\mu_x - \mu_y = 0$):
    \[ T := \frac{\overline{X} - \overline{Y}}{S_p \sqrt{1 / n + 1 /m}} \]
    \centerline{ha distribuzione $t$ con $n + m - 2$ gradi di libertà} \\[2ex]
    Quindi possiamo verificare le ipotesi cosi:
    \[ \text{si rifiuta } H_0 \text{ se }  |T| > t_{\frac{\alpha}{2};n+m-2} \]
    \[ \text{si accetta } H_0 \text{ se }  |T| \leq t_{\frac{\alpha}{2};n+m-2} \]
    possiamo eseguire il test determinando \textit{il p-dei-dati}, denotando con $\upsilon$ il valore assunto da $T$
    \begin{equation*}
        \begin{split}
            \boldsymbol{p-dei-dati} &= P(|T_{n+m-2}| \geq |\upsilon|) \\
            &= 2P(T_{n+m-2} \geq |\upsilon|)
        \end{split}
    \end{equation*}
    \paragraph{Caso unilaterale}
    Per l'ipotesi \textit{unilaterale} abbiamo le due seguenti ipotesi:
    \[ \mu_0 : \mu_x \leq \mu_y \qquad \text{ contro } \qquad H_1 : \mu_x > \mu_y \]
    $H_0$ deve essere \textbf{rifiutata} per valore elevati di $T$, il test di significatività $\alpha$ è:
    \[ \text{si rifiuta } H_0 \text{ se }  T > t_{\alpha, n + m -2} \]
    \[ \text{si accetta } H_0 \text{ se }  T \leq t_{\alpha, n + m -2} \]
    il \textit{p-dei-dati} invece è il seguente (ricordando che $\upsilon$ è il valore assunto da $T$)
    \[ \boldsymbol{p-dei-dati} = P(T_{n+m-2} \geq \upsilon) \]
    \paragraph{Esempio} abbiamo $\overline{X} = 6.450$ e $\overline{Y} = 7.125$ \\
    Calcoliamo le due S, quindi $\boldsymbol{S^2_x \approx 0.581}$ e $\boldsymbol{S^2_y \approx 0.778}$ \\
    Calcoliamo ora lo stimatore $\boldsymbol{S^2_p}$:
    \[ S^2_p = \frac{9}{20} S^2_x + \frac{11}{20} S^2_y \approx 0.689 \]
    e la statistica del test:
    \[ \upsilon = \frac{-0.675}{\sqrt{0.689 ( 1/10 + 1/12)}} \approx -1.90 \]
    \subsubsection{Il caso in cui le varianze sono ignote e diverse}
    \begin{figure}[H]
        \includegraphics[width=\textwidth]{images/boh_10.png}
    \end{figure}
    \subsection{Il test t per campioni di coppie di dati}
    I dati che prendiamo in esempio sono descritti da $n$ coppie di valori $(X_i, Y_i)$ per $i = 1,2,\ldots, n$ \\
    $X_1, X_2, \ldots, X_n \qquad Y_1, Y_2, \ldots, Y_m \qquad$ \textbf{$n$ e $m$ devono essere uguali} \\ 
    Le nostre due variabili sono \textbf{dipendenti} quindi:
    \[ X \sim \mathcal{N}(\mu_x, \sigma_x) \]
    \[ Y \sim \mathcal{N}(\mu_y, \sigma_y) \]
    Se poniamo $\boldsymbol{W_i := X_i - Y_i}$ per $i=1,2,\ldots,n$ possiamo verificare queste due ipotesi
    \[ H_0 : \mu_W = 0 \quad \text{ contro } \quad H_1 : \mu_W \not = 0 \]
    La nostre $W$ provengono da un campione di popolazione $\mathcal{N}(\mu_W, \sigma^2_W)$, il test t quindi ci fornisce le seguenti regole:
    \[ \text{si accetta } H_0 \text{ se } - t_{\frac{\alpha}{2}, n-1} \leq \sqrt{n} \frac{\overline{W}}{S_W} \leq t_{\frac{\alpha}{2}, n-1} \]
    \[ \text{si rifiuta } H_0 \text{ negli altri casi} \]
    \subsection{Verifica di ipotesi sulla varianza di una popolazione normale}
    Sia $X_1, X_2, \ldots, X_n$ un campione di popolazione normale con media incognita $\mu$ e varianza incognita $\sigma^2$, verifichiamo le seguenti ipotesi:
    \[ H_0 : \sigma^2 = \sigma^2_0 \quad \text{ contro l'alternativa } \quad H_1 : \sigma^2 \not = \sigma^2_0 \]
    \centerline{con un valore di $\boldsymbol{\sigma^2_0}$ prefissato} \\[2ex]
    Otteniamo ora il test, abbiamo una distribuzione \textit{chi-quadro} con $n-1$ gradi di libertà, quindi quando $H_0$ è vera:\
    \[ \frac{S^2}{\sigma^2_0}(n-1) \sim \mathcal{X}^2_{n-1} \]
    e quindi otteniamo:
    \[ P_{H_0} \left( \mathcal{X}^2_{1- \frac{\alpha}{2}, n-1} \leq \frac{S^2}{\sigma^2_0} (n-1) \leq \mathcal{X}^2_{\frac{\alpha}{2}, n -1} \right) = 1 - \alpha \]
    Queste sono infine le nostre regole da adottare
    \[ \text{si accetta } H_0 \text{ se } \mathcal{X}^2_{1-\frac{\alpha}{2}, n-1} \leq \frac{S^2}{\sigma^2_0} (n-1) \leq \mathcal{X}^2_{\frac{\alpha}{2}, n-1} \]
    \[ \text{si rifiuta } H_0 \text{ negli altri casi} \]
    Il \textit{p-dei-dati} del test è il seguente:
    \[ \boldsymbol{p-dei-dati} = 2 \min \{P(\mathcal{X}^2_{n-1} \leq c), \quad 1-P(\mathcal{X}^2_{n-1} \leq c)\} \]
    \subsection{Verifica di due popolazione normali che hanno la stessa varianza}
    Abbiamo $X_1, X_2, \ldots, X_n$ e $Y_1, Y_2,\ldots, Y_m$ sono due campioni \textit{normali} \textbf{indipendenti}, con $\mu_x, \sigma^2_x \quad$ e $\quad \mu_y, \sigma^2_y$ incogniti, vediamo le verifiche dell'ipotesi:
    \[ H_0 : \sigma^2_x = \sigma^2_y \quad \text{ contro } \quad H_1 : \sigma^2_x \not = \sigma^2_y \]
    Le due \textit{varianza campionarie} sono:
    \[ S^2_x = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X})^2 \]
    \[ S^2_y = \frac{1}{m-1} \sum_{j=1}^{m} (Y_j - \overline{Y})^2 \]
    Abbiamo una distribuzione F con parametri $n-1$ e $m-1$ quando $H_0$ è vera:
    \[ \frac{S^2_x}{S^2_y} \sim F_{n-1, m-1} \]
    e ne deduciamo che:
    \[ P_{H_0} \left( F_{1- \frac{\alpha}{2}, n-1, m-1} \leq \frac{S^2_x}{S^2_y} \leq F_{\frac{\alpha}{2}, n-1, m-1} \right) = 1 - \alpha \]
    Le nostre regole da adottare sono:
    \[ \text{si accetta } H_0 \text{ se } F_{1-\frac{\alpha}{2}, n-1, m-1} \leq \frac{S^2_x}{S^2_y} \leq F_{\frac{\alpha}{2}, n-1, m-1} \]
    \[ \text{si rifiuta } H_0 \text{ negli altri casi} \]
    Il test del \textit{p-dei-dati} è dato da:
    \[ \boldsymbol{p-dei-dati} = 2\min \{ P(F_{n-1,m-1} \leq \upsilon), \quad 1-P(F_{n-1,m-1} \leq \upsilon )\} \]
    \paragraph{Nota:} il test \textbf{impone} di rifiutare $H_0$ ogni volta che il \textit{livello di significatività} $\alpha$ è \textit{maggiore o uguale} al p-dei-dati
    \paragraph{Esempio} Vengono eseguiti 10 esperimenti nel primo caso e 12 nel secondo, con le seguenti varianze campionarie $S^2_1 = 0.14$ e $S^2_2 = 0.28$, possiamo rifiutare ad un livello di significatività del 5\% ? \\
    Calcoliamo la funzione di ripartizione delle \textit{distribuzioni F}, quindi:
    \[ P(F_{9,11} \leq 0.5) \approx 0.154 \]
    Quindi ora calcoliamo il \textit{p-dei-dati}
    \[ p-dei-dati \approx 2 \min(0.154, 0.846) = 0.308 \]
    \centerline{L'ipotesi nulla \textbf{deve essere accettata. }}
    \subsection{La verifica di ipotesi su una popolazione di Bernoulli}
    Il numero di difetti in un campione di $n$ pezzi ha una distribuzione \textit{binomiale} di parametri $(n, p)$, le verifiche dell'ipotesi sono le seguenti:
    \[ H_0 : p \leq p_0 \quad \text{ contro l'alternativa } \quad H_1 : p > p_0 \]
    \centerline{$p_0$ è un \textit{valore assegnato}}
    \[ \ev{X} = np \]
    \[ Var(X) = np (1-p) \]
    \[ \frac{X - np}{\sqrt{np_0 (1- p_0)}} \sim \mathcal{N}(0,1) \]
    --------------
    \[ \text{si accetta } H_0 \text{ se } \frac{X - np_0}{\sqrt{np_0 (1- p_0)}} \leq z_\alpha \]
    \[ \text{si rifiuta } H_0 \text{ se } \frac{X - np_0}{\sqrt{np_0 (1- p_0)}} > z_\alpha \]
    \section{AN.O.VA} 
    \definizione Analysis of variance, ci serve per confrontare più gruppi diversi per esempio per capire se hanno \textit{medie uguali}
    \[ Z_i := \frac{X_i - \mu_i}{\sigma^2} \sim \mathcal{N}(0,1) \]
    \centerline{le seguenti variabili aleatorie sono \textit{normali standard} e quindi:} \\[2ex]
    Abbiamo $m$ gruppi formati da $n$ oggetti. ogni gruppo rappresenta una variabile aleatoria $X_i \sim \mathcal{N}(\mu, \sigma^2)$
    \[ \sum_{i=1}^{N} Z^2_i = \sum_{i=1}^{N} \frac{(X_i - \mu_i)^2}{\sigma^2} \sim \mathcal{X}^2_N \]
    Essa è una \textit{chi-quadro} con $N$ gradi di libertà, non stimiamo direttamente le $\mu_i$ ma usiamo il fatto che queste sono combinazione lineari di $k$ \textit{parametri incogniti} \\
    In questa ipotesi possiamo dimostrare ciò:
    \[ \sum_{i=1}^{N} \frac{(X_i - \hat{\mu_i})^2}{\sigma^2} \sim \mathcal{X}^2_{N-k} \]
    \centerline{dove \textit{N} sono gli oggetti totali mentre \textit{k} sono i gruppi} \\[2ex]
    Prendiamo $\mu$ come \textit{unico parametro da stimare} cosi che $k=1$ se sostituiamo $\mu$ con $\overline{X}$ che è il suo stimatore, troviamo questa espressione:
    \[ \sum_{i=1}^{N} \frac{(X_i - \overline{X})^2}{\sigma^2} = \frac{N-1}{\sigma^2} \cdot \frac{1}{N-1} \sum_{i=1}^{N}(X_i - \overline{X})^2 = \boldsymbol{\frac{S^2}{\sigma^2}(N-1)} \]
    \subsection{Anova a 1 via}
    In questo caso noi abbiamo $m$ campioni \textit{indipendenti}, formati da $n$ variabili aleatorie con media che \textbf{dipende} dal campione e varianza \textit{fissata} \\
    Denotiamo $X_{ij} \quad i = 1,\ldots, m$ con quello che indica il campione mentre con $j=1, \ldots, n$ indichiamo la posizione all'interno del campione stesso \\[2ex] 
    I parametri $\mu_1, \mu_2, \ldots, \mu_m$ e $\sigma$ sono incogniti, il nostro scopo è quello di verificare l'ipotesi nulla:
    \[ H_0 : \mu_1 = \mu_2 = \cdots = \mu_m \]
    e la sua controparte:
    \[ H_1 : \mu_1 \not= \mu_2 \not= \cdots \not= \mu_m \]
    Dato che ci sono $nm$ \textit{variabili aleatorie indipendenti} la \textit{somma dei quadrati} è una distribuzione \textit{chi-quadro} con $nm$ gradi di libertà:
    \[ \sum_{i=1}^{m} \sum_{j=1}^{n} \frac{(X_{ij} - \ev{X_{ij}})^2}{\sigma} = \sum_{i=1}^{m} \sum_{j=1}^{n} \frac{(X_{ij} - \mu_i)^2}{\sigma^2} \sim \mathcal{X}^2_{nm} \]
    come stimatori degli $m$ usiamo le medie campionarie dei singoli campioni di dati; in particolare $\boldsymbol{X_{i*}}$ denoterà quella del campione i-esimo:
    \[ X_{i*} := \frac{1}{n} \sum_{j=1}^{n} X_{ij} \]
    Siccome $X_{i*}$ è uno stimatore di $\mu_i$ lo \textbf{sostituiamo} nell'equazione di sopra, quindi:
    \[ \sum_{i=1}^{m} \sum_{j=1}^{n} \frac{(X_{ij} - X_{i*})^2}{\sigma^2} = \frac{SS_W}{\sigma^2} \sim \mathcal{X}^2_{nm - m} \]
    \centerline{Essa rappresenta una \textit{chi-quadro} con $nm-m$ gradi di libertà} \\[2ex]
    Calcoliamo ora la media di $\boldsymbol{SS_W}$ otteniamo che:\
    \[ \ev{\frac{SS_W}{\sigma^2}} = nm - m \quad \text{ ovvero } \quad \ev{\frac{SS_W}{nm-m}} = \sigma^2 \]
    \centerline{Cosi abbiamo trovato il primo stimatore di $\sigma^2$} \\[2ex]
    Fino ad ora abbiamo supposto che $H_0$ fosse vera o meno.
    \subsubsection{Stima di $\sigma^2$ valida solo quando $\mu_i = \mu$}
    In questi casi tutti gli stimatori $X_{1*}, X_{2*}, \ldots, X_{m*}$ sono normali di media $\mu$ e varianza $\sigma^2/n$, la loro somma dei quadrati è la seguente:
    \[ \sum_{i=1}^{m} \frac{(X_{i*} - \ev{X_{i*}})^2}{Var(X_{i*})} = \sum_{i=1}^{m} \frac{(X_{i*} - \mu)^2}{\sigma^2 / n} \sim \mathcal{X}^2_m \]
    \centerline{questa è una \textit{chi-quadro} con $m$ gradi di libertà}
    Abbiamo bisogno però di uno \textit{stimatore} di $\mu$, e la loro media campionaria risulta essere la scelta migliore, quindi:
    \[ X_{**} := \frac{1}{nm} \sum_{i=1}^{m} \sum_{j=1}^{n} X_{ij} = \frac{1}{m} \sum_{i=1}^{m} X_{i*} \]
    Nell'equazione sopra ora andiamo quindi a sostituire $\mu$ con $X_{**}$ e otteniamo (quando $H_0$ è vera) 
    \[ \sum_{i=1}^{m} \frac{(X_{i*} - X_{**})^2}{\sigma^2 / n} = \frac{SS_b}{\sigma^2} \sim \mathcal{X}^2_{m-1} \]
    Dove $\boldsymbol{SS_b}$ è:
    \[ SS_b := n \sum_{i=1}^{m} (X_{i*} - X_{**})^2 \]
    Quindi, riassumento, quando $H_0$ è vera:
    \[ \ev{\frac{SS_b}{\sigma^2}} = m -1 \quad \text{ ovvero } \quad \ev{\frac{SS_b}{m-1}} = \sigma^2 \]






















    n
    La prima legge di Newton dice che la forza con cui due corpi si attragono è direttamente proporzionale al prodotto delle due masse ed inversamente proporzionale al quadrato della loro distanza.
    \subsubsection{Legge di gravitazione di Newton}\label{sec:legge-di-gravitazione-di-newton}\[
     F = G\frac{m_1m_2}{r^2} 
    \]
    In quest'espressione $G$ rappresenta la costante gravitazionale:
    \begin{split} 
    G &= 6.67 \cdot 10^{-11} N \cdot \frac{m^2}{kg^2} \\
    & = 6.67 \cdot 10^{-11} \frac{m^3}{kg}\cdot s^2
    
    
    \end{split}\section{Regressione lineare}\label{sec:regressione-lineare}
    Molti problemi di statistica prevedono una singola variabile $Y$ di \emph{risposta} e un certo numero di variabili $\boldmath{x_1, x_2, \ldots, x_r}$ di \emph{ingresso}. La \emph{risposta} è in funzione dei dati, $Y$ è anche detta \emph{variabile dipendente}, mentre le $x_i$ sono le \emph{variabili indipendenti}.\\
    La più semplice relazione potrebbe essere quella lineare:
    \[
     Y = \beta_0 + \beta_1x_1 + \ldots  + \beta_rx_r 
    \]
    (centrare) Dove $\beta_0, \beta_1, \ldots, \beta_r$ sono costanti.
    
    Predire esattamente le $\beta_i$ non è possibile, quindi all'equazione si aggiunge un \emph{errore casuale} denominato \emph{e}:
    \[
     Y = \beta_0 + \beta_1x_1 + \ldots + \beta_rx_r + e 
    \]
    La variabile $e$ ha distribuzione normale standard. $e \sim \mathcal N(0,1)$
    
    L'equazione qui sopra è chiamata \emph{equazione di regressione lineare}.
    questa esprime la regressione di \emph{Y} rispetto alle variabili indipendenti $x_1, x_2, \ldots, x_r$, mentre le costanti $\beta_0, \beta_1, \ldots, \beta_r$ sono dette \emph{coefficienti di regressione} e vanno normalmente stimate.
    
    Un equazione di regressione si dice \emph{semplice} se $r = 1$, e quindi c'è solo una variabile indipendente, negli altri casi si dice regressione \emph{multipla}.
    Quindi la relazione diventa:
    \[
     Y = \alpha + \beta x + e 
    \]
    Indichiamo con $A$ e $B$ (variabili aleatorie) degli stimatori di $\alpha, \beta$.
    L'equazione diventerà:
    \[
     Y = A + Bx + e 
    \]
    Per avvicinarsi alla retta reale la quantità $(Y_i - A + Bx_i)^2$ deve risultare minima.
    (rappresenta il quadrato della differenza tra predizione e valore osservato)
    
    Quindi:
    \[
     SS := \sum_{i = 1}^{n}(Y_i-A-Bx_i)^2  
    \]
    Ricaviamo $A$ e $B$ tale per cui la $SS$ risulta minima:
     
    \begin{split}
    &B = \frac{\sum_ix_iY_i - \overline{x}\sum_i Y_i }{\sum_i x_i^2 - n \overline x^2} \\ \\
    &A = \overline Y - B \overline x
    \end{split}
    La retta $Y = A + Bx + e$ è \emph{la stima della retta di regressione}.
    
    \begin{figure}
    \includegraphics{Images/Pasted image 20230524120359.png}
    \caption{undefined undefined}\n\end{figure}
    
    \section{Distribuzione degli stimatori}\label{sec:distribuzione-degli-stimatori}
    $Y_1, Y_2, \ldots, Y_n$ sono indipendenti con distribuzione normale. $Y_i \sim \mathcal N(\alpha + \beta x_i, \sigma^2)$
    $B$ e $A$ anch'esse hanno distribuzione normale.
    
    $B$ è uno stimatore non distorto di $\beta$ perché il suo valore atteso è uguale a $\beta$:
    \[
     
    E[B] = \beta
    \]
    Quindi la sua varianza risulta essere:
    \[
    \text{Var}(B) = \frac{\sigma^2}{\sum_i x^2_i -n\overline x^2}
    \]
    Anche $A$ è uno stimatore non distorto di $\alpha$ perché il valore atteso è $\alpha$:
    \[
     
    E[A] = \alpha 
    \]
    Varianza di $A$:
    \[
    \text{Var}(A) = \frac{\sigma^2 \sum_i x_i^2}{n(\sum_i x_i^2 - n \overline x^2)}
    \]
    Somma dei quadrati dei residui è usata per stimare la varianza degli errori, $\sigma^2$:
    \[
     SS_R := \sum_i^n(Y_i - A - Bx_i)^2 
    \]
    La $SS_R$ ha distribuzione chi-quadro, con $n-2$ gradi di libertà:
    \[
     \frac{SS_R}{\sigma^2} \sim \mathcal{X}^2_{n-2}  
    \]
    Il valore atteso della $SS_R$ è uguale alla varianza, quindi è uno stimatore non distorto del parametro incognito $\sigma^2$:
    \[
     
    
    E[\frac{SS_R}{\sigma^2}] = n - 2 \quad \Rightarrow \quad E[\frac{SS_R}{n-2}] = \sigma^2
    
    \] 
    \begin{split}
    
    &S_{xY} := \sum_{i=1}^n x_i Y_i - n \overline x \overline Y \quad \text{dispersione di $x$ e $Y$}\\
    &S_{xx} := \sum_{i = 1}^{n} x^2_i - n \overline x^2 \quad \text{dispersione di $x$ e $x$}\\
    &S_{YY} := \sum_{i=1}^{n} Y_i^2 - n \overline Y^2 \quad \text{dispersione di $Y$ e $Y$}
    
    \end{split}
    Possiamo riscrivere B come:
    \[
     
    B = \frac{\sum_ix_iY_i - \overline{x}\sum_i Y_i }{\sum_i x_i^2 - n \overline x^2} \quad \Rightarrow \quad B=\frac{S_{xY}}{S_{xx}}
    \]\subsubsection{In generale}\label{sec:in-generale}
    Nel caso in cui $Y_i, i = 1,2,3,\ldots,n$  siano normali indipendenti con media $\alpha + \beta x_i$ e varianza $\sigma^2$, gli stimatori dei minimi quadrati per $\beta$ e $\alpha$ sono:
    \[
     B = \frac{S_{xY}}{S_{xx}} \quad \quad A = \overline Y - B \overline x $$ e hanno distribuzione: $$ B \sim \mathcal N(\beta,\frac{\sigma^2}{S_{xx}}) \quad \quad A \sim \mathcal N(\alpha,\frac{\sigma^2 \sum_ix_i^2}{nS_{xx}}) 
    \]
    La somma dei quadrati dei residui è calcolata tramite:
    \[
     SS_R = \frac{S_{xx}S_{YY}- S_{xY}^2}{S_{xx}}
    \]
    La $SS_R$ ha distribuzione:
    \[
     \frac{SS_R}{\sigma^2} \sim \mathcal X^2_{n-2}
    \]\section{Inferenza sui parametri della regressione}\label{sec:inferenza-sui-parametri-della-regressione}
    Quanto sono distanti $A$ e $B$ da $\alpha$ e $\beta$?
    Dobbiamo vedere l'intervallo di confidenza
    \subsection{Inferenza su $\beta$}\label{sec:inferenza-su-beta}
    Formula dell'intervallo di confidenza di $\beta$:
    \[
     \beta \in B \pm \sqrt{\frac{SS_R}{(n-2)S_{xx}}} \sim t_{\frac{\alpha}{2}, n-2}
    \]
    Estesa:
    \[
     P\Big(B - t_{\frac{\alpha}{2},n-2} \cdot \frac{\sqrt{SS_R}}{(n-2)S_{xx}} < \beta < B + t_{\frac{\alpha}{2},n-2} \cdot \frac{\sqrt{SS_R}}{(n-2)S_{xx}}\Big) 
    \]
    ==Importante==: $\alpha$ ==NON== è il parametro della regressione, ma è il livello di confidenza.
    \subsection{Inferenza su $\alpha$}\label{sec:inferenza-su-alpha}
    Formula dell'intervallo di confidenza di $\alpha$:
    \[
     \alpha \in A \pm \frac{SS_R\sum_{i}{}x_i^2}{\sqrt{n(n-2)S_{xx}}} \sim t_{\frac{\alpha}{2},n-2}
    \]
    Dove la prima $\alpha$ è il coefficiente della retta mentre $\alpha$ nella t è il livello di confidenza.
    \subsection{Inferenza su $\alpha + \beta x_0$}\label{sec:inferenza-su-alpha--beta-x_0}
    Il valore atteso di $A + B x_0$ è uguale a $\alpha + \beta x_0$  quindi è uno stimatore non distorto:
    \[
     E[A +B x_0] = E[A] + x_0E[B] = \alpha + \beta x_0 
    \]
    La varianza è:
    \[
     \text{Var}(A+Bx_0) = \sigma^2 \Big[\frac{1}{n} + \frac{(\overline x - x_0)^2}{S_{xx}}  \Big] 
    \]
    qual'è la distribuzione di $A+Bx_0$?
    \[
     A+Bx_0 \sim \mathcal N \Big(\alpha+\beta x_0, \sigma^2 \Big[ \frac{1}{n} + \frac{(\overline x - x_0)^2}{S_{xx}} \Big]\Big) 
    \]
    intervallo di confidenza di $\alpha + \beta x_0$:
    \[
     \alpha + \beta x_0 \in A + B x_0 \pm t_{\frac{\alpha}{n}, n-2}\sqrt{\frac{1}{n} + \frac{(x_0- \overline x)^2}{S_{xx}}} \cdot \sqrt{\Big(\frac{SS_R}{n-2} \Big)}
    \]
    $S_{xx}$ risulta piccolo se i punti sono vicini alla media.
    \subsection{Inferenza di $Y_0 = Y(x_0) \rightarrow$ predittivo}\label{sec:inferenza-di-y_0--yx_0-rightarrow-predittivo}
    Nel caso dovessimo prevedere un nuovo elemento della retta di regressione(utilizzando i dati già a disposizione) dobbiamo utilizzare la seguente formula:
    \[
     A+Bx_0 \pm t_{\frac{\alpha}{2},n-2} \cdot \sqrt{\Big(1+\frac{1}{n} + \frac{(x_0-\overline x)^2}{S_{xx}}\Big)\frac{SS_R}{n-2}} 
    \]\subsection{Riassunto:}\label{sec:riassunto}
    \begin{figure}
    \includegraphics{Images/Pasted image 20230524170143.png}
    \caption{undefined undefined}\n\end{figure}
    
    \subsection{Coefficiente di determinazione}\label{sec:coefficiente-di-determinazione}
    Come verifico i miei valori(della retta)? Tramite il coefficiente di determinazione.
    
    Formula del coefficiente di determinazione:
    \[
     R^2 = \frac{S_{YY}-SS_R}{S_{YY}} = 1- \frac{SS_R}{S_{YY}} \quad  \quad 0 \leq R^2 \leq 1 
    \]
    Casi possibili:
    \begin{enumerate}
    \item 
    Se $R^2 = 1$:
    \begin{enumerate}
    \item 
    la dispersione è data solo dalla retta (regressione)
    \end{enumerate}
    \item 
    Se $R^2 = 0$:
    \begin{enumerate}
    \item 
    la dispersione è dovuta solo dal rumore
    \end{enumerate}
    \end{enumerate}
    
    La retta è migliore più $R^2$ è vicino a 1.
    \subsection{Coefficiente di correlazione}\label{sec:coefficiente-di-correlazione}\[
     r = \frac{\sum_i (x_i - \overline x)(y_i - \overline y)}{\sqrt{\sum_i (x_i - \overline x)^2 \sum_i(y_i - \overline y)^2}} = \frac{S_{xY}}{\sqrt{S_{xx}S_{YY}}} 
    \]
    Dimostrazione matematica di $R^2$:
    \[
     r^2 = \frac{S_{xY}^2}{S_{xx}S_{YY}} = \ldots = 1- \frac{SS_R}{S_{YY}}
    \]
    Quindi:
    \[
     |r| = \sqrt{R^2} 
    \]\subsection{Analisi dei residui}\label{sec:analisi-dei-residui}
    Se il nostro modello non segue la forma di una "retta" non possiamo utilizzare la retta di regressione per rappresentare i nostri dati.
    \subsection{Trasformazione al lineare}\label{sec:trasformazione-al-lineare}
    Si può linearizzare tramite diverse funzioni, quella esponenziale in questo modo:
    $W(t) = ce^{-dt}$ dove $e, t$ sono parametri
    
    Calcoliamo il log:
    $\log({W}(t)) \approx \log{(c)}-dt$
    
    Se ora poniamo
    \begin{itemize}
    \item 
    $Y = \log{W(t)}$
    \item 
    $\alpha = \log{c}$
    \item 
    $\beta = -d$
    \end{itemize}
    
    La regressione lineare:
    \[
     Y = \alpha + \beta t + e 
    \]
    Diventa:
    \[
     W(t) \approx e^{A+Bt} 
    \]\subsection{Rimedio al caso eteroschedastico}\label{sec:rimedio-al-caso-eteroschedastico}
    (da rivedere)
    
    $Y_i = \alpha + \beta x_i + e_i$
    $e_i \sim \mathcal N(0,\sigma_i^2) \rightarrow$ errore in crescita $x$
    
    $\text{Var}(e_i) = \frac{\sigma^2}{W_i}$
    $\sum_i W_i(Y-(A+B_{x_0})$
    \subsection{Regressione lineare multipla}\label{sec:regressione-lineare-multipla}\[
    Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \ldots + \beta_k x_k +e
    \]\[
     \min\sum_i(Y_i -(\beta_0+\beta_1x_{i1} + \beta_2x_{i2} \ldots + \beta_kx_{ik})) 
    \]\subsection{Regressione (lineare) polinomiali}\label{sec:regressione-lineare-polinomiali}
    Nel caso in cui il nostro modello non può essere approssimato con un modelli lineari, si possono utilizzare relazioni polinomiali:
    \[
     Y = \beta_0 + \beta_1x+\beta_2x^2 +\ldots+\beta_kx^k + e
    \]
    Dobbiamo minimizzare:
    \[
     \sum_i^n (Y_i-B_0-B_1x_1-\ldots-B_rx_i^r)^2 
    \]
    Per determinare $\beta_0, \beta_1, \ldots, \beta_r$ dobbiamo



























    \paragraph{Esercizio 1} Probabilità che Oneto dia 30L (Lode) \\
    $n = 120$ \\
    $\sum_{i}^{120} x_i = 18$ \\
    $\hat{P} = \frac{18}{120} = 0.15 \rightarrow 15\%$ \\
    \paragraph{Esercizio 2} N studenti da 30 e lode \\
    $n_1 = 18 \leftarrow \text{Oneto}$ \\ 
    $n_2 = 20 \leftarrow \text{Anguita}$ \\
    $n_{1,2} = 10 \leftarrow \text{30L sia con Oneto che con Anguita}$ \\
    $N= ? \quad \text{Studenti da \textbf{30 e Lode}}$ \\
    \begin{minipage}{0.30\textwidth}
        \[ \hat{P_1} \approx \frac{n_1 2}{n_2} \]
    \end{minipage}
    \begin{minipage}{0.30\textwidth}
        \[ \hat{P_1} \approx \frac{n_1}{N} \]
    \end{minipage}
    \begin{minipage}{0.30\textwidth}
        \[ \frac{n_{1,2}}{n_2} = \frac{n_1}{N} \]
    \end{minipage} \\
    $ \Longrightarrow N= \frac{n_1 \cdot n_2}{n_{1,2}} \rightarrow \frac{18 \cdot 20}{10} = 36 $

    \paragraph{Esercizio 3} Stima del numero di incidenti medio in auto n = 10 \\
    $x_1 = \{ 4,0,6,5,2,1,2,0,4,3 \}$ \\
    $\hat{\lambda} = \frac{\sum_{i}^{} x_i}{n} = \frac{27}{10} = 2.7$
    \[ P\{x \leq 2 \} = e^{-2.7} (\frac{2.7^0}{0!} + \frac{2.7^1}{1!} + \frac{2.7^2}{2!}) \approx .4936 \rightarrow 49.36\% \]
    \centerline{Probabilità che non ci siano più di \textbf{2 incidenti} }
    
    \paragraph{Esercizio gaussiana} primo \\
    $x_1 = 1.7$ \\
    $x_2 = 1.82$ \\
    $x_3 = 1.73$\\
    $x_4 = 1.7$\\
    $x_5 = 1.8$ \\
    \[ \hat{\mu} = \frac{\sum_{i}^{} x_i}{n} = \frac{1.7 + 1.82 + 1.73 + 1.7 + 1.8}{5} = 1.75 \]
    \[ \hat{\sigma} = \sqrt{\frac{0.05^2 + 0.07^2 + 0.02^2 + 0.05^2 + 0.05^2}{5}} \approx 0.051 \]
    \paragraph{Esempio:} Sistema di comunicazione
    $\sigma^2 = 4 \quad n = 9$
    \[ x_1 = \{ 5.85, 12, 15, 7, 9, 7.5, 6, 5, 10.5 \} \]
    \[ \hat{\mu} = \frac{1}{n} \sum_{i}^{n} x_i = \frac{1}{9} \sum_{i}^{n} x_i = \frac{81}{9} = 9 \] \\
    \begin{equation*}
        \begin{aligned}
            & P\left(9-1.96 \frac{\sigma}{\sqrt{m}}<\mu<9+1.96 \frac{\sigma}{\sqrt{m}}\right)=0.95 \\
            & p\left(9-1.96 \frac{2}{3}<\mu<9+1.96 \frac{2}{3}\right)=0.95 \\
            & \longrightarrow[7.693,10.31] \rightarrow \mu \text { si trova tra 7.693 e 10.31} \\
        \end{aligned}
    \end{equation*}
    \paragraph{In generale} Prob = $1-\alpha$
    \[ (\overline{x} - z_a \frac{\sigma}{\sqrt{n}}, \overline{x} + z_a \frac{\sigma}{\sqrt{n}} ) \rightarrow \text{Si rileva dalle tavole} \]

    \subsection{Intervalli di confidenza (Unilaterali)}
    \subsection{Esempio:} Pesca stagionale dei salmoni (\textit{Fisso intervallo -> trovo $n$}) \\
    \text{Ad ogni stagione il peso medio dei salmoni è diverso ma $\sigma = 0.3$ Kg} \\
    \text{Intervallo di confidenza al 95\%, quindi $\alpha = 0.05$}
    \[ (\overline{X} - 1.96 \frac{\sigma}{\sqrt{n}}, \overline{X} + 1.96 \frac{\sigma}{\sqrt{n}}) \]
    \[ 1.96 \frac{\sigma}{\sqrt{n}} \geq 0.1 \quad \sqrt{n} \geq \frac{1.96}{0.1} \sigma \]
    \[ n \geq (\frac{1.96}{0.1} 0.3)^2 = 5.88^2 \approx 34.6 \leftarrow \text{salmoni} \]
    \subsection{Intervallo di confidenza} con \textit{media} e \textit{varianza} \textbf{incognite}
    \paragraph{Esempio:} Trasimttente ($\mu$) e ricevitore ($\mu$ + rumore)
    \[ 95\% (7.69, 10.31) \quad \hat{\mu} = 9, \sigma^2 = 4 \]
    $X_i \{ 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5 \}$ \\
    $\hat{\mu} = \overline{X} = \frac{1}{9} \sum_{i}^{n} X_i = \frac{81}{9} = 9$ \\
    $s^2 = \frac{1}{8}\sum_{i}^{}(X_i^2-9.81) \approx 9.5 \quad s = 3.082$
    \[ \mu \in (9-2.306 \frac{3.082}{3}, 9 + 2.306 \frac{3.082}{3}) = (6.63, 11.37) \]
    \centerline{Si può dimostrare che $T_{\frac{\alpha}{2} \cdot n - 1} \ev{S} \geq z_\alpha\sigma$}

    \subsection{Intervallo di confidenza nella varianza}
    \paragraph{Esempio:} Laminatoio n = 4
    $X_i = \{ 0.123, 0.124, 0.126, 0.12 \} \text{spessore in \textbf{mm}}$ \\
    \paragraph{Svolgimento}
    \begin{equation*}
        \begin{aligned}
            \frac{1}{4} \sum_{i}^{4} X_i = \frac{0.493}{4} = 0.12325 \\
            \frac{1}{4-1} \sum_{i}^{4}\left ( X_i - 0.12325 \right )^2 = 1.875 \cdot 10^{-5} \\
            \sigma^2 \in \left ( \frac{s^2(n-1)}{9.348}, \frac{s^2(n-1)}{0.216} \right )
        \end{aligned} 
    \end{equation*}
    \centerline{Dove \textbf{9.348} e \textbf{0.216} sono ricavati dalle tabelle}
    Facciamo la radice: \\
    $\sigma \in (0.0014, 0.0093) \rightarrow 95\% $
    \subsection{Intervallo di confidenza}della differenza di due medie: \\ \\
    \begin{minipage}{0.5\textwidth}
        \centerline{\textbf{N} campioni}
        \[ X_i \sim \mathcal{N}(\mu_1, \sigma_1^2) \]
        \[ \overline{X} = \frac{1}{n} \sum_{i}^{n} X_i \]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centerline{\textbf{M} campioni}
        \[ Y_i \sim \mathcal{N}(\mu_2, \sigma_2^2) \]
        \[ \overline{Y} = \frac{1}{m} \sum_{i}^{m} Y_i \]
    \end{minipage}
    \[ \overline{X} - \overline{Y} \sim \mathcal{N}\left ( \mu_1 - \mu_2, \frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m} \right ) \]
    \[ \mathcal{N}(0,1) \sim \frac{\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}} \]
    $\mu_1 - \mu_2 \in \left ( \overline{X} - \overline{Y} - z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}, \overline{X} - \overline{Y} + z_{\frac{\alpha}{2}}, \overline{X} - \overline{Y} + z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}} \right )$ \\ \\
    Se $\sigma_1^2, \sigma_2^2$ non sono note: \\ \\
    \begin{minipage}{0.5\textwidth}
        $S_1^2 = \frac{1}{n-1} \sum_{i}^{n}(X_i - \overline{X})$ \\ \\
        $(n-1) \frac{S_1^2}{\sigma_1^2} \sim \mathcal{X}_{n-1}^2$
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        $s_2^2 = \frac{1}{m - 1} \sum_{i}^{m} (X_i - \overline{Y})$ \\ \\
        $(n-1) \frac{s_2^2}{\sigma_2^2} \sim \mathcal{X}_{n-1}^2$
    \end{minipage} \\ \\
    \textbf{Possiamo andare avanti solo se \boldsymbol{$\sigma_1^2 = \sigma_2^2 = \sigma^2$}}
    \[ (n-1) \frac{s_1^2}{\sigma^2} + (n-1) \frac{s_2^2}{\sigma^2} \sim \mathcal{X}_{n+m-2}^2 \]
    \[ \frac{\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}{\sqrt{\sigma^2 (\frac{1}{n} + \frac{1}{m})}} \longrightarrow \frac{\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}{\sqrt{S_p(\frac{1}{n} + \frac{1}{m})}} \]
    \[ \sim \mathcal{N}(0,1) \quad \quad \quad \quad \quad \sim T_{n+m-2}\]
    $S_p = \frac{(n-1) S_1^2 + (m-1) S_2^2}{n+m-2}$ \\ \\
    \centerline{Se $\sigma$ sono ignote ma uguali}
    $\mu_1 - \mu_2 \in (\overline{X} - \overline{Y} -T_{\frac{\alpha}{2},n+m-2} \sqrt{s^2(\frac{1}{n} + \frac{1}{m})})$ \\
    $\overline{X} - \overline{Y} + T_{\frac{\alpha}{2}, n+m-2} \sqrt{s^2 (\frac{1}{n} + \frac{1}{m})}$
    \subsection{Intervallo di previsione}
    \paragraph{Esempio} smartwatch contapassi $\boldsymbol{n = 7}$ \\ \\
    \centerline{\begin{minipage}{0.3\textwidth}
        $LUN \quad 6922 \quad X_1$ \\
        $MAR \quad 5333 \quad X_2$ \\
        $MER \quad 7420 \quad X_3$ \\ 
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        $GIO \quad 7432 \quad X_4$ \\
        $VEN \quad 6252 \quad X_5$ \\
        $SAB \quad 7005 \quad X_6$ \\
    \end{minipage}} \\
    \[ DOM \quad 6752 \quad X_7 \]
    \[ \overline{X}_n = \frac{1}{n} \sum_{i}^{m} X_i = \frac{47016}{7} \approx 6717 \]
    \[ 1-\alpha = 95\% \quad \alpha = 5\% \]
    \[ t_{0.0025,6} = 2.997 \]
    \[ S_n = \sqrt{S_n^2} = 7.333.8 \]
    \[ x_{n+1} \in (6717 -2.447 \cdot 733397 \sqrt{1 + \frac{1}{7}}, 6717 + 2.447 \cdot 73397 \sqrt{1 + \frac{1}{7}}) \]
    $X_{n+1} \in (9796, 8637) \mu \in (6037, 7396)$
    \subsection{Qualità di uno stimatore}
    $X = X_1 \ldots X_n \quad \theta \leftarrow \text{parametro} \quad \quad d(x) \leftarrow \text{stimatore di $\theta$}$ \\
    \begin{minipage}{0.5\textwidth}
        \[ \boldsymbol{(d(x) - \theta)^2}\] Errore Quadratico (\textit{misura della qualità})
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \[ \boldsymbol{\ev{(d(x)-\theta)^2}} \] Errore Quadratico Medio (\textit{M.S.E})
    \end{minipage} \\
    Rischio $r(d, \theta) = \ev{(d-\theta)^2}$ 
    Lo stimatore "ottimo" sarà quello con il rischio minimo -> d con r minimo $\theta$
    \paragraph{Esempio}
    $d^* (x) = 4 \quad \text{se} \theta = 4 \Rightarrow d^* = \text{stimatore ottimo(per tutti gli altri valori non va bene)}$
    \subsection{Proprietà di uno stimatore}
    Def: $b_\theta(d) = \ev{d} - \theta \rightarrow \text{bias o polarizzazione}$
    Uno stimatore non è \textbf{polarizzato} se $b_\theta(d) = 0$
    \paragraph{Esempio}:
    $X_1 \ldots X_n \quad \theta \text{media}$ \\
    $d_1(X_1 \ldots X_n) = X_1$ \\
    $d_2(X_1 \ldots X_n) = \frac{X_1 + X_2}{2}$ \\
    $d_3(X_1 \ldots X_n) = \frac{X_1 + X_2 + \ldots X_n}{n}$
    \centerline{Tutti questi sono \textbf{unbiased}}
    \subsection{Stimatore unbaieseo}
    $r(d, \theta) = \ev{(d(x) - \theta)^2} = \ev{(d(x) - \ev{d(x)})^2} = Var(d)$ \\
    tra gli stimatori non polarizzati di ottimo è quello con la varianza minima
    \subsection{Valutazione di uno stimatore}
    \[ X = X_1 \ldots X_n \quad \theta = ? \]
    \centerline{Dove $\theta$ è un \textit{parametro} e $d(x)$ è uno \textit{stimatore} di $\theta$} \\ \\
    $r(d, \theta) \text{(mse) rischio} \quad \quad b_\theta(d) = \ev{d} - \theta$ \\
    se $b_\theta(d) = 0 \Rightarrow r(d, \theta) = Var(d)$ \\
    \centerline{se $b_\theta(d) \not = 0$ ? $r(d, \theta)$ = ?}
    \begin{equation*}
        \begin{split}
            r(d, \theta) = \ev{(d(x) - \theta)^2} &= \ev{(d(x) - \ev{d} + \ev{d} - \theta)^2} \\
            &= \ev{(d - \ev{d})^2 + (\ev{d} - \theta)^2 - 2(d - \ev{d}) (\ev{d} - \theta)} \\
            &= \ev{(d - \ev{d})^2} + \ev{(\ev{d} - \theta)^2} - 2(\ev{d} - \theta) \cdot \ev{(d- \ev{d})}
        \end{split}
    \end{equation*}
    \begin{equation*}
        \begin{split}
           r(d, \theta) &= \ev{(d - \ev{d})^2} + \ev{(\ev{d} - \theta)^2} \\
           &= Var(d) + b_\theta(d)^2 \leftarrow \text{bias$^2$}
        \end{split}
    \end{equation*}
    \subsection{Esempio:} Stimatore della media di una \textit{distribuzione uniforme} \\\\
    \begin{minipage}{0.4\textwidth}
        $\ev{X_i} = \theta / 2$ \\
        $X_1, X_2 \ldots X_n$
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        $d_1 = 2 \frac{1}{n} \sum_{i}^{n} X_i$ \\
        $d_2 = \max X_i$
    \end{minipage} \\[4ex]
    $d_1 : \ev{d_1} = \frac{2}{n} \sum_{i}^{} \ev{X_i} = \frac{2}{n} n \frac{\theta}{2} = \theta$ \\
    $r(d_1, \theta) = Var(d_1) = \frac{4}{n^2} n Var(X_i) = \frac{4}{n} \frac{\theta^2}{12} = \boldsymbol{\frac{\theta^2}{3n}} \Leftarrow$ \textit{Unbiased} \\[4ex]
    \begin{equation*}
        \begin{split}
            F_2(x) = P_r \{ d_2(x) \leq x \} &= P_r \{ \max X_1 \leq x \} \\
            &= P_r \{ X_1 \leq, \forall i \in 1 \} = \prod_{i = 1}^{n} P_r \{ X_i \leq x \} = (\frac{x}{\theta})^n \\
            f_2(x) &= \frac{d}{dx} F_2(x) = n \frac{x^{n-1}}{\theta^n} \quad x \leq \theta
        \end{split}
    \end{equation*} \\[6ex]
    $\displaystyle \ev{d_2} = \int_{0}^{\theta} x f_x(x) \, dx = \int_{0}^{\theta} \frac{n}{\theta^n} x^n \, dx = \frac{n}{\theta^n} \left[ \frac{x^{n+1}}{n+1} \bigg\rvert_{0}^{\theta}\right] = \frac{n}{n+1} \theta$ \\[2ex]
    $\displaystyle \ev{d_2^2} = \frac{n}{\theta^n} \int_{0}^{\theta} x^2 f(x) \, dx = \frac{n}{\theta^n} \int_{0}^{\theta} x^{n+1} \, dx = \frac{n}{\theta^n} \left[ \frac{x^{n+2}}{n+2} \bigg\rvert_{0}^{\theta}\right] = \frac{n}{n+2} \theta^2$ \\[2ex]
    $Var(d_2) = \ev{d^2} - \ev{d_2}^2 = \frac{n}{n+2}\theta^2 - \frac{n^2}{(n+1)^2} \theta^2 = \boldsymbol{\frac{n}{(n+2)(n+1)^2} \theta^2}$ \\[4ex]
    $r(d_2, \theta) = Var(d_2) + (\ev{d_2} - \theta)^2 = \boldsymbol{\frac{2 \cdot \theta^2}{(n+1)(n+2)}}$ \\[4ex]
    $n \geq 4 \quad r(d_2, \theta) < r(d_1, \theta) \quad \quad d_3 = \frac{n+1}{n} d_2$

    \paragraph{In sintesi}
    \begin{equation*}
        \begin{aligned}
            r(d_1, \theta) = \frac{\theta^2}{3n} \Leftarrow \text{Unbiased} \\
            r(d_2, \theta) = \frac{2\theta^2}{(n+1)(n+2)} \Leftarrow \text{Biased} \\
            r(d_3, \theta) = \frac{\theta^2}{n^2 + 2n} \Leftarrow \text{Unbiased} \\
            r(d_4, \theta) = \frac{\theta^2}{(n+1)^2} \Leftarrow \text{Biased}
        \end{aligned}
    \end{equation*}
    \section{Test di ipotesi}
    \paragraph{Ipotesi:} Affermazione rispetto a uno o più parametri di una distribuzione
    Ipotesi da confutare: $H_0$ (ipotesi nulla) \\
    \paragraph{Esempio}:
    \begin{equation}
        \begin{aligned}
            X_1 \ldots X_n \sim \mathcal{N}(\mu, \sigma^2) \\
            H_0 : \mu = 0 \\
            H_a : \mu \not = 0
        \end{aligned}
    \end{equation}
    \centerline{Diamo per scontato che l'ipotesi sia \textbf{vera}}
    \centerline{Dobbiamo cercare di \textit{confutarla}}
    \paragraph{Definizione} Regione critica tale che: \\\\
    $(X_1 \ldots X_n) \in C \rightarrow H_0 \text{è rifiutata}$ \\
    $(X_1 \ldots X_n) \not\in C \rightarrow H_0 \text{è accettata}$ \\
    $\alpha = $ Livello di \textbf{significatività} del test ($\alpha = 10\%, 5\% \ldots$)
    \paragraph{Procedimento}
    \begin{itemize}
        \item Fisso alpha
        \item Suppongo che $\alpha$ sia vera
        \item calcolo stima di $\mu$
        \item verifico che non sia "\textit{troppo distante}"
    \end{itemize}
    $X_1 \ldots X_n \sim \mathcal{N}(\mu, \sigma^2)$ \\
    $H_0 : \mu = \mu_0 \quad H_a : \mu \not = \mu_0$ \\
    $\overline{X} = \frac{1}{n} \sum_{i}^{} X_i $
    \paragraph{Regione critica}
    $\{ (X_1 \ldots X_n): \rvert \overline{X} - \mu_0 \rvert > c \}$ \\
    $P_{r_{\mu_0}} \left\{ | \overline{X} - \mu_0 | > c \right\} = \alpha$ \\
    $P_{r_{\mu_0}} \left\{ \frac{|\overline{X} - \mu_0|}{\frac{\sigma}{\sqrt{n}}} > \frac{c}{\frac{\sigma}{\sqrt{n}}} \right\} = \alpha$ \\
    $P_{r_{\mu_0}} \left\{ |z| > z_\alpha \right\} = \alpha$
    \paragraph{Esempio} (5 transimissioni) \\
    \begin{minipage}{0.45\textwidth}
        $\boldsymbol{n=5}$ \\
        $\overline{X} = 9.5$
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        $H_0 : \mu = 8$ \\
        $\alpha = 5\%$
    \end{minipage} \\[4ex]
    Ipotizzando che $H_0$ sia vera:
    \[ \frac{|\overline{X} - \mu|}{\frac{\sigma}{\sqrt{n}}} = \frac{|9.5 - 8|}{\frac{2}{\sqrt{5}}} \approx 1.68 \]
    Se: \\
    $\alpha = P_r$(rifiuto $H_0$ / $H_0$ vera) \\
    $\alpha \uparrow$ più \textit{"facile"} rifiutare l'ipotesi \\
    $\alpha \downarrow$ più \textit{"difficile"} rifiutare l'ipotesi
    \subsection{Metolodogia alternativa}
    \[ Ts = \frac{\overline{X} - \mu_0}{\frac{\sigma}{\sqrt{n}}} \rightarrow \text{Statistica di test} \]
    $P-$value = \textbf{probabilità} di ottenere un valore più "anomalo" di quello osservato
    \paragraph{Esempio:} $X_i \sim \mathcal{N}(\mu, 4)$ \\
    \begin{minipage}{0.5\textwidth}
        \[ n = 5 \]
        \[ \overline{X} = 8.5\]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \[ H_0 : \mu = 8 \]
        \[ H_a : \mu \not = 8\]
    \end{minipage}
    \[ \frac{|\overline{X} - \mu_0|}{\frac{\sigma}{\sqrt{n}}} = \frac{|8.5 - 8|}{\frac{2}{\sqrt{5}}} = \frac{\sqrt{5}}{2} 0.5 \approx 0.559 \]
    \[ P \{ |z| > 0.559 \} = 2P \{ z > 0.559 \} \approx 2 \cdot 0.288 = 0.579 \rightarrow \text{P-value}\] \\[2ex]
    Se $\boldsymbol{\overline{X} = 11.5}$:
    \[ \frac{|\overline{X} - \mu_0|}{\frac{\sigma}{\sqrt{n}}} = \frac{|11.5 - 8|}{\frac{2}{\sqrt{5}}} \approx 3.913 \]
    \[ P \{ |z| > 3.913 \} = 2P \{ z > 3.913 \} \leq 0.00005 \rightarrow \text{\underline{Rifiuto ipotesi $H_0$}}\]
    \subsection{Test di Hp unilaterale}
    \begin{minipage}{0.5\textwidth}
        \[ H_0 : \mu = \mu_0 (\mu \leq \mu_0) \]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \[ H_a : \mu > \mu_0 \]
    \end{minipage}
    \[ C = \{ (X_1 \ldots n) \cdot \overline{X} - \mu_0 > c \} \]
    \[ P_{r_{\mu_0}} \{ \overline{X} - \mu_0 > c \} = P_r \{ \frac{\overline{X} - \mu_0}{\frac{\sigma}{\sqrt{n}}} > \frac{c}{\frac{\sigma}{\sqrt{n}}}\} = P_{r_{\mu_0}} \{ z > z_a \} = \alpha \] \\[2ex]
    \centerline{Statistica test $\boldsymbol{\frac{\overline{X} - \mu_0}{\frac{\sigma}{\sqrt{n}}} \leq z_\alpha}$ accetto}
    \subsection{Test di ipotesi}
    \begin{minipage}{0.2\textwidth}
        $H_0$ \\[2ex]
        $\mu = \mu_0$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        $H_a$ \\[2ex]
        $\mu \not= \mu_0$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        TS \\[2ex]
        $\frac{\overline{X} - \mu_0}{\sigma / \sqrt{1}}$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        Livello $\alpha$ \\[2ex]
        Rifiuto $H_0$ \\
        se $TS > \frac{z\alpha}{2}$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        P - Value \\[2ex]
        $2P(z \geq |TS|)$
    \end{minipage}
    \paragraph{Altre ipotesi}: \\[2ex]
    \begin{minipage}{0.2\textwidth}
        $H_0$ \\[2ex]
        $\mu < \mu_0$ \\
        $\mu \geq \mu_0$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        $H_a$ \\[2ex]
        $\mu > \mu_0$ \\
        $\mu < \mu_0$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        TS \\[2ex]
        $\frac{\overline{X} - \mu_0}{\sigma / \sqrt{2}}$ \\
        \textit{//}
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        Livello $\alpha$ \\[2ex]
        $H_0 \quad z_\alpha > TS$ \\
        $H_0 \quad z_\alpha < -TS$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        P - Value \\[2ex]
        $P(z\geq TS)$ \\
        $P(z \leq TS)$
    \end{minipage}
    \subsection{Uguaglianza media di due popolazioni}
    \begin{minipage}{0.45\textwidth}
        $X_1 \ldots X_n \sim \mathcal{N}(\mu_1, \sigma_2^2)$ \\
        $\overline{X} = \frac{1}{n} \sum_{i}^{n} X_i$ \\
        $S_x^2 = \frac{1}{n - 1} \sum_{i}^{} (X_i - \overline{X})^2$
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        $Y_i \ldots Y_m \sim \mathcal{N} (\mu_2, \sigma_2^2)$ \\
        $\overline{Y} = \frac{1}{m} \sum_{i}^{m} Y_i$ \\
        $S_y^2 = \frac{1}{m-1} \sum_{i}^{} (Y_i - \overline{Y})^2$
    \end{minipage}
    \[ S_p^2 = \frac{(n-1)S_x^2 + (m-1)S_y^2}{n+m} \]
    \begin{minipage}{0.2\textwidth}
        $H_0$ \\[2ex]
        $\mu_1 = \mu_2$ \\[2ex]
        $\mu_1 = \mu_2$ \\[2ex]
        $\mu_1 = \mu_2$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        $H_a$ \\[2ex]
        $\mu \not =  \mu_2$ \\[2ex]
        $\mu \not =  \mu_2$ \\[2ex]
        $\mu \not =  \mu_2$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        TS \\[2ex]
        $\frac{\overline{X} - \overline{Y}}{\sqrt{\sigma^2_{1/n} + \sigma^2_{2/m}}}$ \\[2ex]
        $\frac{\overline{X} - \overline{Y}}{\sqrt{S_p^2(\frac{1}{n} + \frac{1}{m})}}$ \\[2ex]
        $S_i \in \text{T-student}$ 
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        Livello $\alpha$ \\[2ex]
        rif. $|TS| > z_{\frac{\alpha}{2}}$
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        P - Value \\[2ex]
        $2P(z\geq |TS|)$
    \end{minipage} 
    \paragraph{4) T-test per coppie di dati}
    Se $X_1$ e $X_2$ \underline{\textbf{NON}} sono indipendenti \\
    \begin{minipage}{0.5\textwidth}
        $W_i = X_i - Y_i$
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        $X_i \sim \mathcal{N}(\mu_1, \sigma_1^2)$ \\
        $Y_i \sim \mathcal{N}(\mu_2, \sigma_2^2)$
    \end{minipage}
    \paragraph{ES} Manutenzione (n guasti) tagliand \\[2ex]
    $H_0 : \mu_a - \mu_b \geq 0 \quad \overline{W} = \frac{1}{5}(-7.5 + 2.5 - 2.5 - 3.5 -1.5) = -2.5$ \\[2ex]
    $S^2_W = \frac{1}{4}(W_i - \overline{W})^2 = 13$ \\[2ex]
    $Ts = \frac{\frac{\overline{W}}{S_W}}{\sqrt{n}} = \frac{\frac{-2.5}{\sqrt{13}}}{\sqrt{5}} = 1.55$
    \[ P_r \{ T_{n-1} \leq Ts \} = \{ T_4 \leq Ts \}\]
    \paragraph{5) Test sulla varianza}
    \[ H_0 : \sigma^2 = \sigma^2_0 \quad H_a : \sigma^2 \not = \sigma^2_0 \]
    \[ \frac{(n-1)}{\sigma^2_0} \sim \mathcal{X}^2_{n-1} \]
    $Pr \{ \mathcal{X}^2_{1-\frac{\alpha}{2}, n-1} \leq \frac{(n-1) s^2}{\sigma^2_0} \leq \mathcal{X}^2_{\frac{\alpha}{2}, n-1}\} = 1 -\alpha$
    \paragraph{Uguaglianza di varianza}: \\
    $X_1 \ldots X_n \qquad Y_1 \ldots Y_n \quad H_0 : \sigma^2_x = \sigma^2_y \qquad H_a : \sigma^2_x \not = \sigma^2_y$ \\[2ex]
    $\displaystyle S^2_x - S^2_y \quad Ts = \frac{\frac{S^2_x}{\sigma^2_x}}{\frac{S^2_y}{\sigma^2_y}} = \frac{S^2_x}{S^2_y}$ \\[4ex]
    $\frac{S^2_x}{S^2_y} \sim F_{n-1, m-1} \qquad Pr \{ F_{1-\frac{\alpha}{2}, n-1, m-1} \leq \frac{S^2_x}{S^2_y} \leq - F_{1-\frac{\alpha}{2}, n-1, m-1} \}$ \\
    \centerline{\textit{Non rifiuto se soddisfa la disuguaglianza}}
    \paragraph{Test parametro Bernoulli} (Var discrete.) \\
    $H_0 : p \leq p_0 \quad H_a i p > p_0$ \\[2ex]
    $p(k) = \binom{n}{k} p^k (1-p)^{n-k} \quad \text{\textit{n campioni (Bernoulli)}}$ \\[4ex]
    \paragraph{Binomiale $\sim$ Gaussiana} (quando n è grande) \\
    X n eventi favorevoli \\
    $\ev{X} = np \qquad Var(X) = np(1-p) \quad \mathcal{N}(np, np(1-p))$
    \paragraph{Esempio} Difetti di fabbricazione: \\
    n = 300 $H_o i p \leq p_0 \quad p_0 = 2\%$ \\
    X = 10 n difetti \\[2ex]
    $\displaystyle\frac{X-np}{\sqrt{nP_0(1-p_0)}} = \frac{10-300 \cdot 0.02}{\sqrt{300 \cdot 0.02 \cdot 0.98}} = 1.65$
    \[ Pr \{ z > 1.65 \} = 0.0495 \]
    \subsection{Modelli previsionali}
    \subsubsection{Modelli di regressione previsionale}
    $Y_i = \alpha + \beta x_i + e_i  \qquad e_i \sim \mathcal{N}(0, \sigma^2)$ \\[4ex]
    Problema $\{ x_i, y_i \}^n_{i=1} \quad \alpha, \beta = ?$ \\[4ex]
    Sum of square -> SS \\
    $SS = \sum_{i}^{n} (y_i - \alpha + \beta x_i)^2 \text{\textit{Dove B e A -> var aleatoria}}$ \\[2ex]
    \begin{equation*}
        \begin{cases}
            & \displaystyle\frac{dSS}{dA} = -2 \sum_{i}^{n} (y_i - A - Bx_i) = 0 \\
            & \displaystyle\frac{dSS}{dB} = -2 \sum_{i}^{n} (y_i - A - Bx_i)^2x_i = 0
        \end{cases} 
    \end{equation*}
    \begin{equation*}
        \begin{cases}
            & \displaystyle\sum_{i}^{n} y_i = nA + B \sum_{i}^{} x_i \\
            & \displaystyle\sum_{i}^{n} x_i y_i = n \sum_{i}^{n} + B \sum_{i}^{n} x_i
        \end{cases} 
    \end{equation*}
    \[ A = \frac{1}{n} \sum_{i}^{} y_i - B\frac{1}{n} \sum_{}^{} x_i = \overline{y} - \beta \overline{x} \]
    \subsubsection{Regressione lineare}
    $y = \alpha + \beta x \quad e \sim(0,1) \quad y_i = A + \beta x$ \\[4ex]
    $\ev{B} = \beta \qquad \ev{A} = \alpha$ \\
    $Var[B] = \frac{\sigma^2}{\sum_{i}^{}x^2_i - n\overline{x}} \qquad Var[A] = \frac{\sigma^2 \sum_{}^{} x^2_i}{n(\sum_{}^{} x^2_i - n\overline{x}^2)}$ \\[4ex]
    $SS_R = \sum_{i}^{} (y_i - (A+B x_i))^2 \quad (\text{Somma dei quadrati dei residui})$ \\\\
    $\frac{SS_R}{\sigma^2} \sim \mathcal{X}^2_{n-2}$ \\[4ex]
    \begin{minipage}{0.45\textwidth}
        $\ev{\frac{SS_R}{\sigma^2}} = n-2$
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        $\ev{\frac{SS_R}{n-2}} = \sigma^2$
    \end{minipage}
    \paragraph{MLE}: \\
    \[ f_{y_1 \ldots y_n} (y_1 \ldots y_n) = \prod_{i= 1}^{n} \frac{1}{\sqrt{2 \pi} \sigma^2} e^{-\sum_{}^{} (y-(\alpha + \beta x_i0))^2 / 2\sigma^2} \]
    \centerline{MSE = MLE}
    \paragraph{Notazione}:
    \begin{equation}
        \begin{aligned}
            & S_{x y}=\sum_i\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)=\ldots=\sum_i x_i y_i-n \bar{x} \bar{y} \\
            & S_{x x}=\sum_i\left(x_i-\bar{x}\right)^2=\ldots=\sum_i x_i^2-n \bar{x} \\
            & S_{y y}=\sum_i\left(y_i-\bar{y}\right)^2=\cdots=\sum_i y_i^2-n \bar{y}
        \end{aligned}
    \end{equation}
    \begin{minipage}{0.5\textwidth}
        $S_{xy}$ (Dispersione di x e y)
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        $S_{xy}$ (Dispersione di x)
    \end{minipage} \\
    \centerline{$S_{xy}$ (Dispersione di y)} \\[3ex]
    \begin{minipage}{0.3\textwidth}
        $A = \overline{y} - B \overline{x}$
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        $B = \frac{S_{xy}}{S_{xx}}$
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        $SS_R = \frac{S_{xx} S_{yy} - S_{xy}^2}{S_{xx}}$
    \end{minipage}
    \paragraph{Inferenza su $\mathbf{\beta}$}
    $\frac{B - \beta}{\sqrt{\frac{\sigma^2}{S_{xx}}}} \sim \mathcal{N}(0,1) \qquad \frac{SS_R}{\sigma^2} \sim \mathcal{X}^2_{n-2}$
    \[ \frac{\frac{B - \beta}{\sqrt{\frac{\sigma^2}{S_{xx}}}}}{- \sqrt{\frac{SS_R}{j^2(n-2)}}} \sim t_{2-2} \] \\[4ex]
    $\displaystyle\sqrt{\frac{(n-2) S_{xx}}{SS_R}} (B- \beta) \sim t_{n-2}$ \\
    $\beta \in B \pm \sqrt{\frac{SS_R}{(n-2) S_{xx}}} \quad t_{\frac{\alpha}{2}, n-2} \rightarrow \text{Livello di confidenza}$
    \paragraph{Inferenza su $\mathbf{\alpha}$}
    $\displaystyle\frac{A - \alpha}{\sqrt{\frac{\sigma^2 \sum_{}^{} x^2_i}{nS_{xx}}}} \sim \mathcal{N}(0,1) \qquad \frac{SS_R}{\sigma^2} \sim \mathcal{X}^2_{n-2}$ \\
    coeffieciente della retta: \\[2ex]
    $\alpha \in A \pm \frac{SS_R \sum_{}^{} x^2_i}{\sqrt{n(n-2) S_{xx}}} \sim t_{\frac{\alpha}{2}, n-2} \rightarrow \text{Livello di confidenza}$
    \paragraph{Interferenza su $\alpha + \beta x_0$}: \\[2ex]
    $\ev{A+Bx_0} = \ev{A} + x_0 \ev{B} = \alpha + \beta x_0$ \\
    $Var(A+B x_0) = \ldots = \sigma^2[\frac{1}{n} + \frac{(\overline{x} - x_0)^2}{S_{xx}}]$ \\[3ex]
    \textbf{Distribuzione $A+B x_0$ ?} \\
    $A+B x_0 \sim \mathcal{N}(\alpha + \beta x_0, \sigma^2 [\frac{1}{n} + \frac{(\overline{x} - x_0)^2}{S_{xx}}])$ \\
    Stima di $\alpha + \beta x_0$ \\
    \[ \frac{A + B x_0 - (\alpha + \beta x_0)}{\sqrt{(\frac{1}{n} + \frac{(x_0 - x)^2}{S_{xx}}(\frac{SS_R}{n-2}))}} \sim t_{n-2} \] \\[2ex]
    \[ \alpha + \beta x_0 \in A + B x_0 \pm t_{\frac{\alpha}{n}, n-2} \sqrt{\frac{1}{n} + \frac{(x_0 - \overline{x})^2}{S_{xx}} (\frac{SS_R}{n-2})} \]
    \centerline{\textbf{Piccolo} se i punti sono vicini alla media}
    \subsubsection{Regressione Lineare (e non)}
    $\displaystyle \{ x_i, y_i \}^2_{i=1} \leftarrow$ punti stocastici \\
    \paragraph{Inferenza $\alpha + \beta x_0 = \ev{y} \rightarrow$} non so niente del valore della y in quel punto
    \centerline{Inferenza $y_0 = y(x_0) \theta$} \\[2ex]
    $\displaystyle\alpha + \beta x_0 \in A + B x_0 \pm t_{\frac{\alpha}{2}, n-2} \sqrt{(\frac{1}{n} + \frac{(x_0 - \overline{x})^2}{S_{xx}})\frac{SS_R}{n-2}}$ \\
    $\mathbf{\alpha + \beta x_0} \rightarrow$ Il punto $x_0$ che sta sulla retta $\alpha + \beta x_0$
    \paragraph{Inferenza $y_0 = y(x_0) \rightarrow$} predittivo
    \begin{equation*}
        \begin{aligned}
            & y \sim \mathcal{N}(\alpha + \beta x_0, \sigma^2) \\
            & A + B x_0 \sim \mathcal{N}(\alpha + \beta x_0, \sigma^2 (\frac{1}{n} + \frac{(x_0 - \overline{x})^2}{S_{xx}})) \\
            & y_0 - (A + B x_0) \sim \mathcal{N}(\sigma, \sigma^2 (1 + \frac{1}{n} + \frac{(x_0 - \overline{x})^2}{S_{xx}})) \\
            & y_0 = y(x_0) = A + B x_0 \pm t_{\frac{\alpha}{2}, n-2} \sqrt{(1 + \frac{1}{n} + \frac{(x_0 - \overline{x})^2}{S_{xx}}) \frac{SS_R}{n-2}}
        \end{aligned}
    \end{equation*}
    \paragraph{Coefficiente di determinazione}
    \definizione La verifica dei miei valori
    \formula $S_{yy} = \sum_{i = 1}^{n}(y_i - \overline{y})^2 \rightarrow$ dispersione di y \\
    La dispersione è data da due fattori:
    \begin{itemize}
        \item Retta (regressione)
        \item Rumore
    \end{itemize}
    $\displaystyle SS_R = \sum_{i = 1}^{n}(y_i - (A + B x_i))^2 \rightarrow$ Dipende dalla porzione non spiegata della retta \\
    Utilizzo coefficienti di determinazione:
    \[ R^2 = \frac{S_{yy} - SS_R}{S_{yy}} = 1 - \frac{SS_R}{S_{yy}} \quad 0 \leq R^2 \leq 1 \] 
    Se $\mathbf{R^2 = 1}$ la dispersione è data solo dalla retta \textit{(regressione)} \\
    Se $\mathbf{R^2 = 0}$ la dispersione è data solo dal \textit{rumore} \\\\
    \centerline{La retta è migliore più $R^2$ è vicino a \textbf{1}}
    \paragraph{Coefficiente di correlazione}
    \formula
    \[ r = \frac{\sum_{i}^{} (x_i - \overline{x}) (y_i - \overline{y})}{\sqrt{\sum_{i}^{}(x_i - \overline{x})^2 \sum_{i}^{} (y_i - \overline{y})^2}}= \frac{S_{xy}}{\sqrt{S_{xx} S_{yy}}} \]
    \centerline{$r^2 = \frac{S_{xy}^2}{S_{xx} S_{yy}} = \ldots = 1 - \frac{SS_R}{S_{yy}} \rightarrow $ Dimostrazione matematica di $R^2$}
    \paragraph{Analisi dei residui} $y-(A + Bx) \rightarrow$ verifico tutti gli errori residui \\
    \textbf{Per la non linearità}
    \paragraph{Trasformazione lineare}
    \[ W(t) = ce^{-dt} \]
    \centerline{Dove $ce$ e $-dt$ sono \textit{parametri}} \\[3ex]
    $\log(W(t)) = \log(c) -dt \rightarrow$ Prob. soluzione al non lineare $y = \alpha + \beta x$
    \paragraph{Rimedio al caso eteroschedastico}: \\
    $\displaystyle y_i = \alpha + \beta x_i + e_i \qquad e_i \sim \mathcal{N}(0, \sigma^2_i) \rightarrow$ errore in crescita x \\
    $\displaystyle Var(e_i) = \frac{\sigma^2}{W_i} \sum_{}^{} W_i (y - (A + B x_0))^2$
    \begin{itemize}
        \item Regressione lineare multipla
        \begin{itemize}
            \item $\mathbf{y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \ldots \beta_k x_k + e}$
            \item $min \sum_{i}^{} (y_i - (B_0 + B_1 x_{i1} + B_2 x_{i2} + \ldots + B_k x_{ik}))^2$
        \end{itemize}
        \item Regressione (lineare) polinomiale
        \begin{itemize}
            \item $y = \beta_0 = \beta_{1} x + \beta_2 x^2 + \ldots + \beta_k x^k + e$
            \item $\left\{ \underline{x_i}, y_i \right\}^n_{i=1}$
        \end{itemize}
    \end{itemize}
    $\displaystyle\frac{\diff}{\diff B_0} = 0 = \sum_{i}^{} (y_i - 1 - B_1 x_{i1} + B_2 x_{i2} + \ldots + B_k x_{ik})$ \\
    $\displaystyle\frac{\diff}{\diff B_1} = 0 = \sum_{i}^{} x_{i1} (y_i - B_0 - B_1 x_{i1} + B_2 x_{i2} + \ldots + B_k x_{ik})$ \\[4ex]
    $x^t x\underline{\beta} = x^t \underline{y} \Longrightarrow \underline{\beta} = (x^x x)^{-1} x^t \underline{y}$ \\
    \paragraph{AN.O.VA} (analysis of variance) \\
    Analisi delle varianze / estensione del test di ipotesi sulle medie
    \paragraph{Esempio} voti medi degli anni scolastici \\[2ex]
    \begin{minipage}{0.45\textwidth}
        \textbf{Anno.} \\
        2020-2021 lockdown \\
        2021-2022 lockdown parziale \\
        2022-2023 presenza
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \textbf{Voti medi.} \\
        $\mu_a$ \\
        $\mu_b$ \\
        $\mu_c$
    \end{minipage} \\[2ex]
    $H_0 : \mu_a = \mu_b = \mu_c$ \\[3ex]
    \textbf{1) stimatore di $\sigma^2$:} \\
    \[ \sum_{i=1}^{} \sum_{j}^{} \frac{(x_{ij} - \ev{x_{ij}})^2}{\sigma^2} = \sum_{i=1}^{} \sum_{j=1}^{} \frac{(x_{ij} - \mu_i)^2}{\sigma^2} \sim \mathcal{X}^2_{m\cdot n} \]
    \[ SS_W = \sum_{i=1}^{} \sum_{j=1}^{} \frac{(x_{ij} - x_{i*})^2}{\sigma^2} \sim \mathcal{X}^2_{m \cdot n - m} \]
    $\ev{\frac{SS_w}{\sigma^2}} = n \cdot m - m$ \\[2ex]
    $\ev{\frac{SS_w}{nm - m}} = \sigma^2 \quad$ stimatore 1 \\[4ex]
    \textbf{2) stimatore di $\sigma^2$ supponendo $\mu_1 = \mu_2 = \mu_3 = \ldots = \mu_m = \mu$}
    \[ n \sum_{i=1}^{m} \frac{(x_{i*} - \mu)^2}{\sigma^2} \sim \mathcal{X}^2_m \qquad x_{**} = \frac{\sum_{i=1}^{m} \sum_{i=1}^{n} x_{ij}}{m \cdot n} \]
    $SS_b = n \sum_{i = 1}^{m} (x_{i*} - x_{**})^2 \sim \mathcal{X}^2_{m-1}$ \\
    $\ev{\frac{SS_b}{m-1}} = \sigma^2 \rightarrow$ Stimatore 2
    \paragraph{Verifico stimatori}:
    \[ Ts = \frac{SS_b / m-1}{SS_W / nm - m} \rightarrow \text{intorno a 1 va bene}\]
    \textbf{F Distribution:} $F_{m-1}, mn - m, \alpha$
    \paragraph{ANOVA}: \\
    Se i gruppi sono uguali : $n $ camp = $n \cdot m$ \\
    Se sono diversi : $n $ camp = $\sum_{i}^{} n_i$
    \paragraph{Life testing} (Misura di affidabilità) \\
    $x \geq 0 \rvert$ \textit{tempo di vita} $\qquad \lambda(t) = \frac{f(t)}{1- F(t)}$ \\
    $f(t) = \text{Densità di popolazione}$ \\
    $\lambda(t) = \text{Intensità di rottura (failure rate)}$
    \begin{equation*}
        \begin{split}
            P(x \in (t, t + \triangle t) \rvert x > t) &= \frac{P(x \in (t, t + \triangle t), x > t)}{P(x > t)} \\
            &= \frac{P(x \in (t, t + \triangle t))}{P(x > t)} \\
            &\approx \frac{F(t) \triangle t}{1-F(t)}
        \end{split}
    \end{equation*}
    \paragraph{Intensità di rottura} 
    \definizione Densità condizionale di probabilità che un oggetto funzionante almeno fino a t si guasti "subito dopo"
    \formula \[ \lambda(t) = \frac{F(t)}{1 - F(t)} \] 
    Se la distribuzione è esponenziale:
    \[ \lambda(t) = \frac{\lambda e^{-\lambda t}}{1 - 1 + e^{-\lambda t}} = \lambda \rightarrow \text{dove $\lambda$ è una costante}\]
    \paragraph{Proprietà} $\lambda(t) \Rightarrow F(t)$
    \[ \lambda(s) = \frac{f(s)}{1 - F(s)} = \frac{F'(s)}{1 - F(s)} = \frac{d}{dS} [ - \log (1 - F(s))] \]
    \[ \int_{0}^{t} \lambda(s) = -\log (1-F(s)) + \log (1- F(s)) = 1 - F(t) = e^{-\int_{0}^{t} \lambda(s) ds}\]
    \paragraph{Esempio} Tasso di mortalità di un fumatore ($\mathbf{\lambda_s}$) e di un \underline{non} fumatore ($\mathbf{\lambda_n}$)
    \[ \lambda_s(t) = 2 \lambda_n (t) \]
    \begin{equation*}
        \begin{split}
            &= P(\text{Non fumatore di età \textbf{A} vive fino a \textbf{B}}) \\
            &=P(\text{Non fumatore vive fino a \textbf{B} | è vissuto fino \textbf{A}}) \\
            &= \frac{P(\text{Non fumatore viva fino a \textbf{B}})}{P(\text{Non fumatore viva fino a \textbf{A}})} \\
            &= \frac{1 - F_N(B)}{1 - F_N(A)} \\
            &= \frac{e^{-\int_{0}^{B} \lambda (t) \, dt}}{e^{-\int_{0}^{A} \lambda(t) \, dt}}
        \end{split}
    \end{equation*} \\[2ex]
    Quindi: \\
    $\displaystyle P(\text{Non fumatore di eta \textbf{A} vive fino a \textbf{B}}) = e^{-\int_{A}^{B} \lambda(t) \, dt}$ \\[3ex]
    Per i non fumatori invece: \\
    $\displaystyle P(\text{Fumatore di eta \textbf{A} vive fino a \textbf{B}}) = e^{-\int_{A}^{B} \lambda(t) \, dt} = Ps$ \\[2ex]
    Dove $Ps = (Pn)^2 \rightarrow$ quindi la probabilità di soppravivenza del fumatore è uguale alla probabilità di soppravivenza del non fumatore \textit{al quadrato}
    \paragraph{Probabilità} che un non fumatore arrivi ai 60 anni sapendo che è arrivato ai 50: \\
    \[ \lambda_N(t) = \frac{1}{20} \qquad 50 \leq t \leq 60 \]
    $ P_N = e^{-\int_{50}^{60} \frac{1}{20} \, dt } = e^{-\frac{1}{20}(60-50)} = e^{-\frac{1}{2}} \approx 0.607 \approx 61\%$ \\
    $P_\leq (e^{-\frac{1}{2}})^2 = e^{-1} \approx 0.368 \approx 37\%$
    \paragraph{Stima di affidabilità} N oggetti che si possono guastare \textit{indipendenti} tra di loro \\
    Tempi di vita: $\lambda e^{-\lambda t} \qquad \lambda = \frac{1}{\theta} \Rightarrow \frac{1}{\theta} e^{-\frac{t}{\theta}}$ \\[2ex]
    \textbf{Dati a disposizione}: $x_1 \leq x_2 \leq x_3 \leq x_4 = r \quad i_1 = 2, i_2 = 3, i_3 = n, i_4 = 1$ \\[2ex]
    Studio la variabile aleatoria $X_i$, $i_j$ indica quale \textit{oggetto si è guastato} per j-esimo all'istante $x_j$ \\[3ex]
    (n-r) non si sono guastati $\Rightarrow$ per questi $X_i > x_r$
    \[ f x_1, x_2 \ldots x_r (x_1, x_2, \ldots, x_r) = \prod_{j}^{r} \frac{1}{\theta} e^{-\frac{x_j}{\theta}} = \frac{1}{\theta^r} e^{-\frac{\sum_{j}^{} x_j}{\theta}}\]
    Ora per i non guasti:
    \[ P\left(X_j > x_j \text{ con } j \not \in \{ i_1, \ldots, i_r \}\right) = \prod_{r+1}^{n} (1-F_{X_j} (x_r)) = \left[ 1- (1- e^{\frac{-x_r}{\theta}}) \right] \]
    \begin{equation*}
        \begin{aligned}
            & \log L = -r \log\theta - \frac{1}{\theta} \left[\sum_{i}^{r} x_i + (n-r) x_r \right] \\
            & \frac{d \log}{d \theta} = - \frac{r}{\theta} + \frac{1}{\theta^2} \left[ \sum_{i}^{r} x_i + (n-r) x_r \right] = 0 \\
            & -\theta r + \left[ \sum_{i}^{r} x_i + (n-r) x_r \right] = 0 \\
            & \hat{\theta} = \frac{\sum_{i}^{r} x_i + (n-r) x_r}{r} = \frac{t}{r} = \frac{TTT}{r} \rightarrow \text{Total Time Test}
        \end{aligned}
    \end{equation*}
\end{document} 